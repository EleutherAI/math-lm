{"input": "I mainly work in the field of high-performance computing (HPC). For a 2020 paper I did a Monte Carlo calculation using 24800 CPUs, and one part of the paper was about showing that even up to 24800 CPUs, the speed of the calculation still roughly doubles when doubling the number of CPUs. To reach 24800 CPUs I had to use hundreds of compute nodes, with each node having 40 CPUs.\nI was therefore curious about the possible existence of \"high-performance\" proof assistants.\nWhen I did a literature search, I did find some papers about parallel implementations that run on several cores (CPUs) but nothing at a scale that I'd call large. Before this site went live, I asked about this in the Proof Assistants Chatroom on MathOverflow, and Hu\u1ef3nh Tr\u1ea7n Khanh told me that the proof assistant Lean does spawn multiple threads to do the calculations required to check proof correctness, but only uses shared memory parallelism meaning that it can run on multiple CPUs but not multiple nodes. .\nI wonder if there's any proof assistant software that can be run on not only mutliple cores of a single node, but also on multiple cores spread across multiple nodes?", "output": "Rolling with Cl\u00e9ment's distinction of concurrency, there is a third one:\n\nConcurrency when doing proof search and/or finding counter examples.\n\nIsabelle supports all three kinds of concurrency. However, and this is why this answer might not be satisfying for you, all these are almost embarrassing parallelism only:\n\n(1) is solved by realization that proofs are basically irrelevant, we only care that the certification succeeds at some point (which can be later in time). [1]\n(2) is solved by exploiting the DAG structure of a theory, checking independent theories in parallel. [1]\n(3) The tools sledgehammer (and try0) and nitpick, described in [2] and [3] respectively, leverage some forms of concurrency. The former will check different provers (CVC4, Vampire, z3 etc.) in parallel and reports which have succeeded. It can also make use of remote solvers SystemOnTPTP web service, you might find interesting techniques for some of those servers (I don't know enough to comment on that).\n\n\n1: Parallel Proof Checking in Isabelle/Isar by Makarius Wenzel\n2: Hammering Away: A User\u2019s Guide to Sledgehammer for Isabelle/HOL by Jasmin Blanchette et al.\n3: Picking Nits: A User\u2019s Guide to Nitpick for Isabelle/HOL by Jasmin Blanchette", "meta": {"post_id": 1, "input_score": 25, "output_score": 9, "post_title": "High-performance proof assistants"}}
{"input": "I want to get started with Lean 4, but I need to install it. The download page links to binaries to download, but I don't know what to do with them. Is there a \"best practice\" way to install Lean, or should I just download them and put them in my $PATH?\nI'm using Arch Linux (x86-64).", "output": "Install the latest Lean 4 nightly through elan: in any bash-compatible shell, run:\ncurl https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh -sSf | sh -s -- --default-toolchain leanprover/lean4:nightly\n\nOn Windows, instead run in cmd\ncurl -O --location  https://raw.githubusercontent.com/leanprover/elan/master/elan-init.ps1  powershell -f elan-init.ps1 --default-toolchain leanprover/lean4:nightly  del elan-init.ps1\n\n\nInstall VS Code.\n\nOpen VS Code and install the lean4 extension.\n\nCreate a new file with the extension .lean and add the following code:\n#eval Lean.versionString\n\nYou should get a syntax-highlighted file with a \"Lean Infoview\" on the right that tells you the installed Lean version when placing your cursor on the last line.\n\nYou are set up! You can now also run lake init foo from the command line to create a package, followed by lake build to get an executable version of your Lean program.\n\n\n\nNote: Packages have to be opened using \"File > Open Folder...\" for imports to work. Saved changes are visible in other files after running \"Lean 4: Refresh File Dependencies\" (Ctrl+Shift+X).\n\nNote: This information may be outdated. Find out more here.\n\nSource: Quickstart - Lean Manual", "meta": {"post_id": 2, "input_score": 8, "output_score": 10, "post_title": "How do I install Lean 4?"}}
{"input": "I keep bumping into references to \"Lean\", but then find there are two GitHub repositories for Lean:\n\nhttps://github.com/leanprover\nand\n\nhttps://github.com/leanprover-community\n\n\nCan someone explain the difference? Is one better than the other, or do they have different purposes?", "output": "https://github.com/leanprover is the official Lean organization, and contains the original works being built by Leo de Moura and his team. It also is in particular where development of Lean 4 is happening.\nhttps://github.com/leanprover-community is run by the broader Lean community, albeit in close contact with the Lean developers (some of whom contribute to repositories in both organizations). It hosts the community website and resources, but also Mathlib, which is in some sense Lean's \"killer app\" -- over a million lines of mathematics built by many mathematicians and contributors, and with extremely active continued work. As of July 2023, it has been fully migrated to Lean 4.\nThe leanprover-community repository also hosts a community website, including instructions for getting started with Lean 4 and Mathlib.", "meta": {"post_id": 4, "input_score": 21, "output_score": 33, "post_title": "What is the difference between `leanprover` and `leanprover-community` GitHub repositories?"}}
{"input": "I know that Proof Assistants can be used in fact-checking, but what else can they be used for? How would they be used for specific roles, including industry jobs and everyday life?", "output": "The seL4 microkernel (approx ~9000 lines of C) was verified in Isabelle/HOL. See this page for a discussion of just what this proof means (e.g., in terms of what assumptions underpin it). This code has been deployed by the likes of Collins Aerospace and Boeing as part of DARPA research projects.\nThe CakeML system is a verified compiler for a functional programming language that has proofs of correctness for every transformation from input strings to machine code, and bootstraps itself.", "meta": {"post_id": 6, "input_score": 16, "output_score": 10, "post_title": "What are some examples of proof assistants doing something useful?"}}
{"input": "Formal verification is\n\nthe act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.\n\nProof assistants can help with that; it's only a small jump from algorithms to computer source code. Let's say I have some code written in [insert the language of your choice here]: can I use proof assistants to verify whether it's free from bugs? You may assume it's a low-level language if that matters, but I expect it doesn't because almost all programming languages are Turing complete.", "output": "An important point to clarify is the meaning of \"free of bugs\". Correctness only makes sense relative to a specification. Only once you have decided what property you care about (your definition of \"correctness\"), can you start to think about how to prove it.\nFor safety properties (memory safety (buffer overflows, use after free, etc.), undefined arithmetic (overflow, divide by zero, etc.), unhandled exceptions, etc.), there are popular approaches to detect or prevent them automatically, via type systems (\"well-typed programs do not go wrong\") or program analyses (e.g., Infer).\nFunctional correctness is a class of properties that relate your program to a \"high-level\" specification. For example you may expect that a function int sum(int* x) does compute a sum $\\sum_i x_i$, and that allows you to think about that function as the mathematical sum without worrying about how the numbers are laid out in memory. Since there is no hope for full automation for the most expressive specification languages, this is an area where proof assistants have a lot of potential (and it has been active for decades).\nA most popular example is sel4, a microkernel written in C and verified in Isabelle/HOL. In the following excerpt, note again the precision of \"against its specification\":\n\nseL4's implementation is formally (mathematically) proven correct (bug-free) against its specification, has been proved to enforce strong security properties, and if configured correctly its operations have proven safe upper bounds on their worst-case execution times. It was the world's first operating system with such a proof, and is still the only proven operating system featuring fine-grained capability-based security and high performance. It also has the most advanced support for mixed criticality real-time systems.\n--- https://sel4.systems/About/\n\nFor another example, VST provides a program logic for C embedded in the Coq proof assistant. VST is particularly notable because it is formally related to a verified compiler, CompCert: there is a proof that VST's program logic is sound with respect to the same operational semantics that CompCert was proved against. Software Foundations, Volume 2 contains an introduction in Coq to Hoare logic, a core idea in the development of program logics for imperative languages, such as VST.\nFor a functional language such as Haskell, it is a lambda calculus at its core, similar to proof assistants based on type theory (Coq, Lean, Agda). hs-to-coq banks on that similarity, by translating functions into the proof assistant's language, so you can prove any properties expressible there.", "meta": {"post_id": 9, "input_score": 9, "output_score": 18, "post_title": "Could proof assistants be used to prove that some piece of code is free of bugs?"}}
{"input": "What was the first proof assistant? What was it used for? When and by who was it created? Is it still used today? And what was its purpose?", "output": "I regard \"proof assistant\" as synonymous with \"interactive theorem prover\", which rules out many of the systems mentioned in other answers. For example, the input to AUTOMATH was a deck of punched cards. AUTOMATH was a milestone but it was not a proof assistant.\nI believe that the Boyer/Moore theorem prover was interactive from the start, though it may have supported a batch mode as well. Commands consisted of axioms, requests to prove particular formulas, and hints concerning rewriting. Boyer and Moore published this work originally in 1975. And it continues to this day in the form of ACL2.\nAnother very early system was Weyhrauch's FOL. It was an interactive proof checker for first-order logic. I never saw the point even back in the 1970s, since FOL is both inexpressive and easily automated.\nThe first recognisably modern proof assistant is undoubtedly Edinburgh LCF, which introduced the architecture on which most of today's proof assistants are closely based, especially those of the HOL family. Mike Gordon traces the evolution here.", "meta": {"post_id": 17, "input_score": 25, "output_score": 13, "post_title": "What was the 1st Proof Assistant?"}}
{"input": "How does it compare to dependent records in Agda/Lean/etc.?\nI heard that locales in Isabelle are used to model algebraic structures in a talk, bu. Isn't this a kind of dependent type in some sense? Is Isabelle really simply typed?", "output": "Basically, a locale is a name for a set of fixed parameters, that come with assumptions. Inside the locale, these assumptions are added implicitly to everything you define/prove. When you have concrete parameters that satisfy the assumptions, you can interpret the locale, exposing the stuff inside (instantiated for your concrete parameters).\nIn Isabelle, locales are quite powerful. You can base your locales on other locales, partially instantiate them, rewrite stuff, etc. (for details https://isabelle.in.tum.de/dist/Isabelle2021-1/doc/locales.pdf)", "meta": {"post_id": 23, "input_score": 12, "output_score": 12, "post_title": "What are \"locales\" in Isabelle?"}}
{"input": "I found a site where I can use Coq in my browser: jsCoq Interactive Online System. Are there any major differences between the experience I'm getting on this site and the downloadable version(s)?", "output": "jsCoq is based on the Coq source code (regularly updated to use newer Coq versions), compiled to Javascript (or WebAssembly) to run in the browser using js_of_ocaml. The main differences with running Coq on the desktop is that jsCoq is slower and that the fast reduction machines (vm_compute and native_compute) are disabled. It also comes with a set of bundled libraries, which is similar in spirit to the Coq Platform, but different in terms of library choice and version numbers. Obviously, the GUI is also (very slightly) different to the various editors with Coq support that are out there.\njsCoq is developed at: https://github.com/jscoq/jscoq", "meta": {"post_id": 29, "input_score": 11, "output_score": 14, "post_title": "What are the differences between jsCoq and the versions of Coq that can be downloaded?"}}
{"input": "How would you explain to a beginner in simple terms, what a Proof Assistant is?", "output": "Let us clarify some jargon. Usually we call proof assistants systems which can really help us finding proofs, i.e. they should have a tactic language. Systems which lack this feature (tactics) are usually called type checkers or just type systems. On the other hand, in modular systems, kernels in proof assistants could also be called just type checkers. All of them could be called theorem provers as long as they have at least the hole inference and hole finding features.", "meta": {"post_id": 35, "input_score": 18, "output_score": 19, "post_title": "What is a Proof Assistant?"}}
{"input": "From the Wikipedia article on Proof Assistant it shows some Proof Assistants are based on Higher Order Logic, (HOL Light) and some are based on Dependent Types, (Coq).\nAre there any other means upon which to build a Proof Assistant? (Thinking MetaMath and ACL2).", "output": "There are a lot of bases, theories and techniques in proof assistants. Let me show you how deep the rabbit hole is (in suggested order of implementation):\n(Fibrational) Dependent Type Theories\n\nCoC (Calculus of Constructions, PTS, Coquand)\nCiC (Calculus of Inductive Constructions, Paulin-Mohring)\nMLTT (Martin-L\u00f6f)\nHoTT-I (Isaev)\nCubical (Cubical Type System, CCHM)\nHTS (Homotopy Type System with Two Equalities, 2LTT, Voevodsky, ACKS)\nde Rham (Infinitesimal Shape Modality, Cherubini, Groupoid)\nCartesian Cubical (ABCFHL)\nGuarded Cubical (BBCGSV)\nRezk (Synthetic Theory of $\\infty$-categories, Riehl, Shulman, Kudasov)\nEquivariant Super HoTT (Schreiber)\n\nThese are what we basically call Type Theories in Martin-L\u00f6f sense with 5 inference rules, some of them are monadic modalities. But on the market there are present proof assistants based on different theories, like custom model checkers or direct term rewriting systems. They internalize upper MLTT type systems into itself, like Dedukti. So the full answer to this question could be a good survey type PhD! Systems 1-7 are implemented as examples and included as parts of cubical.systems.\nEmbeddable DSL in host language\nBefore fibrational provers, the logic of computable functions was built upon relying on STLC flavours. In that sense you can write a needed type-checking rules in ML language as in meta theory and embed programs as strings instead of direct internalizations in the style of synthetic dependent type theories. This technique is not unique to HOL series, but is known as embeddable DSL in host-language technique. In that sense ACL2 is embeddable DSL in ANSI Common Lisp.\nAd-hoc dedicated model checkers and other LF\nEverything that is big enough or impractical to run in the main Proof Assistant environment usually is being rewritten into separate ad-hoc model checker for the sake of speed, simplicity, etc. This category could include all known models checking cores: Reference SMT solver Z3 by Leonardo de Moura, TLA+ by Leslie Lamport, even Metamath-like systems or set-theoretical systems like Mizar. I think if you built the basis only for Set theory but not all mathematics it is called ad-hoc. Logical Frameworks like Twelf or system with separate language for automation of inference rules like Andromeda are also could be treated as separate ad-hoc logical systems. Modern powerful ad-hoc term rewriting systems like Dedukti could eat 2LTT with less effort than HoTT in HOL by Josh Chen.", "meta": {"post_id": 36, "input_score": 32, "output_score": 29, "post_title": "What are the bases for different Proof Assistants?"}}
{"input": "How would you explain so a beginner in simple terms why Proof Assistants are important, and how / why they are used?", "output": "(A lightly edited copy and paste of my answer to a similar question on the Lean Zulip chat: What is the point of Lean's maths library.)\nIn my talk about Lean-GPT-f at Harvard, I gave five different motivations for formalized mathematics:\n\nMechanically check mathematical proofs.  While it doesn\u2019t yet make sense to have mathematicians formalize all of their work, it does make sense to formalize particularly foundational, interesting, or troublesome proofs, such as undergraduate mathematics, the independence of CH, or Scholze\u2019s lemma in the Liquid Tensor project.\nDigitize mathematics.  Everything nowadays is digitized, and systematically organized into databases.  However, mathematics is remarkably resistant to this trend.  Sure we have pdfs and LaTeX files of most articles, but none of these formats capture the mathematical content of the articles.  Formalization of a large body of mathematics, whether the full proofs or just the theorem statements, would open up many possibilities for automatic retrieval and search of mathematics.\nUnify and archive mathematical knowledge.  Right now the world of mathematics consists of a mostly disjoint collection of articles (most of them mostly true).  There have been past heroic efforts from Euclid to Bourbaki to systematically organize the current mathematical knowledge of the time.  More modern incarnations include Wikipedia, nLab, and the Stacks Project.  Formalization provides yet another approach to this, whereby we systematically build up a library of mathematics starting at the foundations, up to the most interesting new developments.\nProve the correctness of software and hardware. There have been some great success stories so far in software and hardware verification and I see a lot of potential for verified security software.  It is also the most likely way to monetize ones skills in formal theorem proving.  As the mathematics in formal libraries get more sophisticated, so do the types of algorithms that we can formalize.\nMaking mathematics accessible to computers in a new way.  I\u2019ve heard it said that the one thing mathematics is good for is more mathematics.  This \u201cfalse truism\u201d also holds for formalization.  We don\u2019t know the full applications of formal mathematics right now except that it will improve our ability to formalize more mathematics in the future.  However, we do have hints already at the ways that one can use a large library of formal mathematics.  AI projects like TacticToe, HOList, Tactician, and GPT-f are trained off of many person-years of formalization effort, and hopefully will eventually become a virtuous loop, helping users better formalize more mathematics.  It\u2019s hard to predict what we can do with a large library of formalized, computer-readable, interconnected mathematics, but it is very reasonable to believe it could have important positive future impacts on mathematics, technology, science, and education.", "meta": {"post_id": 57, "input_score": 19, "output_score": 19, "post_title": "Usage and Importance of Proof Assistants"}}
{"input": "Related to What is the easiest proof assistant to start with? \u2013 I suppose familiarity with programming in general would be beneficial to learning how to work with proof assistants. But which kind of programming languages (used in 'regular' programming, think the top 100 tags on Stack Overflow) would be most helpful? Do functional programming languages offer benefits over object-oriented or procedural ones?", "output": "An old (vintage / classic) functional language that has been specifically designed for proof assistants is the Standard ML language. Quoting the book ML for the Working Programmer\n\nEvery successful language was designed for some specific purpose: Lisp\nfor artificial intelligence, Fortran for numerical computation, ... ML was designed for theorem proving.\n\nThe same book even contains a chapter devoted to writing a FOL tactical theorem prover from scratch.", "meta": {"post_id": 60, "input_score": 14, "output_score": 8, "post_title": "Which programming languages are most similar to proof assistants?"}}
{"input": "The Natural Number Game suggests the possibility of rewriting mathematics textbooks with embedded interactive Lean code. I'm envisioning something like Google Colab or Mathematica, where you can insert either code blocks or text/LaTeX blocks.\n\nQuestion: Is there already a good package for doing this?\n\nIf there were one, and it were available readily, this would accelerate the interface between mainstream mathematics and proof assistants. (Imagine classic mathematics texts rewritten with embedded code exercises.)\nThe best so far, to my knowledge, Patrick Massot's formatter discussed here, and maybe this is good enough? I'd be interested in knowing what is being done, and collecting the results here.\n(Maybe this is CW?)", "output": "There is also Patrick Massot's https://github.com/PatrickMassot/leanblueprint which is powering\n\nhttps://leanprover-community.github.io/sphere-eversion/blueprint/\nhttps://leanprover-community.github.io/liquid/\nhttps://b-mehta.github.io/unit-fractions/blueprint/index.html\n\nThis is not so much a package that formats Lean code into LaTeX, but it is certainly helping to\n\naccelerate the interface between mainstream mathematics and proof assistants.", "meta": {"post_id": 62, "input_score": 26, "output_score": 8, "post_title": "Is there software for interfacing Lean code with LaTeX?"}}
{"input": "In Lean, explicit function arguments are enclosed in round brackets and implicit ones in curly brackets, as in this example:\nlemma surjective.comp {g : \u03b2 \u2192 \u03c6} {f : \u03b1 \u2192 \u03b2} (hg : surjective g) (hf : surjective f) :\n  surjective (g \u2218 f) := ...\n\nHowever, occasionally we see some strange double curly brackets like this:\ndef injective (f : \u03b1 \u2192 \u03b2) : Prop := \u2200 \u2983a\u2081 a\u2082\u2984, f a\u2081 = f a\u2082 \u2192 a\u2081 = a\u2082\n\nI'm sure I once read an explanation of this, but my attempts to search for it now have failed.  What do the double curly brackets mean?", "output": "Single braces{\u22ef} indicate a maximally inserted implicit argument and {{\u22ef}} a weakly inserted implicit argument, as explained in the manual.\nConsider the definition of the identity map in which the first argument is implicit:\ndef id {\u03b1 : Type u} (x : \u03b1) : \u03b1 := x\n\nIn an expression such as id 3 Lean will figure out the implicit argument \u03b1 to be \u2115. But what will it do when faced just with id, not applied to anything? There are two options:\n\nIt could create a new meta-variable ?M (also known as an existential variable) and convert the lone id to id ?M. The value of ?M will be determined later as more information becomes available. This is known as a maximally inserted implicit argument, because it is inserted whenever possible. The above definition does so.\n\nIndicate that the argument should not be maximally inserted by using {{\u22ef}} instead:\ndef id {{\u03b1 : Type u}} (x : \u03b1) : \u03b1 := x\n\nNow, when faced with id, Lean will read it precisely as id without inserting anything. This strategy is known as weakly inserted implicit arguments.\n\n\nLet's try it out:\nuniverse u\n\ndef id\u2081 {\u03b1 : Type u} (x : \u03b1) : \u03b1 := x\n\n#check id\u2081\n-- output: id\u2081 : ?M_1 \u2192 ?M_1\n\ndef id\u2082 {{\u03b1 : Type u}} (x : \u03b1) : \u03b1 := x\n\n#check id\u2082\n-- output: id\u2082 : \u03a0 \u2983\u03b1 : Type u_1\u2984, \u03b1 \u2192 \u03b1\n\nIn most cases the difference between the maximally and weakly inserted arguments does not matter. But when it does matter it becomes very annoying if only the wrong one is available. For example, the OP shows the definition of injective map using {{\u22ef}}. They are there, because with {\u22ef} the expression injective f would mean injective f ?a\u2081 ?a\u2082 which reads \u201cfor some fixed but yet to be determined a\u2081 and a\u2082, if f a\u2081 = f a\u2082 then a\u2081 = a\u2082\". That's not what \u201cf is injective\u201c means.", "meta": {"post_id": 66, "input_score": 30, "output_score": 31, "post_title": "In Lean, what do double curly brackets mean?"}}
{"input": "I am trying to update a large body of Lean code to work with the current version of mathlib.  This is difficult because there have been a large number of backwards-incompatible changes even since I last did this in mid-November.  When I come across something that no longer works, I guess some keywords and try to search for commits or issues or current code in github that explains what has changed and why.  Is there any recommended method that is more systematic than this?", "output": "The mathlib commit log serves as a de facto changelog. Since the project updates far too frequently to keep a traditional changelog, a good method if you know approximately when breaking changes occured is to look at commits around that date. Titles are hopefully descriptive, and link to the pull request which explains the change in more detail.\nThe leanprover-community organization hosts CI for third-party projects, including a GitHub Actions script that will try to update your project periodically. If changes in mathlib break your code, it will open an issue on your repository linking to a diff of the mathlib changes that may be to blame.\nNote that this action is most helpful if you fix the breakage relatively quickly. Breaking changes can compound, and it can only alert you at the first breaking change.", "meta": {"post_id": 71, "input_score": 22, "output_score": 17, "post_title": "How to track backwards-incompatible changes in mathlib?"}}
{"input": "The most well-known tutorials/learning materials, including Book 1, Chapter 2 of Software Foundations for Coq and the Natural Number Game for Lean, use natural numbers to introduce induction. But there are multiple complaints by new learners that having to prove trivial facts of natural numbers via induction feels unintuitive and intimidating (one example).\nHas there been any attempt at a tutorial that doesn't start with natural numbers? I can imagine using monomorphic lists instead, say a list of booleans. For example, proving associativity of list concatenation (a ++ b) ++ c = a ++ (b ++ c) should feel less intimidating (because it is no longer a \"trivial\" property of natural numbers the students would have taken as granted) and more interesting than proving associativity of addition on natural numbers. And then the properties of natural numbers could be introduced by pointing to similarities with list operations.", "output": "Joseph Hua, Ken Lee, and Bendit Chan have recently released the HoTT Game, designed as an introduction to Cubical Agda for mathematicians with no proof verification experience. The two parts of this tutorial are Trinitarianism (nicely summarized here) which introduces all the basic notions of (homotopy) type theory, and Fundamental Group of the Circle, whose goal is showing that $\\pi_1(S^1)\\cong\\mathbb Z$.\nGranted, at some points working with $\\mathbb Z$ reduces to working with (two copies of) $\\mathbb N$, but unlike the tutorials mentioned in the original post, the theorems of this tutorial are not about arithmetic of $\\mathbb N$.", "meta": {"post_id": 103, "input_score": 18, "output_score": 10, "post_title": "Has there been any attempt at a tutorial that doesn't start with natural numbers?"}}
{"input": "Most proof assistants (with some exceptions like Isabelle/ZF or the B method) rely on type theory.\nSee also the MathOverflow question What makes dependent type theory more suitable than set theory for proof assistants?\nNext, formalizing mathematics is one of the main use cases of an interactive proof assistant.\nBut many pen-and-paper mathematical proofs are written in a set-theoretic fashion (e.g., in ZFC).\nFinally, there are many different type theories (first-order, higher-order, dependently-typed or not, intuitionistic or not, etc.)\nSo, formalizing existing set-theoretic results typically involves some formal hindrances.\nKeeping the constructiveness issue apart, and focusing on the definability of mathematical objects:\n\nwhat mathematical concepts can't typically be \"formalized as is\", within the main proof-assistants / type-theories?\n(e.g., we can think about matrices in a non-dependently-typed theory\u2026)\nis a \"workaround\" possible, to still be able to formally deal with this mathematical concept in the considered theory? (if yes, feel free to cite a formalization example)", "output": "Andrej's answer that \"Almost no pen-and-paper mathematics is written in ZFC\" is correct.  But it's perhaps also worth noting that some pen-and-paper mathematics is written in ZFC (or, at least, something closer to ZFC than to type theory), such as much of the mathematics done by set theorists (in which phrase, of course, \"set\" refers traditionally to a ZFC-set).\nA good deal of set theory can be rephrased more structurally to make sense in type theory, but this would be a rephrasing, and the amount of work that would be required is, I think, variable.  For instance, set theorists work with ordinals defined in the von Neumann way, with $\\in$ as the well-founded relation.  This in particular implies that ordinals are literally unique representatives of isomorphism classes of well-ordered sets.  In type theory it is more natural to work \"structurally\" with arbitrary well-ordered sets rather than with von Neumann ordinals.  This makes some things more cumbersome because of isomorphisms that get carried around (although under univalence, these isomorphisms become equalities), but probably doesn't change things a whole lot.\nOn the other hand, as far as I know, no one has ever come up with a convincing structural presentation of Godel's constructible universe $L$.  The only structural approaches to $L$ that I know of essentially involve building a model of ZF(C) in some way and then constructing $L$ inside that model.  Note that $L$ is, as far as I know, still the only way to prove the relative consistency of the Axiom of Choice, so it is of interest even to mathematicians who don't care about ZF for its own sake.  (In contrast, the symmetric/permutation methods used to prove relative consistency of not-AC, and the forcing methods that can be used to prove consistency of other properties such as GCH and not-GCH, seem much more amenable to structuralization.)  [Note: this paragraph was edited to remove a false claim that GCH is not forceable; see comments.]\nAs another example, Scott's trick is a method used in pen-and-paper mathematics that is of interest beyond set theory, but relies on the axiom of foundation from ZFC, and so is not obviously formalizable in type theory.", "meta": {"post_id": 107, "input_score": 26, "output_score": 10, "post_title": "What set-theoretic definitions can't easily be formalized in a type theory?"}}
{"input": "Conor proposed a seemingly promising way towards universe polymorphism: https://mazzo.li/epilogue/index.html%3Fp=857&cpage=1.html IMHO, the idea behind this proposal is:\n\nYou build things starting at level 0 and going up to whatever constant level you need, but then you can lift the whole thing up any number of other levels.\n\nHowever, it seems to be not implemented in any system except redtt. Is it flawed somehow, or it's just people are unaware of its existence?\nIs it sufficiently powerful? Is it formalized?", "output": "I don\u2019t know for sure, but I don\u2019t think this has been explored, no. At least I can say for sure it\u2019s not around in Coq\u2019s world (yes, time has passed and Coq now has universe polymorphism), although I think the way Coq does things subsumes this.\nBasically, how polymorphism works in Coq is that when you have a (polymorphic) definition, the system collects all constraints the universe levels in the definition need to satisfy for it to be well-typed. When you use it later on, the definition gets instantiated with new levels, and the system checks at that point that the constraints on those are satisfiable.\nThis is quite similar to choosing the right \"universe lifting\" as McBride proposes, but there\u2019s one lack of flexibility in the proposal: the relation between the universes remains fixed, while in the constraint-based approach you also can make the \"distance\" between the universes in the definition vary. I'm really not sure how annoying that might get in practice, but it might bite, requiring to duplicate definitions with different distances (rather than at different levels). On the other hand, universe constraints in Coq are becoming a performance bottleneck in quite a few places, so this might be a way to mitigate those\u2026", "meta": {"post_id": 113, "input_score": 15, "output_score": 9, "post_title": "What happened to Conor McBride's universe polymorphism proposal?"}}
{"input": "I recall many years ago seeing a very large and well-interlinked (by computer) list of verified results starting from base assumptions and leading to all sorts of things that naive me did not expect computers to be able to handle e.g. measure theory. I don't recall what software was used, and certainly don't see any sort of link on e.g. https://leanprover.github.io/.\nWhere can I find these types of lists?", "output": "Most of the main theorem provers have central libraries of formal mathematics.\nHere I've including their official statistics (accurate as of 2022-02-10) and more importantly a link to where you can find up-to-date statistics.\n\nIsabelle - Archive of Formal Proofs (AFP) (statistics)\n\n193,100 lemmas\n661 articles\n423 authors\n3,324,900 lines of code\n\n\nLean - mathlib (statistics)\n\n77,316 theorems\n32,040 definitions\n2,436 files\n221 contributors\n783,799 lines of code\n\n\nMizar - Mizar Mathematical Library (MML) (statistics)\n\n62,163 theorems\n12,840 definitions\n1,357 articles\n263 authors\n\n\nMetamath - set.mm (enumerated list)\n\n29,300 axioms, definitions, and theorems\n\n\nHOL Light - HOL Light repo (no statistics)\nCoq - Coq Standard Library\n\nUnlike the others, Coq doesn't have a central mathematics library that I'm aware of, although it has a lot of mathematics.  There are also no official statistics that I can find.", "meta": {"post_id": 141, "input_score": 27, "output_score": 11, "post_title": "Where can I find lists of theorems that have been verified?"}}
{"input": "The basics of model theory contain, as far as I understand, some theorems that are difficult to prove in their full generality.\nFor example, the compactness theorem in FOL for languages with arbitrarily many symbols was proven by Maltsev in 1936. G\u00f6del originally proved the countable case in 1930.\nTrying to formalize model theory and make a usable library in a proof assistant seems like it would be pretty tough, since you have to make some awkward foundational choices. What exactly is $\\models$? It's probably too big to be a set.\nI'm wondering if there are any proof assistants out there that have a model theory library or ones that were specifically designed to tackle this area (or do other similar things, like study set-based semantics of non-classical logics with a classical set theory in the background).", "output": "First, recall that model theory (of FOL) is just mathematics about certain types of mathematical objects, \"structures\" as they are defined in model theory.  So it isn't really any more difficult than working with say groups or topological spaces.  (I'm probably brushing too much under the rug, but at least Lean and Coq, since they have universes, they should avoid the \"set\" issue you mention.  Also, I think the technical term for how model theory is usually handled is \"deep embedding\", but I'm not sure I could give a good definition off hand.)\nHOL-Light\nJohn Harrison formalized FOL model theorem in HOL-Light. The paper is Formalizing basic first order model theory. Some code I've found is some theorems on logic up the the compactness theorem, and this folder about the logic of arithmetic which I think also has other logic stuff including some completeness theorem results (and maybe also the incompleteness theorem).  This is another completeness theorem possibly for a different logic.\nOther theorem provers\nAlso see the Lean project A formalization of forcing and the unprovability of\nthe continuum hypothesis.  Besides being the type of work I think you are interested in, it also has a good background section which I'll quote here:\n\nFirst-order logic, soundness, and completeness\nThere are many existing formalizations of first-order logic. Shankar [39] used a deep embedding of first-order logic to formalize incompleteness theorems. Harrison gives a deeply embedded implementation of first-order logic in HOL Light [18] and a proof-search style\naccount of the completeness theorem in [19]. Margetson [33] and Schlichtkrull [34] use\nthe same argument for the completeness theorem in Isabelle/HOL, while Berghofer [6] (in\nIsabelle) and Ilik [22] (in Coq) use canonical term models.", "meta": {"post_id": 144, "input_score": 11, "output_score": 11, "post_title": "Proof assistants or libraries in proof assistants for working with model theory"}}
{"input": "Coq and Lean are two of the most common proof assistants out there (but the question of course applies to other proof assistants too).\nWhat are the main differences between Coq and Lean? Ideally it would be nice to know the differences for a mathematician not interested in foundations and for someone more involved in CS and type theory.", "output": "It it hard to write an answer here which is not just a technical list of differences in specification, but also avoids the flame war of \"my theorem prover is better than yours\".  (I'd be happy to delete or change this answer if it comes across too flame-war-ish or incorrect.)\nLean is essentially classical, meaning it usually uses the law of excluded middle and the axiom of choice for proofs when it is convenient.  This is mostly a social norm, in that this is the standard of the Lean community and main math library, mathlib.  But it is also technical in a sense in that it is difficult to avoid classical reasoning with the common tactics (and basically impossible to do homotopy type theory even within term proofs).  For many just interested in formalizing pure math (except of course, say, homotopy type theory or the internal logic of some topos) this is fine and maybe preferable.  If you want more flexibility on axioms or want to be able to extract algorithms from your proofs it is a problem.  (However, some projects in Coq also use classical reasoning, so it isn\u2019t like it is forbidden.  It just might mean that projects which use classical reasoning won't be used by other projects which are trying to stay constructive.)\nThere is also a difference in community goals.  I can't speak to the goals of the Coq community, but it seems to me that the main goal of the Lean3 community is to add as much mathematics as they can to the mathlib library, the main mathematical library of Lean.  There is a strong emphasis on unity and cohesion in this library.  It is constantly refactored to make it work well together.  This (according to Lean users) makes it possible to easily combine separate areas of mathematics like rings and topological spaces (to get topological rings), but it also means that any project which is not in mathlib or trying to stay up-to-date with mathlib will quickly bit-rot.  (Even Lean3 itself---at least the community edition most commonly used---changes rapidly as well, sometimes in backward incompatible ways.)\nWe will see in Lean4 if the focus is the same or changes.  Already, many are using Lean4 as a very capable programming language, so there may be more diversity of projects and less focus on cohesion, but nonetheless, I think the contributors of mathlib would like the Lean4 version to still be highly cohesive.\nThere is also a lot written on the internet about say Lean vs Coq.  Of course note that since Lean and (to a lesser degree) Coq are constantly changing, some of that information may be out of date.  (For example, there was a time when Lean 2 supported both classical and constructive logic, even homotopy type theory.)", "meta": {"post_id": 153, "input_score": 58, "output_score": 33, "post_title": "What are the main differences between Coq and Lean?"}}
{"input": "Isabelle/HOL\u2019s logic is called a logic of \u201ctotal functions\u201d, yet there are functions that are obviously not total, such as division div.\nHow does this make sense?", "output": "If you have a Isabelle/HOL function f :: a \u21d2 b between two types a and b (NB: the function arrow in Isabelle is \u21d2, not \u2192), then \u2013 by definition of what it means to be a function in HOL \u2013 whenever you have a value x :: a, then the expression f x (i.e. f applied to x) is a value of type b. Therefore, and without exception, every Isabelle function is total.\nThis has a few consequences when modeling functions that are commonly considered partial.\n\nIn some cases, like subtraction on naturals, the function is made artificially total by returning a particular value: 0 - 1 = 0.\nThis may look fishy at first, but it is ok (theorems about subtraction that don\u2019t hold for the extended version will have necessary preconditions) and useful (a fair number of theorems do hold, and can be used without having to discharge such preconditions).\n\nIn other cases the partial function takes on an arbitrary, but unspecified and unknown value.\nConsider the Eps :: \"('a \u21d2 bool) \u21d2 'a\" operator, which models choice. When the predicate P is true for some x, then Eps P is one of these, and P (Eps P) holds (\u2203x. P x \u27f9 P (Eps P) is a theorem). But nothing stops you from using Eps with a never-true predicate, e.g. Eps (\u03bb n, n = n + 1) :: nat. This is a value of type nat, but you don\u2019t know which one, and no useful theorems are provided for it.\nSimilarly, the undefined :: 'a term is a value of any type, and again one that you don\u2019t know anything about.\n\nRelatedly, in HOL all types are inhabited \u2013 for example by the undefined above. This is crucial for the tricks shown above, and otherwise not a big deal either \u2013 in contrast to Logics based on Type Theory, which conflate types with propositions, and inhabited types with theorem.\n\n\nI wrote a longer text on partiality and definedness of Isabelle/HOL function.", "meta": {"post_id": 154, "input_score": 18, "output_score": 17, "post_title": "How can all functions be total in Isabelle?"}}
{"input": "I think in NBE, you get rid of certain substitutions, and it solves the problem of binding representation (so you don't have to use indices or capture-avoiding substitution or something). But I believe there are other differences I'm not aware of.\nIn particular, I'm interested in elaboration of dependently typed lambda calculus, where nbe can be used to calculate the normal forms of core terms.", "output": "When I asked this question of some people with experience implementing proof assistants, their answer was \"eta-laws\".\nIf you test equality using only some kind of reduction algorithm, then generally speaking your equality will only incorporate beta-reductions such as $(\\lambda x. M)(N) \\equiv M[N/x]$.  Eta-equivalences such as $M \\equiv (\\lambda x. M x)$ are quite difficult to implement with a reduction-only algorithm.  There are various reasons for this, such as the fact that if you want to reduce $\\lambda x. M x$ to $M$ you need to check that $x$ doesn't occur in $M$, and if you want to expand $M$ to $\\lambda x. M x$ you need to have type information present to know that $M$ has a function-type.  I won't say it's impossible, but normalization-by-evaluation is a clean, generalizable, and easy-to-reason-about family of algorithms that perform both $\\beta$-reduction and $\\eta$-expansion in a type-directed way.\nTo be sure, I believe this answer depends on a somewhat broader meaning of \"normalization by evaluation\" than is sometimes used.  For instance, this property doesn't depend on the representation of values of function-type as actual metalanguage functions; it's sufficient to defunctionalize them and we can still call that \"normalization by evaluation\" for this purpose.\nIn addition, one doesn't have to check $\\beta\\eta$-equality by first normalizing to a $\\beta\\eta$-normal form and then doing an $\\alpha$-equivalence check: one can incorporate the two $\\beta$ and $\\eta$ stages directly into a \"bidirectional\" equality-checking algorithm.  But for purposes of this answer (which is, again, not one that I invented, but was given by those in the know) we also consider that to be a form of \"normalization by evaluation\".", "meta": {"post_id": 196, "input_score": 18, "output_score": 10, "post_title": "What are the advantages of normalization by evaluation over traditional reduction-based normalization?"}}
{"input": "I started programming a Haskell-clone recently (source code).\nAfter basic hindley-milner, problems start to arise. Implementation of data/codata statements seem to ensure my language is breaking up on every front.\nAt this point I seem to lack foundations for the language. System F introduces kinds, but would it be better to approach theorem provers and pick a different calculus as foundation?\nAlso looking for overall suggestions for this kind of a project.", "output": "If you are asking \"can we use proof assistants to develop foundations of programming languages\" then the answer is positive. Two well-known such developments are:\n\nSoftware foundations by Benjamin Pierce, implemented in Coq.\nProgramming language foundations in Agda by Phil Wadler, implemented in Agda.\n\nThere are also specialized proof assistants that make it easier to develop the meta-theory of a programming language, such as Twelf, Abella and Beluga.", "meta": {"post_id": 205, "input_score": 15, "output_score": 12, "post_title": "Can theorem provers be used to form foundations for programming languages?"}}
{"input": "One could define streams in the following manner\nSection stream.\n  Variable A : Type.\n\n  CoInductive stream : Type :=\n  | Cons : A -> stream -> stream.\nEnd stream.\n\nBut as far as I can tell, such a coinductive stream is isomorphic to a function nat -> A. Arguably, working with the coinductive definition is harder.\nWhat is an example of a situation where working with a coinductive type is a more intuitive option to express a certain object? When is it more convenient to use a coinductive proof? What are some instances when the coinductive proof/value makes the most sense?", "output": "First a note: you are using the syntax for so-called \"positive\" coinductive types, which makes them look like inductive types, defined by \"constructors\".  This has various problems: it's not well-justified semantically, and syntactically it breaks \"subject reduction\".  Modern versions of Coq also support \"negative\" coinductive types, which look like record types rather than inductive types, defined by \"fields\", and are much better-behaved, so it is suggested to use those instead.  See the reference manual.\nAs for the actual question, I believe one place that coinduction is valuable is when computational behavior matters.  An infinite list represented by a function nat -> A is \"random access\": you can ask it for f n whenever you wish, and it doesn't in principle need to know anything about f k for k < n to compute f n.  Whereas the elements of a coinductive stream must be computed in order.  This certainly makes the function-representation easier to reason about.\nHowever, if your \"infinite list\" has the property that in practice we have to compute all the f k for k < n in order to find f n, then working with the coinductive version respects this, and encourages the user to take account of it.  Given a stream we can turn it into a function where f n is computed by pulling off n elements of the stream and returning the last one, but this performs a lot of computation if n is large, and if we call f a lot of times it could be very inefficient.  A coinductive data representation encourages the writing of corecursive functions, which force us to deal with this kind of stepwise-computed list more efficiently.\nStreams are arguably a bit too simple for this behavior to be clearly visible.  I like to think of a general coinductive type as a \"server\", in line with Conor McBride's comment in Let's see how things unfold:\n\nAs a total programmer, I am often asked 'how do I implement a server as a program in your terminating language?', and I reply that I do not: a server is a coprogram in a language guaranteeing liveness.\n\nAn element of a coinductive \"server type\" represents the server in a particular state.  When we call its destructor, we pass it some request from a client, and it returns a response to the client and also the new state of the server.  By contrast, using the corresponding \"functional\" representation, in order to compute the response of the server to any query we would have to replay the entire history of interaction of all clients with the server up to that point.  Clearly this is inefficient, and also a bad representation of how a real-world server actually works.\nCoinductive types can also be valuable in situations where type dependency makes it tricky to even come up with a functional representation, and if you do come up with one it's more awkward.  I'm most familiar with this in the case of homotopy theory and higher category theory.  For instance, a \"globular set\" can be defined coinductively as a set $A$ together with a family of globular sets indexed by $A\\times A$:\nCoInductive GSet : Type := {\n  carr : Type;\n  hom : carr -> carr -> GSet;\n}.\n\nThe corresponding functional representation is much less clear.  This can be enhanced to deal with infinite-dimensional categories in a surprisingly clean way.", "meta": {"post_id": 208, "input_score": 18, "output_score": 18, "post_title": "When to use coinductive types?"}}
{"input": "In the Lean mathlib, I see some places where a typeclass argument is included in a class definition, such as locally_finite_order. In other places, I see the \"extends\" keyword used, such as ring. What are the differences and tradeoffs between these two ways of doing things?", "output": "Section 2.2 of Anne Baanen's recent paper Use and abuse of instance parameters in the Lean mathematical library gives a very nice explanation of this, referring to them as \"unbundled subclasses\" (like locally_finite_order) and \"bundled subclasses\" (like ring). This paper discusses, in various places, situations where we might prefer one over the other.\nOne example is in section 5.1. Defining a class\nclass module (R M : Type) extends add_comm_monoid M := ...\n\nwould automatically create an instance that says, \"in order to show add_comm_monoid T, it suffices to show module ?x T.\" Instances like this can lead to huge performance issues, so the preferred pattern here is the unbundled definition:\nclass module (R M : Type) [add_comm_monoid M] := ...\n\nIn the other direction, overuse of unbundled subclasses can lead to performance problems of a different kind (Baanen section 10, or a blog post by Ralf Jung). Full unbundling leads to exponential blowup in the number of type class arguments (and thus searches) needed as you climb a hierarchy of classes. Baanen gives the example:\ninstance prod.comm_monoid\n  [has_one M] [has_one N] [has_mul M] [has_mul N]\n  [semigroup M] [semigroup N] [mul_one_class M] [mul_one_class N]\n  [monoid M] [monoid N] [comm_semigroup M] [comm_semigroup N]\n  [comm_monoid M] [comm_monoid N] :\ncomm_monoid (M \u00d7 N) := ...", "meta": {"post_id": 209, "input_score": 16, "output_score": 13, "post_title": "Extends vs including a typeclass argument"}}
{"input": "Here is a stripped down version of something that is causing me trouble.\nvariables {\u03b1 : Type*} [decidable_eq \u03b1]\n\ndef S (a : \u03b1) : set \u03b1 := \u03bb b, b = a\n\ndef P (a : \u03b1) : decidable_pred (\u03bb (c : \u03b1), c = a) := \n by { apply_instance, }\n\ndef Q (a : \u03b1) : decidable_pred (\u03bb (c : \u03b1), c \u2208 (S a)) := \n by { simp[S], apply_instance, }\n\n\nLean is happy with the definition of P, but rejects the definition of Q as follows:\ntactic.mk_instance failed to generate instance for\n  decidable_pred (\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a)\nstate:\n\u03b1 : Type u_1,\n_inst_1 : decidable_eq \u03b1,\na : \u03b1\n\nIn both P and Q, we ask tactic.mk_instance to generate a decision procedure for a predicate.  The predicate in Q is equivalent under \u03b2-reduction to the one in P, but tactic.mk_instance succeeds with P but fails with Q.  Is this just a weakness in the tactic or is there some more subtle issue?  Is there some straightforward way to work around this kind of problem?", "output": "The predicate in Q is equivalent under \u03b2-reduction to the one in P\n\nNot according to Lean in this case. set.mem is a def, which are not unfolded during typeclass inference by default. Thus inference fails at unifying it with =:\nset_option trace.type_context.is_def_eq_detail true\n\n[class_instances]  class-instance resolution trace\n[class_instances] (0) ?x_0 a_1 : decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) := _inst_1 (?x_1 a_1) (?x_2 a_1)\n[type_context.is_def_eq_detail] [1]: decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) =?= decidable (?x_1 a_1 = ?x_2 a_1)\n[type_context.is_def_eq_detail] [2]: (\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1 =?= ?x_1 a_1 = ?x_2 a_1\n[type_context.is_def_eq_detail] after whnf_core: a \u2208 \u03bb (b : \u03b1), b = a_1 =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] [3]: @set.mem \u03b1 a (\u03bb (b : \u03b1), b = a_1) =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] [4]: set.mem =?= eq\n[type_context.is_def_eq_detail] on failure: set.mem =?= eq\n[type_context.is_def_eq_detail] on failure: @set.mem \u03b1 a (\u03bb (b : \u03b1), b = a_1) =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] on failure: decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) =?= decidable (?x_1 a_1 = ?x_2 a_1)\nfailed is_def_eq\n\nWe can fix this with either local attribute [reducible] set.mem or simp[S, set.mem].\nThis still begs the question, why is set.mem (and others, including set itself) not marked as reducible? AFAIR this was a conscious decision to prevent it from \"unexpectedly\" unfolding and thus mixing the \"world of sets\" with the \"world of predicates\". People usually think of them as separate concepts, so it can be confusing when your goals are suddenly switched to the other world.", "meta": {"post_id": 222, "input_score": 15, "output_score": 10, "post_title": "Why does apply_instance fail to generate decidable_pred here?"}}
{"input": "As mentioned in another question, the way that VSCode and other editors implement unicode characters that cannot be typed directly from most keyboards is an editor feature.\nAs such, different editors could do it differently, so that for instance, $\\varphi$ and $\\phi$ could be entered as \\varphi, \\phi, the other way around, or something else.\nBut perhaps there is still an official recommendation on how editors should let you type unicode characters; or was a choice made to not make an official recommendation?", "output": "Because maintaining such a big list of Unicode abbreviations is annoying and none of the editor mode maintainers are interested in encouraging untransferrable abbreviations, as far as I know, all of the Lean editor modes just copy the abbreviations.json file (the VS Code plugin is the most popular, so new abbreviations usually land there first). In practice I haven't heard of divergence between editor abbreviations files ever being an issue.\nSo to the extent that there is an \"official recommendation\" to use the VS Code plugin for Lean, that comes with the bundled list of abbreviations, but users can also add their own abbreviations if they want.", "meta": {"post_id": 237, "input_score": 11, "output_score": 9, "post_title": "Is there an official recommendation for editors to input Lean unicode characters?"}}
{"input": "In Ulf Norell's PhD thesis, which is considered the standard reference of the Agda 2 language, the universes are cumulative, say, Set i is not just an instance of Set (suc i), but also a subtype of it.\nHowever, in the implementation of Agda, this is not true. We only have the \"instance of\" relation, not the subtype relation, until recently (--cumulativity).\nIMO, it is good to have more flexibility, but why would Agda developers delete this in the beginning? What inconveniences or problems did cumulative universes bring to us?", "output": "One reason (not particularly specific to Agda) might be that eta reduction is not compatible with cumulativity.\nWith cumulativity (that is $Set_0 \\leq Set_1$), and given\n\\begin{align*}\nf &:= \\lambda X.(\\lambda Y.Y)X &: Set_0\\to Set_1\\\\\nf_\\eta &:= \\lambda Y.Y &: Set_1\\to Set_1\\\\\nf_\\beta &:= \\lambda X.X &: Set_0\\to Set_0\n\\end{align*}\nwe would have (and we can prove, say, in Coq)\n$$ f \\equiv_\\eta f_\\eta\\qquad\\text{ and }\\qquad f\\equiv_\\beta f_\\beta$$\nwhich are well-typed because\n$$(Set_1\\to Set_1) \\leq (Set_0\\to Set_1)\\quad\\text{ and }\\quad(Set_0 \\to Set_0) \\leq (Set_0\\to Set_1) ,$$\nbut we have the awkward situation that $f_\\eta$ is not comparable to $f_\\beta$.\n\nSo while Coq has cumulativity but does not eta-reduce (it does eta-convert however), Agda supports eta-reduction (please correct me if this is wrong) but does not support cumulativity.", "meta": {"post_id": 239, "input_score": 13, "output_score": 14, "post_title": "Why did Agda give up cumulative universes?"}}
{"input": "Trying to answer another question, I can't help second guessing myself about whether eta equality is provable in Coq. In particular, can one prove\nTheorem eta : forall (A : Type) (f : A -> A), (fun x => f x) = f?\n\nDoes this require functional extensionality?\n\nEdit: I already accepted an answer, but a better (followup) question would be, if Coq supports eta-conversion, why does Eval compute in (fun x => f x). not reduce to f?", "output": "As for your follow-up question: Coq implements \u03b7-conversion, but not \u03b7-reduction/expansion, because \u03b7 really is treated at the level of the conversion test and not at that of reduction (which is a sub-routine thereof). Concretely, the conversion test works roughly as follows when comparing t and u:\n\nreduce t and u to weak head normal forms (using a bunch of reduction rules including \u03b2 but not \u03b7)\nif one of the reducts is a neutral n and the other is an abstraction fun x : A => t, recursively compare n x and t (this corresponds to \u03b7-expanding the neutral followed by a congruence rule for abstraction)\n\nThe reasons to do things like this are a bit tricky, but the bottom line is that implementing \u03b7 inside reduction (be it as expansion or reduction) would either break good properties or simply be unfeasable in current Coq.\nI did a talk about this at WITS a few weeks ago, the video does not seem to be up on Youtube yet (it should be there at some point) but the slides and abstract are on my page.", "meta": {"post_id": 250, "input_score": 13, "output_score": 10, "post_title": "Is eta-equality provable in Coq?"}}
{"input": "To my knowledge, the well-known ones like Coq, Agda, Lean are all purely functional languages, and therefore require any computational function to be written in lambda calculus style. For example, you cannot directly write down an imperative algorithm for isprime: nat -> bool equivalent to, say, a C-like function\nbool isprime(nat n) {\n    if n < 2 {\n        return false;\n    }\n    for i in 2 to n-1 {\n        if n % i == 0 {\n            return false;\n        }\n    }\n    return true;\n}\n\nInstead, Coq/Agda/Lean requires the computation to be written in functional style, or you need to encode the computational model itself and reason about the process of computation manually (e.g. via modeling state transition as a binary relation and reasoning about its reflexive transitive closure).\nIs there any proof assistant implementation that allows imperative computation? Is it at least theoretically possible? What are the main challenges to allowing it (if any)?", "output": "Although Lean 4 is a pure functional programming language, it is capable of expressing functions almost exactly like your example:\ndef isprime (n : Nat) : Bool := Id.run do\n  if n < 2 then\n    return false\n  for i in [2:n-1] do\n    if n % i == 0 then\n      return false\n  return true\n\nBut you can argue that this is a cheat, since really lean is a functional language at heart, and this is true. What about \"truly imperative\" languages? Well, one issue with non-pure-functional languages is that functions are not pure, which means that you have to be careful about using them in specifications. What does the proof language even look like? Dependent type theory is one way to unify the concept of \"writing programs\" and \"writing proofs\"; in an imperative language these necessarily diverge.\nIt is, of course, possible to prove properties of imperative programs, but these tend to take the form of annotations around code, either using refinement types or design by contract style requires and ensures clauses around functions, loop invariants and other critical points in the code, with some automation to fill in the gaps. Why3 and Dafny are two strong examples of languages designed for reasoning about imperative code.\nAnother interpretation of your header question is \"a proof assistant that does imperative computation in the course of proving theorems\", and I think there are plenty of examples of this, because the metalanguage of a scriptable proof assistant can be any language at all. Isabelle and the HOL family use ML or OCaml as their metalanguage, which is not a pure functional language (although it has lots of functional leanings). Metamath is a proof language primarily used for proving theorems in ZFC, and the mmj2 proof assistant for it (written in Java) allows running user tactics written in Javascript, which is not really a functional language. Generally, I would say that there is no need for a functional language in this space, but rather the people working in the space have a predisposition toward functional languages, either because they are easily embeddable or because functional features are good for writing operations on syntax.", "meta": {"post_id": 251, "input_score": 10, "output_score": 15, "post_title": "Any proof assistant that allows imperative computation within the language itself?"}}
{"input": "In the theory and design of proof assistants based upon dependent types, I feel like there\u2019s a somewhat cultural divide between the \"MLTT\" world (with Agda as the main representative proof assistant) and the \"CIC\" world (with Coq). For instance, there are two presentations in annex A of the HoTT book, the first one presenting conversion as the equivalence relation generated by computation rules (this is I believe the traditional \"CIC\" presentation), while the second uses a typed conversion judgment (I would relate this to the \"MLTT\" culture).\nWhat would you list as the features most representatives of each side, be it in the theoretical presentation or in the implementations? Or do you think this view of two different cultures is not accurate, and if so why?", "output": "The technical answers are correct, but they completely overlook the philosophical differences between the two formalisms. Martin-L\u00f6f type theory closely reflects Arendt Heyting's explanations of the logical signs and is intuitionistic by construction. CIC is a formal calculus based on string substitution with no underlying philosophy that I can detect.\nI have personally heard Martin-L\u00f6f criticise CIC, in particular because it is impredicative. The habit of using CIC to do constructive mathematics has never made sense to me.", "meta": {"post_id": 267, "input_score": 25, "output_score": 12, "post_title": "What are the differences between MLTT and CIC?"}}
{"input": "Today, there's an exciting large-scale effort to digitize mathematics in Lean's mathematics library mathlib.\nI understand that a transition to Lean 4 is looming. I've heard that Lean has had issues with backwards compatibility in the past, but I don't know the details.\nWhen is Lean 4 expected to become the most widely used version of Lean? Will mathlib suffer from this transition? If so, how?", "output": "BIG UPDATE\nThe port to Lean4 is officially complete. There is a couple changes that we are still hoping to officially put into mathlib3 before moving it into mathlib4, but the content in mathlib3 is now a strict subset of the content of mathlib4, apart from tactics. In the very near future, the mathlib3 repository will be frozen, and all future development should now be done in Lean 4. This does not mean that Lean 4 is stable yet - there is a lot of hard work right now into this.\n\nThe community hopes to eventually move fully to Lean4. There's some details about the history of earlier Lean versions here, but they're not too relevant as the first two versions were very experimental. Now, some members of the community and Microsoft are diligently working on mathport, which will hopefully provide a full syntax-ful translation of Lean3 code into idiomatic Lean4 code. The main test-case for this, at the moment, is mathlib itself, and we are pretty determined to make it work.\nThis blogpost offers some further details on what to expect from the transition - there's still not been a consensus on how gradual the transition is, but I suspect that eventually there will be a day declared where there is an announcement \"no more PRs to mathlib3 will be accepted, please move your work over to Lean 4\".\nUPDATE (Jan '23): Currently, there's a \"progressive locking\" model going on, where we are porting files and, as soon as a file is ported to mathlib4, any PR to it in mathlib3 must be accompanied with an equivalent PR to mathlib4 making the same changes. About 20% of all files have been ported, and also many tactics (definitely over 20% of them!), but I do not think this means 20% of the work has been done - later files in the import hierarchy are often much harder to port (and also often longer).\nUPDATE (Apr/May '23): We are now at 60% progress; this can be tracked here.", "meta": {"post_id": 274, "input_score": 31, "output_score": 35, "post_title": "What will happen to mathlib when we transition to Lean 4?"}}
{"input": "Many proof assistants start with the notion of $Type : Type$ which is inconsistent. This choice makes type checking and some natural recursions arguably easier. Of course, universe levels can be added on afterward, but I'm curious if any proof assistants have taken a different path by adding a notion of unsafety.\nIn the Rust programming language, there is an unsafe fragment that relaxes some of the restrictions of the full language. Some of those \"superpowers\" include:\n\nDereference of a raw pointer\nCall other unsafe functions\n\nCritically, safe interfaces can be constructed from unsafe components, with the implicit assumption that invariants required by safe code are enforced by the programmer (instead of the type/borrow checker).\nThe trusted computing base (TCB) of a proof assistant is generally considered a desirable thing to keep small (Coq and Isabelle for instance have reasonably small TCB, in my opinion). With unsafety, any unsafe portions of library or application code (or proof code!) would also be part of the TCB, at least in terms of making sure the invariants/properties that are used in the unsafe code are true. However, this is not an obviously bad situation because the core type checker could become much simpler if unsafety is designed as a feature from the beginning. Moreover, many different type systems could be shallowly embedded by directly using unsafety to build safe interfaces. As an example, many different variants of universe hierarchies could be implemented directly from unsafe recursive types and other standard safe features. Likewise, many flavors of safe recursive types can also be derived from unsafe recursive types by requiring monotonicity proofs as guards.\nThis is how I imagine such a proof assistant might look: Suppose we have only the combinators $Y, S, K, I$ with $S, K, I$ typed in any consistent way (such that $Y$ is not derivable from them). Then, we mark $Y$ as unsafe and require that any safe combinator, $C$, constructed using $Y$ must satisfy the invariant that using $C$ to construct any other safe combinator is still safe (Ideally, we want a local invariant to make this situation more reasonable to prove). Now, the definition of $C$ is part of the TCB, but any usage of $C$ is not.\nHave any proof assistants taken an approach like this or similar?", "output": "I will summarize the answers from the comments and add my own about Coq.\nMany proof assistants have a way of adding \"unsafe\" declarations and tracking which definitions rely on them.\n\nCoq has Unset Universe Checking (a local version of the -type-in-type command-line option), which I believe is used, for example, by UniMath to introduce a computational resizing axiom.  I believe Print Assumptions tracks whether or not a definition depends on use of this flag\nAgda has the infective --type-in-type and the coinfective --safe flags, on a per-file granularity\nLean has the meta keyword for unrestricted recursion\nIdris has per-definition totality flags", "meta": {"post_id": 277, "input_score": 19, "output_score": 8, "post_title": "Have any proof assistants incorporated a notion of unsafety with Type : Type?"}}
{"input": "The paper in question: An Implementation of a Dependently Typed Lambda Calculus by Andres L\u00f6h, Conor McBride and Wouter Swierstra.\nI'm wondering whether it's correct in what's it is doing. Precisely on page 7. In type checker. It's got these evaluations with empty environments. Eg. let v = eval\u2193 t []. Precisely when these are evaluated, could it crash to the d !! i in the Figure 4?\nDependently typed languages are related to implementation of proof assistants.", "output": "All is well.\nThe implementation distinguishes between bound variables, represented as Bound i where i :: Int and free variables, represented as Par n where n :: Name. The evaluation function eval\u2191 in Figure 4 converts names to values using vpar and it replaces bound variables by looking them up in the environment (this is the line with d !! i).\nIn Figure 12 there are several calls of the form eval\u2193 \u03c4 []. These are valid because in all cases \u03c4 does not contain any \"exposed\" bound variables, therefore it is safe to evaluate it in the empty environment (\u03c4 may contain free variables, but those are not problematic).\nIndeed, the clause for typing an abstraction Lam e in the definition of type\u2193 typechecks e by first replacing the bound variable 0 with a free variable of the form Par (Bound i), where i counts the abstraction depth. This way we make sure that an \"exposed\" bound variable will never be  encountered.", "meta": {"post_id": 281, "input_score": 11, "output_score": 8, "post_title": "In the \"Simply Easy!\" paper, why is it safe to evaluate types in the empty environment?"}}
{"input": "This answer to this question about \u03b7-equivalence in Coq draws a distinction between judgmental equality and definitional equality.\nIn the simply typed lambda calculus (henceforth STLC), the following is an example of a typing rule.\n$$ \\frac{\\Gamma \\vdash t_1 : \\alpha \\to \\beta \\;\\; \\text{and} \\;\\; \\Gamma \\vdash t_2 : \\alpha}{\\Gamma \\vdash t_1(t_2) : \\beta } \\;\\; \\text{is modus ponens} $$\nIn my understanding, $t_1 : \\alpha \\to \\beta$ is a typing judgment.\nSTLC can be formalized using typing judgments exclusively.\nJudgmental equality is slightly more confusing.\nDoes it refer to a new type of judgment such as $t_1 = t_2$ for terms and $\\alpha_1 = \\alpha_2$ for types, as described here?\nI think an equality type is just a type $t_3 : \\alpha \\equiv \\beta$ that can be inhabited or not like any other type.\nAssuming the above is true, then, I'm not sure what definitional equality would be.", "output": "This answer to this question about \u03b7-equivalence in Coq draws a distinction between judgmental equality and definitional equality.\n\nThis is an incorrect attribute to the answer you linked to. Sarah didn't say that judgmental equality is distinct from definitional equality. I suggest reading that answer again.\nJudgmental equality usually means the fact that two terms are semantically exactly the same (in other words, they share the same \"semantical object\"). Definitional equality is a synonym of judgmental equality.\nEquality types are types that encode certain equality, or \"witness\" certain equality. Usually, an equality type is parameterized by two terms (and maybe something else), where every instance of such type is a proof that these two terms satisfy an equality. Equality relations encoded in an equality type are usually called \"propositional equality\". There can be different equality relations encoded by different equality types, depending on the exact definition of the equality types. In my impression, they're mostly stronger than the mathematical sense of equivalence relation (binary relation that is refl, sym, and trans).\nObviously, judgmental equality implies propositional equality (check!), but the reverse direction is not true. In case the reverse is true, we usually refer to the type theory as an extensional type theory, with the reverse known as 'equality reflection'.", "meta": {"post_id": 284, "input_score": 18, "output_score": 13, "post_title": "What is the difference between judgmental equality, definitional equality, and equality types?"}}
{"input": "In Lean, when proving basic theorems, one runs into the following kind of thing:\nimport tactic\nvariables (P : Prop)\n\nexample : P \u2192 P :=\nbegin\n  intro p,\n  exact p,\nend\n\nAfter the intro p, step, the tactic state is\nP : Prop\np : P\n\u22a2 P\n\nAt this point, we need to use the exact tactic to close the goal. All exact seems to do is to tell Lean that the hypothesis p is the same (syntactically? definitionally?) as the target.  It seems as if it would be a reasonable design goal (in the interest of efficiency) to have Lean automatically close the goal once the target is at least syntactically identical to one of the hypotheses.\nIs this a design choice, or is there some deeper reason why this doesn't happen, and what is it?", "output": "My take on that is that, in their first approximation, tactics are just ways to build terms (typically proof terms), so it is expected to have basic tactics that closely correspond to the various term formers. In particular, in your example\n\nintro x for \u03bb x, \u2026\nexact e for e.\n\nThis hopefully justifies the existence of a basic tactic like exact.\nAnd it may also justify why, unless you ask for it, no automation happens. Consider using tactics to build terms, and imagine end would just close goals using any tactic:\ndef fst {a : Type} : (a,a) \u2192 a :=\nbegin intro x, cases x with x1 x2, end\n\nNow there are two assumptions that can be used, but the choice certainly matters.\nBut when using more advanced tactics, some of these do have such automation built in, e.g. simpa.", "meta": {"post_id": 289, "input_score": 12, "output_score": 15, "post_title": "In Lean, why is the exact tactic necessary when the goal is the same as a hypothesis?"}}
{"input": "It seems Lean can produce .olean files from Lean files, and the mathlib project provides an infrastructure to download olean files, which seems to allow the developer to edit a module without having to load and process its dependencies.\nBut what are they exactly? What is their format, and what do they contain?", "output": "(My answer is for Lean 3.  I don't know how .oleans changed in Lean 4.)\nAlso, see this discussion on the Lean Zulip.\nTL;DR\nAn .olean is a \"compiled\" .lean file.  They make everything faster.\nFor users of Lean 3\nA .lean file is computationally expensive to check and process each time.  Checking all of Lean's mathlib library for example would take hours.  By running lean --make one can \"compile\" a lean file and all its dependencies to .olean files.  (Here \"compile\" is maybe best thought of an analogy.)\nIn particular there is a hash in the .olean to make sure it is in sync with the .lean files.  If the files are in sync, then when you import a dependency, Lean will skip the .lean file and only look at the .olean file.  This is orders of magnitude faster.  (Actually, it is even possible to just have use .olean files without the corresponding .lean files.)\nBecause of the huge speedup, when one uses a project based on mathlib, one should get the .olean files as well.  (This is naturally done with leanproject.)  Also, if Lean all of a sudden gets slow or takes a lot of memory, it is possible you accidentally opened or touched a dependency file in a way which is is making Lean reprocess that file and everything that depends on that file, skipping the .olean files.\nFor advanced use (such as working on mathlib), see the advice here.\nMore technical details\nAn .olean file\n\n...is basically a dump of everything in the Lean environment for that file.\n...doesn't contain any information about the tactic proofs.  Instead it only contains the term proofs which are much faster to check.  (I don't think Lean even checks the proofs in an .olean file when it is loaded, but I'm not positive.)\n...contains all the information needed by the Lean VS Code extension (which is run using the Lean language server).  For example if you hover over a definition, VS Code will give you information about the location of that definition, its type,  and its value.  You can also jump to that definition.  This is because that information is stored in the .olean file.\n...provides all the information needed to parse a Lean file.  Notation, definition names and types, and tactic parsing code is stored in the .olean files in some form or another.\n...provides all the information about the environment that tactics need.  Some tactics like library_search need to search over all theorems in the environment.  Others tactics like simp look at a list of simp lemmas to apply.  That data is all stored in the .olean files.\n...provides all the information needed by #print.  This includes the term proof of a theorem.  (Yet another reason why the term proof is in the .olean file.)\n...can be used by other checkers to double check that the proofs are correct.  There are a few external checkers available for .olean files.  They implement the same logic as in Lean's kernel.  This provides an extra guarantee that your proof is correct.  (I could be mistaken, but I think the continuous integration of mathlib runs at least one external checker on all of mathlib every time there is a PR.)\n...is not in a human readable format.  I think it is stored as a sort of linked graph similar to how it is stored internally in Lean.\n...is not well documented.  The best documentation is likely Mario's olean-parser mentioned in Joachim's answer.\n...is probably not the best way to access Lean information.  One of the best ways to get and work with the information in the .olean files is through Lean meta-programming.   Basically you can write a Lean program to traverse the environment.  This isn't well documented either, but there are a lot of knowledgeable folks and examples on the Lean Zulip chat.", "meta": {"post_id": 292, "input_score": 14, "output_score": 13, "post_title": "What is in an olean file?"}}
{"input": "Attempts to apply machine-learning (as opposed to hand-crafted automation) to formal theorem proving are still in their infancy.\nStill, there have been some successes. GPT-f found some proofs that were shorter than existing proofs in the Metamath library, and were accordingly accepted there.\nAre there other applications of machine-learning to constructing proofs in any proof assistant that attained a success of at least this magnitude - i.e. finding somewhat better versions of existing proofs, if not entirely new proofs?\n\"Better\" can be measured by whatever standards that proof assistant's community usually uses to evaluate proofs, i.e. a shorter proof might not count if it is less readable or less maintainable.", "output": "The group at OpenAI has recently published a new paper, among other problems they  have been able to prove two questions from the International Mathematical Olympiad. Isabelle's Sledgehammer has been able to find solutions to some of these examples as well, and they of course have been proved by many people already.\nI guess it's open for debate whether the proofs found by GPT-f are better than the ones found by ATP or constructed by humans, but at least Kevin Buzzard finds some of the proofs quite surprising.", "meta": {"post_id": 309, "input_score": 23, "output_score": 8, "post_title": "Successes of machine learning in formal theorem proving"}}
{"input": "tl;dr: Are there any good introductions/tutorials on how to formalize definitions and theorem statements in Lean? (in contrast to introductions on formalizing proofs)\n\nSome background: I am a phd student in combinatorial optimization and a few month ago I decided that I wanted to learn more about proof assistants and in particular Lean. So, I set myself the goal to try to get to a point where I can start to formalize mathematics I am actually currently working on (e.g. some exercises or even some lemma from my thesis) - just to have a goal and see whether I like it.\nSo, I read/worked through some of the \"obvious\" introductions (like the natural number game and Theorem proving in Lean) and I think I am now at a point where I can formalize some non-trivial proofs.\nHowever, whenever I am now trying to work on something new, I notice that I get mostly stuck on writing the definitions/theorem statements and I get the feeling that I don't really know how to do them \"the right way\" for Lean (e.g. when should I use def/class/structure/instance/...? Should my objects be types or sets? What arguments should should be implicit/explicit? ...?).\nNow, I understand that this is probably just another non-trivial thing one has to learn but I don't really know of any resources I can learn this from. Some of the tools get explained in the later chapters of \"Theorem proving in Lean\", but at least for me there is not enough relating these explanations to their applications in mathematics. One can of course also look into mathlib for examples, but most of the time this just shows me what a good formalization looks like but not really why it is the way it is and how one gets there.\nSo, in conclusion: Are there any good introductory texts or video tutorials on how write good definitions/theorems in Lean? (or any other ways to learn how to do that)", "output": "Lean for the Curious Mathematician 2020\nYou seem to be the exact right audience for this workshop. We have video recordings on youtube: https://www.youtube.com/watch?v=8mVOIGW5US4&list=PLlF-CfQhukNlxexiNJErGJd2dte_J1t1N\nTutorials\nSome other tutorials and resources are listed here: https://leanprover-community.github.io/learn.html#hands-on-approaches", "meta": {"post_id": 314, "input_score": 23, "output_score": 18, "post_title": "Tutorials for formalizing mathematical definitions/statements in Lean"}}
{"input": "Type systems, and the proof assistants based on them, are frequently divided into predicative and impredicative.\nWhat exactly does this mean? I've heard the slogan \"impredicativity means you can't quantify over things you haven't defined yet\", but I don't know how to apply this definition to a type system.\n\nPredicativity is mentioned here in this answer to this question. I don't really understand predicativity and don't understand how the concept in a classical setting lines up with or doesn't line up with the concept in a proof assistant setting.\nThis question form the Mathematics Stack Exchange and its answer describe what predicativity is when applied to the axiom schema of separation in ZFC.\nI'll do something a little nonstandard in the notation and always split the variable intepretations fed to $\\varphi(\\vec{p}; \\vec{x})$ into parameters $\\vec{p}$ and ordinary variables $\\vec{x}$.\nThe impredicative version is the ordinary one, given below.\n$$ \\forall x \\exists y \\forall z \\mathop. (z \\in y) \\leftrightarrow \n((z \\in x) \\land \\varphi(\\vec{p}; z)) \\;\\; \\text{is impredicative separation} $$\nThe predicative version is similar, but $\\varphi(\\vec{p}; z)$ is constrained to contain exclusively quantifiers bound by a parameter, i.e. $\\forall x \\in q \\mathop. \\square$ and $\\exists x \\in q \\mathop. \\square$ where $q$ is in $\\vec{p}$ or is bound by an earlier quantifier.\nBased on my understanding on the text quoted in the linked answer, this does not constitute a complete ban on impredicative quantification since the axiom schema of predicative separation itself contains unbound quantifiers $\\forall x \\exists y \\forall z \\mathop. \\square$ in its prenex that cannot be paraphrased away.\nSo in this case, \"predicativity\" applies only to the value of the metavariable $\\varphi(\\cdots)$ and a set theory with this axiom would still be an \"impredicative theory\". (Maybe?)\nChanging gears a little bit, if I look at the inference rules of the calculus of constructions, on which the proof assistant Coq is based, I'm not sure how to assess whether the formalism is predicative or not.\nHere are the rules for convenience.\n$K$ and $L$ range over $\\{P, T\\}$.\n$$ \\frac{}{\\Gamma \\vdash P : T} \\;\\; \\text{$P$ is a large type} $$\n$$ \\frac{}{\\Gamma, x: A, \\Gamma' \\vdash x : A} $$\n$$ \\frac{\\Gamma \\vdash A : K \\;\\;\\text{and}\\;\\; \\Gamma, x : A \\vdash B:L}{\\Gamma \\vdash (\\forall x : A \\mathop. B) : L} \\;\\; \\text{is universal introduction}$$\n$$ \\frac{\\Gamma \\vdash A : K \\;\\;\\text{and}\\;\\; \\Gamma, x: A \\vdash N :B}{\\Gamma \\vdash (\\lambda x: A \\mathop. N):(\\forall x : A \\mathop. B)} \\;\\; \\text{is function introdction} $$\n$$ \\frac{\\Gamma \\vdash M : (\\forall x : A \\mathop. B) \\;\\; \\text{and}\\;\\; \\Gamma \\vdash N:A }{\\Gamma \\vdash M(N) : B[x:=N]} \\;\\; \\text{is function elimination (but dependent!)} $$\n$$ \\frac{\\Gamma \\vdash M:A \\;\\;\\text{and} \\;\\; A=_\\beta B \\;\\;\\text{and}\\;\\; \\Gamma \\vdash B:K}{\\Gamma \\vdash M : B } $$\nIn CoC, the \"quantifiers\" are always bound, but the text of the article implies that the calculus of inductive constructions is impredicative, perhaps suggesting that CoC itself is too.\n\nSome of its variants include the calculus of inductive\nconstructions (which adds inductive types), the calculus of\n(co)inductive constructions (which adds coinduction), and the\npredicative calculus of inductive constructions (which removes some\nimpredicativity).", "output": "Impredicativity is one of those soft concepts that appears in many related forms, but it is difficult to explain what precisely they share. Let me try anyhow.\nImpredicativity allows us to single out, construct, or characterize a particular entity $e$ of some totality (set, type, universe) $T$ by quantification over all of $T$. Such constructions are sometimes considered problematic, especially when $T$ is a \"non-completed\", \"large\" or otherwise mysterious collection, say the class of all sets, or the subsets of an infinite set.\nIf we ignore the fears that haunt predicative mathematicians, there still remains the technical observation that impredicativity often greatly increases the logical strength of a formal system. In fact, unbridled impredicativity was the culprit for the early paradoxes of set theory.\nHere are some examples.\nThe axiom schema of separation\nIn set theory we may define an element of the class $\\mathsf{Set}$, i.e. a set, by quantifying over the entire class $\\mathsf{Set}$ using unbounded quantifiers. For example,\n$$\\{x \\in P(\\emptyset) \\mid \\forall y . y \\times x \\cong y\\}$$\nis a silly way of constructing the set $\\{\\{\\emptyset\\}\\}$ (sorry, I cannot think of a real example right now, but I am sure set theorists can come up with essential uses of unbounded separation). This is considered impredicative because $y$ ranges over all of $\\mathsf{Set}$, including the set that is being defined.\nIt should be clear that bounded separation $\\{x \\in A \\mid \\phi\\}$ in which all quantifiers appearing in $\\phi$ are bounded (of the form $\\forall y \\in S$ and $\\exists y \\in S$) may be considered predicative since the sets over which the quantifiers range have been constructed already (or else we could not mention them).\nThe related principle of unbounded comprehension, stating that for any predicate $\\phi$ there is a set $\\{x \\mid \\phi\\}$ is of course a form of impredicativity. In fact, this form of impredicativity is too strong, as it leads to Russell's paradox. It shows that impredicativity is not to be trifled with.\nComplete lattices\nA poset $(P, {\\leq})$ is a complete lattice when it is closed under arbitrary suprema (in which case it is also closed under arbitrary infima).\nThis is impredicative because it allows us to obtain an element $x \\in P$ by quantifying over a subset $S \\subseteq P$ of which $x$ may be an element (\u201c$x$ is supremum of $S$\u201d quantifies over $S$). The prime example is Tarski's fixed-point theorem: given a monotone map $f : P \\to P$, the least fixed-point $x$ of $f$ is defined as\n$$x \\mathbin{{:}{=}} \\sup \\{ y \\in P \\mid y \\leq f(y) \\}.$$\nThis definition of $x$ is impredicative because $x = f(x)$ and so $x$ is the element of the set that was used to specify $x$.\nPowersets are complete lattices and thus may be considered a source of impredicativity. It should be noted that as soon as the powerset of the singleton $\\{\\star\\}$ exists, all powersets exist because $P(A) \\cong P(\\{\\star\\})^A$. So the haunting is already done by the smallest non-trivial powerset. This is why topos theory is impredicative at its core: the subobject classifier $\\Omega$ is precisely $P(\\{\\star\\})$.\nImpredicativity in type theory\nIn type theory quantification is carried out by products. An impredicative universe $U$ is thus one that is closed under quantification over $U$:\n$$\n\\frac{\\vdash F : U \\to U}{\\vdash (\\Pi (A {:} U) \\,.\\, F A) : U}\n$$\nAn example is the universe of propositions $\\mathsf{Prop}$ which is closed under arbitrary products:\n$$\n\\frac{\\vdash A \\, \\mathsf{type}\n\\quad\nx {:} A \\vdash P(x) : \\mathsf{Prop}\n}{\\vdash \\forall x {:} A . P(x) : \\mathsf{Prop}}\n$$\nThe universe $\\mathsf{Prop}$ is impredicative because as a special case of the above we may quantify over $\\mathsf{Prop}$ itself. This is very useful, of course, as it allows us to define logical connectives by Church encodings, e.g.,\n$$p \\lor q = \\forall r {:} \\mathsf{Prop} \\,.\\, (p \\to r) \\to (q \\to r) \\to r.$$\nThe above definition is impredicative because the meaning of $p \\lor q$ is given by quantification over all propositions, one of which is $p \\lor q$ itself.\nSystem F is impredicative because it allows a type to be defined by quantification over all types, in much the same way as the impredicative encodings in $\\mathsf{Prop}$. For related reasons second-order Peano arithmetic is considered impredicative (a predicate may be defined by quantification over all predicates).\nA universe $U$ that contains itself and is closed under products is impredicative \u2013 and this is the case of impredicativity having gone too far because it leads to an inconsistency.\nPredicativity\nThose who feel uneasy about impredicativity, and those who want to carefully callibrate the logical strength of their formal systems, look for ways of avoiding impredicativity. In general this works by replacing self-referential constructions with iterative and inductive constructions which \"build the object from below\".\nFor example, Tarski's theorem (see above) may be proved as follows: give a monotone map $f : L \\to L$, its least fixed point is the limit of the ordinal-indexed chain\n\\begin{align*}\nx_{\\beta}  &\\mathrel{{:}{=}} \\sup \\{ f(\\alpha) \\mid \\alpha < \\beta \\} \n\\end{align*}\nWe have replaced impredicativity with transfinite recursion on ordinals \u2013 and in any specific case only a set-sized amount of such recursion is required.\nIn type theory impredicativity is typically avoided by introduction of universe levels, so that quantification over a universe $U_k$ yields a type in the next universe $U_{k+1}$.\nOne might still wonder whether some impredicativity is involved even in the most innocuous looking constructions. For example, when we replace the Church encoding of $p \\lor q$ with direct-style inference rules, we might write the elimination rule as\n$$\n\\frac{p \\vdash r \\qquad q \\vdash r}{p \\lor q \\vdash r}\n$$\nIs this not quantification over all propositions $r$, just as before?\nNo, there is an important difference: in the case of the Church encoding of $p \\lor q$ the quantification was internal to the formal system (we used a quantifier that is one of the constructors of type theory), whereas the above rules use external or meta-level or schematic quantification. The latter has much less of a punch.", "meta": {"post_id": 326, "input_score": 37, "output_score": 35, "post_title": "What is predicativity?"}}
{"input": "I know of a handful of automated theorem provers for classical first-order logic such as Vampire (source code).\nInternally, I think most of these provers work by translating premises and the negated goal into disjunctions of literals and then applying resolution. This also requires Skolemization as a preprocessing step, which removes existential quantifiers in the prenex of a wff and adds new function symbols to the language. Here's a link to the the Skolemization implementation in Vampire.\nThis technique will not work in a constructive setting. Negating the goal, converting to negative normal form and then prenex normal form is not constructively valid.\nWhat automated theorem provers for intuitionistic first-order logic are there (whether they're capable of emitting proof terms or not)? If there are any, broadly speaking, what strategy do they use?", "output": "The firstorder tactic in Coq is an \"experimental extension of tauto to first-order reasoning.\"  The  tauto tactic \"implements a decision procedure for intuitionistic propositional calculus based on the contraction-free sequent calculi LJT* of Roy Dyckhoff [Dyc92]\".  Perhaps one of the Coq developers can speak to the theory behind firstorder.\nThese tactics do not rely on classical axioms, and do generate proof terms.", "meta": {"post_id": 347, "input_score": 15, "output_score": 10, "post_title": "Are there automated theorem provers for constructive logics? What strategy do they use?"}}
{"input": "I already know that refl is called a tactic, and that rfl is a term; can you explain with examples how they technically differ? I've read that refl is \"more powerful\", but I never knew beforehand if one would definitely work where the other may not. Bonus points for by exact rfl explanations. I also wonder if there are significant changes to these between different versions of Lean 3...", "output": "Separately from the rfl (term) vs refl (tactic) distinction, there is also the distinction between rfl and refl in lemma names:\n#check @eq.refl  -- \u2200 {\u03b1 : Sort u_2} (a : \u03b1), a = a\n#check @rfl      -- \u2200 {\u03b1 : Sort u_2} {a : \u03b1}, a = a\n\n#check @iff.refl -- \u2200 (a : Prop), a \u2194 a\n#check @iff.rfl  -- \u2200 {a : Prop}, a \u2194 a\n\n#check @le_refl  -- \u2200 {\u03b1 : Type u_2} [_inst_1 : preorder \u03b1] (a : \u03b1), a \u2264 a\n#check @le_rfl   -- \u2200 {\u03b1 : Type u_2} [_inst_1 : preorder \u03b1] {a : \u03b1}, a \u2264 a\n\nThe difference here is that that rfl uses an implicit {} binder for a, while refl uses a explicit () binder. So iff.rfl is shorthand for iff.refl _, le_rfl is shorthand for le_refl _, etc.", "meta": {"post_id": 409, "input_score": 19, "output_score": 9, "post_title": "What is the difference between refl and rfl in Lean 3?"}}
{"input": "I need to work with the set of all lines in the Cartesian plane. For my context, the natural way to think of this is that a line can be described by an equation $Ax + By + C = 0$, where $A$ and $B$ cannot both be zero. So a natural representation is as triples $(A,B,C)$ where $A$ and $B$ are not both zero. But the lines characterized by $(A,B,C)$ and by $(7A, 7B, 7C)$ are the same, so I need a quotient. To be explicit, I want to look at the set $Y$ defined by\n\\begin{align}\nX &= \\Bbb R^2 - \\{(0,0)\\}\\\\\nY &= X \\times \\Bbb R\n\\end{align}\nand the equivalence relation on $Y$ where $((A,B), C) \\sim ((P, Q), R)$ if and only if there's a real number $k$ such that $P = kA, Q = kB, R = kC$. Equally good would be to say that\n$$\nY = \\Bbb R^3 - \\{(0,0,C) \\mid C \\in \\Bbb R \\},\n$$\nbut that seemed as if it might be more difficult to express in Lean.\nThose who like geometry will recognize that my set $Y$ is just $\\Bbb RP^2$ minus one point, i.e., it's topologically a Mobius band (with \"open\" edges).\nOnce I have a description of $Y$, preferably as a type, I can write down the equivalence relation and prove it's an equivalence and even construct the quotient.\nCan someone point me in the right direction to defining $Y$ (or even $X$) as a type? (As you may have guessed, I'm a beginner at Lean.)", "output": "Let me answer your immediate question first with the following code snippet (which relies on mathlib):\nimport data.real.basic\nimport data.matrix.notation\n\ndef X := { v : fin 2 \u2192 \u211d // v \u2260 0 }\n\n-- The first def for `Y`.\ndef Y := X \u00d7 \u211d\n\n-- The second def for `Y`.\ndef Y' := { v : fin 3 \u2192 \u211d // v 0 \u2260 0 \u2228 v 1 \u2260 0 }\n\n-- Alternative for `Y'`, using matrix notation:\ndef Y'' := { v : fin 3 \u2192 \u211d // \u2200 c : \u211d, v \u2260 ![0,0,c] }\n\nSince projective spaces are interesting objects as well, I think it would be worthwhile to first define $\\mathbb{RP}^2$, or more generally, $\\mathbb{P}^n(k)$, where $k$ is any field, or even better, $\\mathbb{P}_k(V)$, the projectivization of the $k$-vector space $V$. Once projective spaces are defined as a type, the next steps would be to build up the API to make them usable! Finally, remove a point from $\\mathbb{P}_{\\mathbb{R}}(\\mathbb{R}^3)$ to obtain your desired object.\nFor the initial definition of $\\mathbb{P}_k(V)$, I see essentially two ways to do this:\n\nConsider the type of all one-dimensional subspaces of $V$.\nTake a quotient of $V \\smallsetminus \\{0\\}$ by the equivalence relation given by multiplication by elements of $k^\\times$.\n\nThe first approach could be defined as\nimport linear_algebra.finite_dimensional\n\nvariables (k V : Type*) [field k] [add_comm_group V] [module k V]\n\n-- Note that an infinite dimensional space has `finrank = 0`, by convention.\ndef projectivization := { M : submodule k V // finite_dimensional.finrank k M = 1 }\n\nwhile the second can be defined as follows (modulo the missing proof):\nimport linear_algebra.finite_dimensional\n\nvariables (k V : Type*) [field k] [add_comm_group V] [module k V]\n\n-- Since `y` is nonzero by assumption, `c \u2022 (x : V) = y` will force `c \u2260 0` as well.\ndef projectivization.setoid : setoid { v : V // v \u2260 0 } := \n{ r := \u03bb x y, \u2203 c : k, c \u2022 (x : V) = y,\n  iseqv := sorry }\n\ndef projectivization := quotient (projectivization.setoid k V)\n\nNote that both of these code snippets rely on mathlib, and, of course, one could easily change the implementations (e.g. replacing the finrank condition by an equivalent Prop.\nI hope these are useful to get started, and additional help is always available on the Lean zulip.", "meta": {"post_id": 418, "input_score": 13, "output_score": 11, "post_title": "Representing $\\Bbb RP^2$ in Lean: building a type representing a particular set"}}
{"input": "TL;DR: How / where to formalize results concerning the logical strength of systems? Are proof assistants having a weak base theory but also enough infrastructure to make it feasible?\n\nI'll start by giving some context. In studying Logic and foundations, a clear interest is the consistency of the set of axioms that you use; especially when showing that some statement is \u201cindependent\u201d (or simply doesn't follow) from others. The cardinal example (pun slightly intended) of this is the Continuum Hypothesis (CH) that since Cohen we know that it can't be proved nor disproved with current mathematical methods.\nThe catch, obviously is that \u201ccurrent mathematical methods\u201d above corresponds to \u201cZFC set theory\u201c (for which it was proved), and even by adding large cardinal assumptions (which include, by translation, CIC with universes). But if those foundations are self-contradictory, the previous paragraph is wrong. The correct reading is: If ZFC is consistent, then it doesn't prove CH; to the effects of the present question, the interesting version is: If ZFC is consistent, so is ZFC + \u00acCH.\nThere are other equiconsistency results as that. For example, the following theories are equiconsistent:\n\nZF + Choice for countable families + \u201cEvery subset of $\\mathbb{R}$ is Lebesgue measurable\u201d;\nZFC + \u201cthere exists an inaccessible cardinal\u201d (i.e., there exists a set universe).\n\nIn order to be able to formalize this theorem in a reasonable context, the underlying logic of the proof assistant should be at least weaker that ZFC plus one universe. Same goes for the independence of CH. On the other hand, proof assistants like Lean (and Coq?) assume a sequence of universes from the start; also, Isabelle/HOL has a session devoted to set theory, but its consistency strength is that of statement 2 above.\nThe question arises even for Isabelle/ZF if you want to work with theories weaker than ZFC. In this state of affairs, I would like to know if there is any proof assistant that has a very weak foundation, that allows to work smoothly with theories of varying strength.\nBy searching the site, I found a related question asking for the formalization of model theory, and another relevant one\nmentioning also other systems of different strength (Peano / Primitive Recursive Arithmetic / Bounded Arithmetic, systems of reverse mathematics). But one point to be considered is that if one starts with plain PA / PRA then you have to code all the reasoning inside it, and usually tactics work only at the surface.", "output": "You could address your goals by using a generic proof assistant, one that supports user-definable theories, such as Isabelle and Metamath, and I am sure there are others. These allow you to define your own weak formal system and work with it.\nHowever, once you define your favorite formal system, you will have a bare-bones proof assistant that will only become usable once you implement a substantial library of basic facts and constructions \u2013 and that is a rather daunting task. We need to learn how to lower the bar to having domain-specific proof assistants.\nAnother possibility is to use a popular proof assistant that supports a very strong formalism, but to carefully avoid its power. While not entirely satisfactory, doing so will allow you to actually get things done.\nSpecifically, suppose you want to show that if $\\mathrm{ZFC} + X$ is consistent then so is $\\mathrm{ZFC} + Y$, and you'd like to do this by demonstrating that a model of $\\mathrm{ZFC} + X$ yields a model of $\\mathrm{ZFC} + Y$. In whatever proof assistant you like, formalize the basics of Tarskian semantics: the syntax and logic of the first-order theory $\\mathrm{ZFC}$, $\\in$-structures, models of $\\mathrm{ZFC}$ etc. Then you prove a theorem\nZFCX\u21d2ZFCY : \u03a0 (M : \u2208-structure) . (M \u22a8 ZFC+X) \u2192 \u03a3 (N : \u2208-structure) . (N \u22a8 ZFC+Y)\nZFCX\u21d2ZFCY = \u2329proof-goes-here\u232a\n\nSince your proof assistant is too powerful, it allows you to just construct a model N of ZFC+Y from scratch, but you shouldn't do that. Instead you resist the Dark Side and use forcing to construct N from M in the weak fragment that you wish you were working in.\nThis is not ideal but is still well-worth doing. Think of it as partial formalization. You have formalized the desired construction, but you still have to manually check the proof to verify that it remains within the confines of a weak fragment (this may not be so simple if you use the standard library and lose track of what the library is doing). People actually do such things for lack of a better method. For example, the UniMath library formalizes mathematics in Univalent foundations on top of the assumption Type : Type \u2013 which is inconsistent! The authors review the code carefully and avoid dangerous moves. (I have it on good authority that the inconsistent assumption keeps them awake at nights.)", "meta": {"post_id": 425, "input_score": 13, "output_score": 9, "post_title": "Attempts to accommodate theories of different consistency strength in single assistant"}}
{"input": "In the early 2000s (or maybe even earlier) Freek Wiedijk published a list of 100 theorems which were a sort of litmus test of the state of the art in formalized mathematics.  As the completion rate nears a stable point, I want to ask the community's reflection on the list and its future.\n\nHas Freek's list been a positive impetus to the community?  Especially for the (much?) smaller half of the community focusing on developing libraries of formal mathematics?\nAs this list is nearly complete (save for FLT), what next?  Do we need a new list, or something similar, to bring visibility to formal theorem proving, encourage growth, compare tools, and measure progress in the field?\nIf so, what would that next thing look like?  Would it be just mathematics?  Mathematics and software?  Is there any hope that one could get buy-in from lots of proof assistant communities, or are their respective aims just too different?  Who would maintain it?  Or should each community just maintain their own lists according to their own standards?\n\nEdit: I am mostly just asking if we need a new list or something similar to replace it, not asking what the next big thing in all of formal theorem proving needs to be.  The immediate motivation for this question was a discussion about if there is any meaningful way to compare the various libraries of formal mathematics since they have different methods of counting theorems and some focus more on pure mathematics than others.  While any individual or any proof assistant community can make their own list, I wanted to use this forum involving many different proof assistants to discuss if there is any agreement on this matter.  I wrote the question a bit more broadly since I thought this list was a good way to measure progress in the field and show what can be done.", "output": "Freek's list served a very helpful purpose. It's nice to have a list of standard challenges which are recognisable to those outside the community, and its near-completion makes it very clear that it is possible to formally prove interesting theorems.\nThat said, we do not need another list. The point -- that it is possible to do serious mathematics in proof assistants -- has been made, and there's no sense in making the same point over and over again.\nNow that we know it is possible, the next problem is showing that mechanised mathematics is convenient. Plenty of working mathematicians use tools like SageMath and Mathematica, and they do so because they makes computations, plotting, and algebraic manipulations much faster and more reliable. But very few people use Coq because it makes proofs easier[1]. For sensible mathematics, it is only a mild exaggeration to say proof assistants are actually proof obstructants!\nIn my view, there are two major intertwined problems.\n\nWe simply do not know how to build large and usable libraries of mechanised mathematics. A working mathematician has a large body of knowledge and standard facts at her fingertips, and can use them straightforwardly in her work. This is partly a matter of lacking experience, and partly a matter of not knowing which features are missing from our proof assistants.\nFor example, the work in Coq on HierarchyBuilder is a good example of adding features to a proof assistant to support the needs of large-scale verification. Conversely,the Lean mathlib project is a very important attempt to work out how to design a large corpus (\"the undergrad curriculum\") by actually doing it.\n\nNo proof assistant has anything approaching a predictable envelope of operation.\nComplex tools need to have a clear and teachable set of principles of operation. Without a good mental model, it's very hard for a human operator to figure out which things they try will work, how things can go wrong, and how to repair errors when they do occur.\nAt the moment, there are basically no proof assistants which satisfy this criterion. You basically have to understand the exact internals of the implementation to handle complex problems.\nAnd while one might be tempted to think this is impossible for decidability/complexity reasons, I\nthink this is wrong.\nFor example, SMT solvers actually do satisfy this criterion. If you stick to using quantifier-free combinations of decidable theories, they basically just work, even though the problems are NP-complete in general.\n\n\n[1] It really does make your life easier when doing formal verification. If you need to revise the induction hypothesis on a seven-bajillion case induction over all the instructions, addressing modes, and protection levels of the ARM instruction set or something, then a proof assistant will save your life.", "meta": {"post_id": 436, "input_score": 21, "output_score": 16, "post_title": "Impact of \"Formalizing 100 Theorems\" and what is next"}}
{"input": "In Agda with --guardedness, we can define productive definitions using copatterns like the following\nrepeat : \u2200 {A} -> (a : A) -> Stream A\nrepeat a .head = a\nrepeat a .tail = repeat a\n\nGiven the coinductive definition of Stream. Similarly we can define zipWith:\nzipWith : \u2200 {A B C} -> (A -> B -> C) -> Stream A -> Stream B -> Stream C\nzipWith f a b .head = f (a .head) (b .head)\nzipWith f a b .tail = zipWith f (a .tail) (b .tail)\n\nHowever, if we want to go further and define the Fibonacci stream:\nfib : Stream Nat\nfib .head = 0\nfib .tail .head = 1\nfib .tail .tail = zipWith _+_ fib (fib .tail)\n\nAgda productivity checker will complain that the last clause is not (structurally) productive. In my impression, the recommended way to make fib work is to introduce a size parameter to the definition of Stream. However, since sized types are incompatible with guardedness, we have to completely get rid of guardedness if we want to use sized types (or give up --safe, which is undesirable). So, I wonder if there's a way to make fib work with guardedness (which means, no sized types)?", "output": "I recommend the state machine definition that we'd use for looping over Fibonacci numbers:\nfib' : \u2115 \u2192 \u2115 \u2192 Stream \u2115\nhead (fib' x y) = x\ntail (fib' x y) = fib' y (x + y)\n\nfib : Stream \u2115\nfib = fib' 0 1\n\nThe classic Haskell zipWith definition is more intended to show off several language features at once, and it's both less efficient and harder to reason about than the above version.\nIt's a good general approach to try to figure out the state that has to remembered and updated on each unfolding, when we try to convert complicated coinduction to versions that are more palatable to Agda.", "meta": {"post_id": 446, "input_score": 8, "output_score": 8, "post_title": "Is it possible to define `fib` coinductive stream w/o sized types?"}}
{"input": "How close can you get to introducing and discharging hypotheses in metamath?\nI'm trying to define a symbol formal system for the $\\land$-only fragment of classical propositional logic.\nIt has the following rules.\n$$ \\frac{A \\;\\;\\text{and}\\;\\; B}{A \\land B} \\;\\;\\text{and}\\;\\; \\frac{A\\land B}{A} \\;\\; \\text{and} \\;\\; \\frac{A \\land B}{B} \\;\\; \\text{are the inference rules} $$\nI'm interested in proving a simple statement $P \\land Q \\vdash Q \\land P$.\nHere's my attempt to prove it so far in metamath. It is semi-syntactically correct, failing at the verify proof step in the command line interface.\nI'm working using demo0.mm as an example.\nHere's what I have so far.\n$( define 1 (top) 0 (bottom) and * (conjunction)\n   Also define |- entailment $)\n\n$c 1 0 * wff |- ( ) $.\n\n$( define metavariables P,Q,R $)\n\n$v P Q R $.\n\nwff-P $f wff P $.\nwff-Q $f wff Q $.\nwff-R $f wff R $.\n\nwff-conj-P-Q $a wff ( * P Q ) $.\n\n$( inference rule 1: conjunction intro:\n\n   A and B\n   -------\n    * A B\n$)\n\n${\n   conj-intro-min  $e |- P $.\n   conj-intro-maj  $e |- Q $.\n   conj-intro      $a |- ( * P Q ) $.\n$}\n\n$( inference rules 2 and 3 : conjunction elim $)\n\n${\n   pi1-prem   $e |- ( * P Q ) $.\n   pi1        $a |- P $.\n$}\n${\n   pi2-prem   $e |- ( * P Q ) $.\n   pi2        $a |- Q $.\n$}\n\n$( prove that ( ( * P Q ) ) |- ( * Q P ) $)\ncommutative-conj $p ( ( * P Q ) ) |- ( * Q P ) $=\nwff-P\nwff-Q\nwff-conj-P-Q\n$.\n\nThe proof of commutative-conj is very, very incomplete and does not work.\nI know that metamath is vaguely Hilbert-style and ... I'm pretty sure I can't just introduce a hypothesis.\nSo, I tried to do the next best thing and write a rule that contains a free variable in its conclusion that wouldn't unify with anything.\n$( An extremely weak form of monotonicity of entailment\n\nIf  |- A  holds, then  B |- A  holds.\n\n$)\n${\n   weak-mono-entailment-prem $e       |- P $.\n   weak-mono-entailment      $a ( Q ) |- P $.\n$}\n\nHowever, this approach seems to require me to assert more and more ad hoc facts about $\\vdash$ in order to make any progress at all ...\nIs there a better, more idiomatic way to mimic introduction and discharge of hypotheses?", "output": "Concerning your actual proof attempt:\nHere is the completed proof:\n${\n   commutative-conj-prem $e |- ( * P Q ) $.\n   $( prove that ( ( * P Q ) ) |- ( * Q P ) $)\n   commutative-conj $p |- ( * Q P ) $=\n     wff-Q wff-P wff-P wff-Q commutative-conj-prem pi2 wff-P wff-Q\n     commutative-conj-prem pi1 conj-intro $.\n$}\n\nJason has already explained the changes needed to the theorem statement itself.\nHere is how the Metamath program displays the proof:\nMM-PA> SHOW PROOF commutative-conj\n 5     pi2-prem=commutative-conj-prem $e |- ( * P Q )\n 6   conj-intro-min=pi2             $a |- Q\n 9     pi1-prem=commutative-conj-prem $e |- ( * P Q )\n10   conj-intro-maj=pi1             $a |- P\n11 commutative-conj=conj-intro    $a |- ( * Q P )\n\n\nStep 5, it invokes commutative-conj-prem,\nStep 6, it invokes pi2, with step 5 as a premise,\nStep 9, it invokes again commutative-conj-prem,\nStep 10, it invokes pi1, with step 9 as a premise,\nStep 11, if finally invokes conj-intro, with steps 6 and 10 as premises.\n\nYou'll notice some steps are not shown. These are used to build the expressions which are used as substitutions. In your case, those are invocations of wff-p and wff-q. They can be shown using the SHOW PROOF commutative-conj / ALL command.\nHere is the way the proof can be built with metamath-exe:\nMM> PROVE commutative-conj\nEntering the Proof Assistant.  HELP PROOF_ASSISTANT for help, EXIT to exit.\nYou will be working on statement (from \"SHOW STATEMENT commutative-conj\"):\n21 commutative-conj-prem $e |- ( * P Q ) $.\n22 commutative-conj $p |- ( * Q P ) $= ... $.\nUnknown step summary (from \"SHOW NEW_PROOF / UNKNOWN\"):\n1    commutative-conj=? $? |- ( * Q P )\nMM-PA> ASSIGN 1 conj-intro\n3 -1   conj-intro-min=?            $? |- Q\n4      conj-intro-maj=?            $? |- P\nMM-PA> ASSIGN 3 pi2\n5 -1     pi2-prem=?                  $? |- ( * $4 Q )\n7      conj-intro-maj=?            $? |- P\nMM-PA> ASSIGN 5 commutative-conj-prem\n7      conj-intro-maj=?               $? |- P\nMM-PA> ASSIGN 7 pi1\n 9        pi1-prem=?                     $? |- ( * P $6 )\nMM-PA> ASSIGN 9 commutative-conj-prem\nMM-PA> IMPROVE ALL\nA proof of length 1 was found for step 8.\nA proof of length 1 was found for step 7.\nA proof of length 1 was found for step 4.\nA proof of length 1 was found for step 3.\nA proof of length 1 was found for step 2.\nA proof of length 1 was found for step 1.\nSteps 1 and above have been renumbered.\nCONGRATULATIONS!  The proof is complete.  Use SAVE NEW_PROOF to save it.\nNote:  The Proof Assistant does not detect $d violations.  After saving\nthe proof, you should verify it with VERIFY PROOF.\n\n(this is the full output, the necessary commands are those after the interactive prompt MM-PA>)\nMore generally:\nIf you would like a proof format which has hypotheses one the left, before the turnstile, it is also possible to define it, see for example Mario's HOL database, where your statement would be written something like:\nancom $p |- ( R , S ) |= ( S , R ) $=\n\nThat's probably as close as you can get.\nIn practice, in the set.mm database, it common to use a \"deduction style\" format for proofs, where the implication -> takes the role of the inference rules, and a generic ph takes the role of the context. Then pm3.22 would correspond to your theorem. Its statement is written:\npm3.22 $p |- ( ( ph /\\ ps ) -> ( ps /\\ ph ) ) $=\n\nBut of course, from a proof theoretical point of view, that's a different statement.", "meta": {"post_id": 447, "input_score": 8, "output_score": 8, "post_title": "What is the closest you can get to introducing and discharging hypotheses in metamath?"}}
{"input": "What is the difference between intensional vs extensional type theories and how come the type checking is undecidable for extensional type theory? Also, how does it affect the expressiveness of theorem provers?", "output": "Extensional type theory is characterized by the reflection rule, which says that if the identity type ${\\rm Id}(a,b)$ is inhabited, then $a\\equiv b$ ($a$ and $b$ are judgmentally equal).  It is called extensional type theory because this means that the judgmental equality coincides with the identity type, and the latter is extensional (or, at least, more extensional than the judgmental equality would be in the absence of the reflection rule --- just how extensional it is depends on whether you have principles like function extensionality and univalence).  Intensional type theory is so-called because its judgmental equality is intensional, whereas its identity types can be even more extensional than those in extensional type theory (because the reflection rule is incompatible with the \"strongest extensionality principle\", namely univalence).\nIn a dependent type theory, type-checking is complicated because of the conversion rule that if $a:A$ and $A\\equiv B$ then $a:B$.  This essentially requires that a type-checking algorithm must include an algorithm for checking judgmental equality.  When combined with the reflection rule, this means a type-checking algorithm would have to include an algorithm for checking inhabitation of a type (namely the identity type).  But inhabitation of types can be used to encode the truth of arbitrary mathematical statements, so (e.g. by the halting problem) it is impossible to have a terminating algorithm for checking inhabitation of types, and hence impossible to have a terminating type-checking algorithm for extensional type theory.\nThere is a theorem, due I believe to Martin Hofmann (Conservativity of Equality Reflection over Intensional Type Theory), that extensional type theory is conservative over intensional type theory extended by axioms of uniqueness-of-identity-proofs and function extensionality.  Thus, there is no difference in the \"expressiveness\" of these theories.  Given the undecidability of type-checking for extensional type theory, therefore, very few proof assistants have tried to implement it.", "meta": {"post_id": 470, "input_score": 9, "output_score": 9, "post_title": "Type Checking Undecidable in Extensional Type Theory"}}
{"input": "How can I prove that the matched pattern is equal to the original argument?\nFor example, in the following function, what can I write instead of sorry to prove the equality?\ndef e1 (x: Option Nat): Nat :=\n  match x with\n  | none => 1\n  | some y =>\n    have : some y = x := sorry\n    y+1\n\n\nThe only option I could come up with was using a pair with an equality proof, but that seems a bit too complex:\ndef e2 (x: Option Nat): Nat :=\n  let x' := x\n  have x'_eq: x' = x := rfl\n  match x', x'_eq with\n  | none, _ => 1\n  | some y, y_eq =>\n    have : some y = x := y_eq\n    y+1\n\nIs there an easier way to do this?", "output": "In Lean 3, you can solve this using the following syntax:\ndef e1 (x : option \u2115) : \u2115 :=\nmatch _, rfl : \u2200 y, y = x \u2192 _ with\n| none, h := 1\n| some y, h :=\n  have some y = x := h,\n  y+1\nend\n\nThe match x, y : T with syntax means \"generate a function of type T called e1._match or similar, and then replace this match expression with e1._match x y\". T is usually therefore of the form X \u2192 Y \u2192 _, where _ is the type of the values within the match expression.\nNote that this is essentially the same as your solution, but the match expression lets us combine it all into one line.", "meta": {"post_id": 485, "input_score": 7, "output_score": 9, "post_title": "Preserve equality with match expression"}}
{"input": "Theorem to prove:\n\nThe sum of the binomial coefficients over an antidiagonal is a Fibonacci number. More specifically, the $n$th antidiagonal sums to the $n+1$th Fibonacci number, where the antidiagonal is counted from zero and the Fibonacci sequence starts with $F_0 = 0$ and $F_1 = 1$.\n$$ \\forall\\, n \\in \\mathbb{N}, \\quad \\sum_{i=0}^{n}{{i}\\choose{n-i}} = F_{n+1} $$\n(from this code golf challenge)\n\n\nImage source: Math is Fun - Pascal's triangle\nBy digging through the mathlib I could formulate this equality as the following:\ntheorem antidiagonal_choose_sum_is_fib (n : \u2115) :\n  ((list.nat.antidiagonal n).map (function.uncurry nat.choose)).sum\n  = n.succ.fib\n\nwith the proof:\nimport data.nat.choose.basic\nimport data.nat.fib\nimport data.list.defs\nimport data.list.nat_antidiagonal\n\nlemma fib_ind : forall (P : \u2115 -> Prop) (p0 : P 0) (p1 : P 1) (pss : forall m : \u2115, P m -> P m.succ -> P m.succ.succ), forall n : \u2115, P n :=\nbegin\n  intros P p0 p1 pss n,\n  have pns : P n /\\ P n.succ,\n  induction n, tauto, tauto, tauto\nend\n\nlemma sum_zip : forall (f g : \u2115 -> \u2115) (l : list \u2115), (list.map f l).sum + (list.map g l).sum = (list.map (\u03bbx, f x + g x) l).sum :=\nbegin\n  intros, induction l,\n  simp,\n  simp, rw <- l_ih, ring,\nend\n\nlemma sum_eq_elementwise : forall (f g : \u2115 -> \u2115) (l : list \u2115), (forall x, x \u2208 l -> f x = g x) -> (list.map f l).sum = (list.map g l).sum :=\nbegin\n  introv, induction l,\n  simp,\n  intro h, simp, rw h l_hd, rw l_ih,\n    introv mem_tl, apply h, simp, tauto,\n    simp,\nend\n\ntheorem antidiagonal_choose_sum_is_fib (n : \u2115) :\n  ((list.nat.antidiagonal n).map (function.uncurry nat.choose)).sum\n  = n.succ.fib :=\nbegin\n  induction n using fib_ind with n_m n_ind0 n_ind1,\n  simp,\n  simp,\n  rw nat.fib_add_two, rw nat.add_one, rw <- n_ind0, rw <- n_ind1,\n  unfold list.nat.antidiagonal, simp, repeat {rw nat.add_one},\n  rw list.sum_range_succ, rw list.sum_range_succ', rw list.sum_range_succ _ (n_m.succ), simp,\n  rw <- add_assoc, rw sum_zip, simp, apply sum_eq_elementwise,\n  introv in_range, rw list.mem_range at in_range, have le_n_m := nat.succ_sub (nat.le_of_lt_succ in_range),\n  rw le_n_m, apply nat.choose_succ_succ\nend\n\nOnline demo is here.\nProof outline:\n\nTheorem: $ \\forall\\, n \\in \\mathbb{N}, \\quad \\sum_{i=0}^{n}{{i}\\choose{n-i}} = F_{n+1} $.\nUse Fibonacci induction.\nFor $n = 0$ and $n = 1$, use direct evaluation.\nFor $n = k+2$, assume the equation holds for $n = k$ and $n = k+1$.\nExpand $F_{k+3}$ once and substitute the two equations to get $$ \\sum_{i=0}^{k+2}{{i}\\choose{k+2-i}} = \\sum_{i=0}^{k}{{i}\\choose{k-i}} + \\sum_{i=0}^{k+1}{{i}\\choose{k+1-i}} $$\nChop off the first and last terms of the leftmost summation and the last of the rightmost one to get $$ \\sum_{i=0}^{k}{{i+1}\\choose{k+2-(i+1)}} + {{0}\\choose{k+2}} + {{k+2}\\choose{0}} = \\sum_{i=0}^{k}{{i}\\choose{k-i}} + \\sum_{i=0}^{k}{{i}\\choose{k+1-i}} + {{k+1}\\choose{0}} $$\nConstant terms evaluate to 1.\nHandle the sums element-wise: $$ \\forall\\, 0 \\le i \\le k, \\quad {{i+1}\\choose{k+1-i}} = {{i}\\choose{k-i}} + {{i}\\choose{k+1-i}} $$\nwhich follows from the identity of Choose.\n\nAs a proof review question, any advice to improve the proof is appreciated, including but not limited to:\n\na possible better formulation of the theorem\na possible better translation of the hand-written proof\na suggestion for a more readable formal proof\nnaming of lemmas and theorems (I'm aware that a naming convention exists in mathlib, but I'm not quite sure how it works)\npossible use of better lemmas in the course of the proof (is a more general lemma or a more specific lemma better?)", "output": "I'll first go through piece-by-piece and suggest some local improvements.\nFor the induction principle, when you find yourself doing intros as the first step of a tactic proof, you should strongly consider putting arguments \"before the colon\" rather than leaving them within universal quantifiers.  Another thing is that when you have multiple goals (for example, as produced by have or induction) it is good practice to use curly braces to focus on the goal -- that way when the proof breaks you can more easily figure out what needs fixing. And, a tip: if you are using the same tactic on each goal produced by a tactic, you can use the ; operator.\nlemma fib_ind (P : \u2115 \u2192 Prop)\n  (p0 : P 0) (p1 : P 1) (pss : \u2200 m : \u2115, P m \u2192 P m.succ \u2192 P m.succ.succ) (n : \u2115) : P n :=\nbegin\n  have pns : P n /\\ P n.succ,\n  { induction n; tauto },\n  tauto,\nend\n\nFor naming, we can refer to mathlib itself: it's nat.two_step_induction, so we can replace the tactic proof with the term nat.two_step_induction p0 p1 pss n. Note: the P argument would more commonly be given as an implicit argument.\nThe next lemma gets a similar treatment, but, with list.map, you can make use of dot notation: l.map f means \"since l has type list, use list.map and have l be its first list argument.\" Since this lemma is about a homomorphism property, I think it would normally be written with the equality flipped, and for naming, this might be unnecessarily long, but sum_map_apply_add_apply would work (one way mathlib names arise is to read off relevant parts of the in-order traversal of the abstract syntax tree).  Also, non-terminal simps should be avoided, since if the simp set later changes, you can have a hard time fixing a proof.  I used squeeze_simp to figure out which lemmas it was using, and I inserted *, which means \"and rewrite using hypothesis from the local context.\"\nlemma sum_map_apply_add_apply (f g : \u2115 \u2192 \u2115) (l : list \u2115) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nbegin\n  induction l,\n  { simp, },\n  { simp only [list.sum_cons, list.map, *],\n    ring, },\nend\n\nHowever, searching the documentation for \"list sum map add\", I came across multiset.sum_map_add. The proof in the source code (after interpreting what the to_additive attribute does) essentially carries over to lists. After generalizing it appropriately, this should probably find its way into mathlib:\nlemma list.sum_map_add {\u03b9 \u03b1 : Type*} [add_comm_monoid \u03b1] {f g : \u03b9 \u2192 \u03b1} (l : list \u03b9) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nlist.sum_hom\u2082 _ _ add_add_add_comm (add_zero _) _ _\n\nThe next lemma boils down to the fact that l.map f = l.map g (a congruence lemma), so searching for \"list map congr\" I got this:\nlemma sum_eq_elementwise (f g : \u2115 \u2192 \u2115) (l : list \u2115) (h : \u2200 x, x \u2208 l \u2192 f x = g x) :\n  (l.map f).sum = (l.map g).sum :=\nbegin\n  rw list.map_congr h,\nend\n\nFor the main theorem, there is a lot of unfolding of definitions. It's better to encapsulate those into some additional lemmas, since mathlib style is to try to avoid relying on definitions unless they're considered part of the interface (this isn't always clear).  Two lemmas that seem useful are for popping terms off the front and back of the antidiagonal:\nlemma list.nat.antidiagonal_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 1) =\n  ((list.nat.antidiagonal n).map (prod.map id nat.succ)) ++ [(n + 1, 0)] :=\nbegin\n  simp only [list.nat.antidiagonal, list.range_succ, add_tsub_cancel_left, list.map_append,\n    list.append_assoc, tsub_self, list.singleton_append, list.map_map, list.map],\n  congr' 1,\n  apply list.map_congr,\n  simp [le_of_lt, nat.succ_eq_add_one, nat.sub_add_comm] { contextual := tt },\nend\n\nlemma list.nat.antidiagonal_succ_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 2) =\n  (0, n + 2) :: ((list.nat.antidiagonal n).map (prod.map nat.succ nat.succ)) ++ [(n + 2, 0)] :=\nbegin\n  induction n with n ih,\n  { refl, },\n  { rw [nat.succ_add, list.nat.antidiagonal_succ, ih],\n    simpa, },\nend\n\nIn these, I'm (somewhat) misusing a feature of simpa, which is that it basically tries refl at the end. (These lemmas should also probably find their way into mathlib, but I'm not too familiar with the list antidiagonal functions. Edit: mathlib#12028 and mathlib#12029)\nWith these, we can simplify the main proof.  I flipped the equality because I imagine that the main use would be to take existing fibonacci numbers and rewrite them into this other form. I also changed n.succ to n + 1 since that tends to be the normal form for arithmetic expressions. By the way, it's common to open function, so uncurry doesn't need to be fully qualified.\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = ((list.nat.antidiagonal n).map (uncurry nat.choose)).sum :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, list.nat.antidiagonal_succ_succ', list.nat.antidiagonal_succ'],\n  simpa [\u2190add_assoc, \u2190list.sum_map_add, uncurry, \u2190nat.choose_succ_succ],\nend\n\n\nNow that we've gone through everything, let's put it all together. We have three lemmas that arguably should be in mathlib already, and then your proof is pretty close to what you said in your outline:\nimport data.nat.choose.basic\nimport data.nat.fib\nimport data.list.defs\nimport data.list.nat_antidiagonal\n\nopen function\n\nlemma list.sum_map_add {\u03b9 \u03b1 : Type*} [add_comm_monoid \u03b1] {f g : \u03b9 \u2192 \u03b1} (l : list \u03b9) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nlist.sum_hom\u2082 _ _ add_add_add_comm (add_zero _) _ _\n\nlemma list.nat.antidiagonal_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 1) =\n  ((list.nat.antidiagonal n).map (prod.map id nat.succ)) ++ [(n + 1, 0)] :=\nbegin\n  simp only [list.nat.antidiagonal, list.range_succ, add_tsub_cancel_left, list.map_append,\n    list.append_assoc, tsub_self, list.singleton_append, list.map_map, list.map],\n  congr' 1,\n  apply list.map_congr,\n  simp [le_of_lt, nat.succ_eq_add_one, nat.sub_add_comm] { contextual := tt },\nend\n\nlemma list.nat.antidiagonal_succ_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 2) =\n  (0, n + 2) :: ((list.nat.antidiagonal n).map (prod.map nat.succ nat.succ)) ++ [(n + 2, 0)] :=\nbegin\n  induction n with n ih,\n  { refl, },\n  { rw [nat.succ_add, list.nat.antidiagonal_succ, ih],\n    simpa, },\nend\n\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = ((list.nat.antidiagonal n).map (uncurry nat.choose)).sum :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, list.nat.antidiagonal_succ_succ', list.nat.antidiagonal_succ'],\n  simpa [\u2190add_assoc, \u2190list.sum_map_add, uncurry, \u2190nat.choose_succ_succ],\nend\n\n\nAll that said, using lists isn't the usual way you work with sums in mathlib. Instead, you work with finset and the \"big operators.\" Contingent on the mathlib PR for manipulating antidiagonals, the theorem ends up being just this:\nimport data.finset.nat_antidiagonal\nimport data.nat.fib\nimport algebra.big_operators.basic\n\nopen_locale big_operators\nopen finset\n\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = \u2211 p in nat.antidiagonal n, nat.choose p.1 p.2 :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, nat.antidiagonal_succ_succ', nat.antidiagonal_succ'],\n  simp [nat.choose_succ_succ, add_assoc, add_left_comm, sum_add_distrib],\nend\n\nAfter doing the basic manipulations you described in your proof outline, simp ends up doing the heavy lifting. An interesting thing about this tactic is that it's able to put things into a normal form even if the lemmas can potentially form rewrite loops. The add_assoc and add_left_comm lemmas here are giving simp the capability of sorting the additions, as a weak kind of ring. (Usually you'd include add_comm, too, but it wasn't necessary here.)", "meta": {"post_id": 511, "input_score": 14, "output_score": 14, "post_title": "Proof review: Sum of nCk over antidiagonal = Fibonacci"}}
{"input": "The $\\lambda\\mu$-calculus is a variant of the $\\lambda$ calculus introduced by Parigot to capture classical logic. The Wikipedia article describing it in more detail is here. In modern proof assistants, it is expected that the user should opt-in to breaking canonicity via adding their own axioms if they want to enjoy classical reasoning.\nThere is an alternative world though, where classical logic is the basis for the core logic. The $\\lambda\\mu$-calculus is one such calculus that allows this to be possible. I am interested in it specifically because a $\\lambda\\mu$-cube was constructed for it without much difficulty. It is known that given the Calculus of Constructions that a proof assistant with inductive types is only a stone's throw away by adding on strong sigmas, bolting on an inductive datatype theory, or some other combinations of core types.\nThere are existing approaches that use classical reasoning such as HOL and other LCF logics where a classical logic can be embedded. However, I am interested in the class of proof assistants that are on equal footing with Coq, Agda, and Lean. That is, they ought to have dependent types at the very least.\nYet, I do not know of any proof assistants that have tried (and definitely none that have had any success!) in using the $\\lambda\\mu$-calculus as the computational basis. My question is thus in two parts: First, what difficulties might one run into when attempting this? Second, has anyone tried it?\nI want to dispense with the obvious objection of normal forms. The calculus does not have a strong notion of normal form (or at least, you have to choose between confluence and normal forms). This is not a show-stopper, a designer can pick confluence and use that for definitional equality while allowing propositional equality to \"repair\" the lack of strong normal forms. I do not think even usability will be hurt, as I imagine the $\\lambda$-calculus fragment can still enjoy $\\beta\\eta$-equality. If my assumptions here are wrong I would be interested to know.", "output": "I think that the first question can be generalized to \"What difficulties are there in adding side-effects to a proof assistant based on dependent type theory.\"\nFirst-class continuations are just one particular side-effect\nThe \u03bb\u03bc-calculus is only an instance of a more general class of extensions of the \u03bb-calculus with side-effects. There is no unique definition of what a side-effect is, but a good generic approximation is \"any feature of the language that makes the difference between call-by-name and call-by-value observable\". One can for instance mention exceptions, mutable state, IO, threads and whatnot.\nThe effect introduced by the \u03bb\u03bc-calculus is the notion of first-class exceptions. The \u03bc binder, indeed, allows to capture the current continuation,  which can be resumed at a later time with the [\u03b1]t construct. This is not possible in the standard \u03bb-calculus, and it is precisely what introduce classical abilities in this system.\nIn PLT, we ain't afraid of no effects\nThis other answer insists at length, and rightfully, on the fact that first-class continuations break the CbN-CbV equivalence and thus should be endowed with a stricter reduction strategy to preserve the equational theory. This is not a problem tied to dependent types specifically, but rather by definition the landmark of side-effects.\nNow, dependent types rely on the runtime semantics of the proofs, so the degeneracy of the equational theory percolates immediately in the logic. But the very same problem happens already with effectful non-dependent programming languages, and there, it is folklore that one needs to care about the exact reduction strategy. No sane programmer would expect a call-by-name equation to hold in their favourite ML implementation.\nSo in this regard, I believe that that answer is somewhat missing the really problematic point.\nI Can Haz callcc?\nDependent type theories are biased towards call-by-name. The one rule that allows lifting computation to typing is the conversion rule. But this rule bakes in the fact that conversion is call-by-name, as it is generated by the unrestricted \u03b2-rule.\nSo one could expect that throwing side-effects into MLTT is going to be fine, since it already made a choice of strategy for you. Obviously you have to be careful that indeed your effects are call-by-name, but apart from this technical detail it should be fine, shouldn't it?\nAlas! If you do this with callcc (which is essentially a flavour of \u03bb\u03bc-calculus) you get an inconsistent theory. What went wrong?\nThe real problem with effects and dependent types\nWhat really separates dependent type theory from other higher-order logics is that it features inductive types equipped with large dependent elimination. The latter is what allows the user to write weird types like forall (b : bool), if b then nat else empty where the shape of a type really depends on the value of a term.\nThis is precisely this feature that causes the issue with side-effects, and there is a simple intuitive and mostly correct explanation for this phenomenon.\nAs OP observed, side-effects break the canonicity property of the theory. A term, while reducing to a value, may perform side-effects that are observable. Thus, for instance, not all terms M : bool are convertible to either true or false in presence of effects. A typical PLT-centric way to phrase that is the progress lemma, which says that terms are either values or reduce. And indeed, values of an inductive type have the right shape, but by contrast terms can be wild.\nNow, large dependent elimination bakes in the fact that arbitrary terms behave like values. For booleans, for instance, it states that P true and P false are enough to prove forall b : bool, P b for any P : bool -> Type. But obviously, in presence of side-effects there are non-standard boolean terms that are neither true nor false.\nNo wonder why this principle results in an inconsistent theory!\nEmbracing effects\nEffectful dependent types are a pretty niche area. Still, we do have some proposals for it to work properly.\nSome effects can be added relatively straightforwardly and justified by a syntactic translation. As already argued, we need to do something with large elimination or face inconsistency. We believe that a simple linearity criterion is enough to separate the wheat from the chaff.\nLinearity is the critical insight of Baclofen Type Theory (BTT), a restriction of MLTT compatible with side-effects. In a nutshell, BTT restricts large elimination to predicates P : bool -> Type that are linear, i.e. which satisfy an equation ensuring that they actually behave as if they were evaluating their argument in a call-by-value / strict way.\nToo good to be true?\nUnfortunately, while BTT seems to validate a large class of effects, we still do not have a reasonable syntactic model of BTT for first-class continuations. The reason are technical and tied to the impredicative nature of the CPS monad. Yet, we believe that it may be possible to present a variant of BTT with first-class continuations as a standalone theory whose properties are proved directly on the syntax. This requires more work, and novel research, which is why there is no such thing yet.\nIn any case, BTT might not be a theory you want to work in.\n\nFrom a mathematical standpoint, it really puts emphasis on the potentially effectful nature of proofs-as-programs. This is clearly at odds with the standard mathematical practice.\nFrom a programming standpoint, mixing call-by-name and effects is not very nice. You always have to program defensively and rely on weird storage operators to ensure you actually get a value.\n\nIt might be more tenable in the long term to switch to a call-by-value (or more generally, call-by-push-value / polarized) type theory. Nonetheless, this move would only make the life of the computer scientist easier, while making the whole experience even more alien to the mathematically inclined user.", "meta": {"post_id": 541, "input_score": 19, "output_score": 13, "post_title": "What difficulties are there in basing a proof assistant on the $\\lambda\\mu$-calculus and has any proof assistant tried?"}}
{"input": "I want to simplify the expression 0 * 1 * 1 * 1 * 0 using simp only [mul_zero, zero_mul]. I would like mul_zero to fire first, so that the entire expression is simplified to zero in only one step. If mul_zero fires first, I would expect the simplification process to take 4 steps, due to there being one step to eliminate each 1.\nIs there a way I can tell Lean to prioritize which simplification lemma to try first?", "output": "Using mathlib's simp_rw, you can order your lemmas:\nexample (a : nat) : a + 0 * 1 * 1 * 1 * 0 = a := by simp_rw [mul_zero, add_zero]\n\nsimp_rw [lemma1, lemma2] does the hand-waved equivalent of simp only [lemma1], simp only [lemma2].", "meta": {"post_id": 550, "input_score": 10, "output_score": 9, "post_title": "Can Lean simp arguments be ordered?"}}
{"input": "Isabelle has the frontend Isar which mimics some features of the Mizar system.\nI'm curious if Coq has anything similar, i.e. an alternative to tactic scripts that's designed to be readable or similar to the mathematical vernacular.\nThis discussion on the Coq forums mentions Czar and SSReflect.", "output": "There indeed used to be a \"declarative language for Coq\" often referred to as Czar, but it was removed (from Coq's core) a long time ago. If it had had any actual users, it would have been easy at the time to restore it as an external plugin though.\nHaving tested it when it was still there, my personal impression was that it was actually very difficult to use, and nowhere near what Isar has achieved for Isabelle. My understanding is that one of the main reason why Isar was so successful is thanks to SledgeHammer.\nFortunately today, Coq has CoqHammer (which is even available as part of the Coq Platform). Furthermore, Coq's tactics are pretty flexible for different styles. So, with a combination of tactics such as assert, enough, now_show, and CoqHammer tactics, it is actually possible to reconstruct the Isar style in Coq (no need for a specific proof mode for this).\nAnother declarative-style set of tactic is provided by the coq-waterproof package (see an example in their README). The proof style looks very much like mathematical text. However, while Czar was suffering from a lack of IDE support (it's almost impossible to remember exactly how to write a \"natural language\" proof in what is in fact a controlled language), coq-waterproof is a support package for the Waterproof IDE (which is targeted at students). So this might be close to what you are looking for, but for pedagogical purposes. If the goal is to reproduce the Isar style with productivity in mind, then I would still rather recommend relying on Coq's tactics like assert and enough + CoqHammer.\nFinally, SSReflect was also mentioned. It is in fact very different from a declarative mode because it is very \"procedural\". However, the style in which SSReflect users write their proof is supposed to be close to the style of proofs in mathematical texts (one line per significant mathematical step) and reliance on the have: tactic to clarify the most important steps. The main difference between the SSReflect style and the Isar style is that SSReflect only relies on very small-scale automation, making the proofs in SSReflect very robust to evolution in the underlying Coq language, libraries or specifications.", "meta": {"post_id": 571, "input_score": 10, "output_score": 11, "post_title": "Is there a Mizar-like sublanguage for Coq?"}}
{"input": "Ever since the work by Gimenez for his PhD thesis, Coq has supported positive coinductive types. For example, the type of always-infinite streams containing elements of type A and accessor functions for it can be written:\nCoInductive Stream (A : Type) := Cons : A -> Stream A -> Stream A.\nDefinition hd A (s : Stream A) : A := match s with Cons _ x _ => x end.\nDefinition tl A (s : Stream A) : Stream A := match s with Cons _ _ s => s end.\n\nThis encoding allows us to prove from first principle:\nLemma Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\nProof.\nintros A s; case s; simpl; reflexivity.\nQed.\n\nHowever, since Coq version 8.9, the Coq manual advises to instead use negative coinductive types that require Coq's primitive projections:\nSet Primitive Projections.\nCoInductive Stream (A: Type) := Cons { hd : A; tl : Stream }.\n\nUsing this approach, Cons_hd_tl from above is no longer provable, and we must add it as an axiom:\nAxiom Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\n\nThe reason for nevertheless preferring negative coinductive types is that, unlike their positive counterparts, they do not break the subject reduction property of Coq's underlying formal system.\nEven though they are advised against, positive coinductive types are not officially deprecated in Coq's implementation, and remain in heavy use in high-profile projects such as CompCert. Converting such developments to use negative coinductive types would likely require significant effort.\nHow important is the breaking of subject reduction in practice, and can both negative and positive coinductive types be supported in Coq for the long term?", "output": "Intuition of the problem\nA good rule of thumb is to consider that in intensional type theory, coinductive types and function types share a lot of properties. In particular,  equality over streams is equally as evil as equality over functions, in so far as it almost never holds without assuming additional extensionality principles. The only way to prove such an equality is essentially by reflexivity.\nAs observed in the question, the current typing rules of coinductive types allow proving eta-expansion as a theorem\nAxiom Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\n\nwhich does not hold by reflexivity for any closed stream s, thus breaking subject reduction.\nThe deep root of the issue comes from the dependent elimination rule of coinductive types, since it allows proving that any coinductive term is necessarily a constructor. This is clearly breaking parametricity, as there are closed terms of coinductive types that do not even start with a constructor, namely cofixpoints. The SR failure is a consequence of this.\nWhy Subject Reduction?\nThere are various reasons to desire SR, some of them theoretical, some of them practical.\n\nOn the theoretical side, it makes the semantics of the type theory hard to define naturally. The soundness of an implementation of a type-checker for such a theory becomes quite murky as a consequence. One has to defensively guard against SR breakages, or face weird ill-typed situations otherwise. The issue is much more important than in non-dependent type systems, because the typing rules themselves embed the runtime semantics of the program, hence any defect of the latter percolates into the former.\nOn the practical side, this is not a black-and-white issue. It actually depends on the magnitude of SR breakage. One can survive within a locally broken system (case in point: Coq), but the more failures of SR, the less practical the proof assistant. Indeed, failure of SR means that type-checking is not stable by the equational theory of the language, a clear source of non-modularity. A perfectly fine proposition may become ill-typed after substituting a subterm by another convertible one. This is particularly infuriating when it happens because it is virtually impossible to debug, as one has to manually apply the typing rules to see where it fails.\n\nThankfully, in Coq, the practical issues of SR are exceedingly uncommon, for two reasons. First, the use of coinduction in Coq is somewhat marginal compared to the rest of the features of the system. Second, even developments using coinduction rarely rely on the computational content of the proofs and most coinduction is performed in opaque Prop proofs. Yet, the theoretical defects are still annoying. For instance, the MetaCoq formalization of CIC uses SR in critical parts. Thus implementing coinductive types as found in today's Coq will be problematic.\nSolutions\nThere are two possible solutions to recover SR for positive coinductive types, a brutal one and a finer-grained one.\nThe not-reasonable one\nThe brutal one is to eschew decidability of type-checking altogether and allow infinite unfolding of (co)fixpoints. While fixing SR, this is not reasonable. Decidability of type-checking is pretty much the core of the claim of the foundational status of proof systems like CIC. Furthermore, any actual implementation of an undecidable type-system will eventually fail at some point, be it intentionally (e.g., if the system implements some kind of timeout) or unintentionally (e.g., because of stack overflow or out-of-memory errors). This is intuitively a failure of SR in disguise, and implicitly indicates that there are now two logical systems, the \"ideal\" one and the \"implemented\" one.\nThe reasonable one\nThe cleverer solution is to attack the problem at the root, which was argued above to be dependent elimination of positive coinductive types. It is evil to assume that a coinductive term is always a constructor, the only thing that is allowed is to depend on the observations that one can perform with it. In the case of negative coinductive types, observations are primitive notions defined to be the projections of that type, so this is guaranteed by construction. For their positive flavour, one has to be slightly more imaginative and stare at the typing rule of dependent elimination.\nI have a proposal implemented as a PR that, to the best of my understanding, is enough to restore SR. The idea is to restrict the return type of coinductive pattern-matching in a way that is reminiscent of the restriction of inductive dependent pattern-matching in presence of side-effects.\nThe criterion is the following: assume w.l.o.g. an indexed coinductive type T : forall i : I, Type. A pattern-matching with return clause P : forall i : I, T i -> Type is valid iff it satisfies the following definitional equation:\ni : I, x : T i \u22a2 P i x \u2261 P i (cofix F := x).\nNow, to see why this prevents the SR failure, one has to peek into the semantics of (co)fixpoints. In Coq, you cannot unfold arbitrarily (co-)fixpoints, otherwise this leads to non-decidability of typechecking. There is thus a syntactic guard for (co-)fixpoint unfolding, namely:\n\nfor inductive fixpoints, they only unfold when applied to something that starts with a constructor, e.g.\n(fix F := t) (S n) \u2261 t{F := (fix F := t)} (S n)\nfor co-inductive cofixpoints, they only unfold when they are inside a pattern-matching, e.g.\nmatch (cofix F := t) with p \u2261 match t{F := (cofix F := t)} with p\n\nThe latter corresponds intuitively to the fact that the cofix is being forced.\nNow, assume some dependent pattern-matching over a term of a coinductive type T. In general, this pattern-matching is of the shape match t as x return P x with p. Its type is thus P t. But now, assume that t \u2261 (cofix F := t\u2080), in which case the unfolding rule mandates that expression is convertible to match t\u2080{F := (cofix F := t\u2080)} as x return P x with p, which now has type P (t\u2080{F := (cofix F := t\u2080)}).\nTherefore, for SR to hold it is a necessary condition to have P (cofix F := t\u2080) \u2261 P (t\u2080{F := (cofix F := t\u2080)}).\nBecause t\u2080 is arbitrary, this could very well be a variable as well. The criterion is based on this observation. While I have no mechanized proof of this, I convinced myself that this is actually a sufficient condition for SR to hold in general.\nConsequences\nIf this PR gets merged into Coq, it will restore SR at the cost of making dependent elimination on positive coinductive types quite weird. This is expected, since one must not be able to prove the eta-expansion theorem in CIC. It is safe to add it as an axiom, still, just as function extensionality can be posited.\nI am not sure it will be that practical because it also means that many proofs, e.g., the ones using destruct over coinductive terms without care for the convertibility criterion, will need to be ported at some point. The PR provide a flag to allow local breakage of SR, but eventually this flag will be removed. Also, this side-condition makes the elimination rule of coinductive types non-modular, because one cannot abstract over an arbitrary return predicate, since it would not satisfy the criterion.\nThus, I still believe that in the long run it is more natural to switch to negative coinductive types, even if we restored SR.", "meta": {"post_id": 583, "input_score": 22, "output_score": 18, "post_title": "What is the state of coinductive types and reasoning in Coq?"}}
{"input": "The Stack Exchange bot reminded me that I had committed myself to asking some questions, but please allow a possibly naive question, possibly of a philosophical nature rather than mathematical/computer-theoretical.\nSo, is it conceivable that the development of proof assistants make mainstream mathematicians switch their framework, from a set theoretical one, to a type theoretical one.\nIt is not that mathematicians all know about the axioms of ZFC, I'm certain most don't and most don't care, but mathematics are full of idiosyncratic set-theoretical gimmicks such as saying that a group is a 3-tuple consisting of a set, a law, a neutral element, and some people (most students) pay attention to the fact that it is a 3-tuple and not something else. (I have been asked which component of a similar 5-tuple a submodule should be\u2026)\nType theory (especially how it is implemented) simultaneously departs from such constructions, and insists on the list of objects/axioms (so that the computer-definition of a group also contains a proof of associativity, etc.).\nOn the other hand, the concept of a set is clearly an important one.\nSimilarly, some constructions are built on sets in a way which might look unnatural \u2014\u00a0say defining the quotient set as a set of equivalence classes \u2014\u00a0but are now well understood, and can be avoided by a more categorical point of view.\nI also have in mind more advanced notions of algebra/number theory where set theoretical notions look less avoidable, such as the definition of an ideal. (One could try to replace them by the epimorphism to the quotient ring, but that does not look very easy to handle.)", "output": "The following is a made up opinion piece based on the observation of a statistically small sample of mathematicians. Please take it with a grain of salt.\nA typical working mathematician has little time for and only cursory interest in the foundations of mathematics. They are able to operate perfectly well without stressing over formalities of whatever foundation they imagine standing on.\nThey don't particularly care what foundation they are using, as long as it does not interfere with their work. They think of a foundation as a necessary thing that is useful on a grand scale of things, but is not really crucial in their everyday work. Often they profess using a particular foundation, but their mathematical texts are closer to another one. What I am saying is that mathematicians do not actually use foundations that they could switch between.\nWhen such mathematicians start using proof assistants, they are forced to pay attention to foundations. They become more proficient in logic, they learn about introduction and elimination rules, they learn that some proofs are better than others etc. But what really matters, in my opinion, is that they actually start to believe that proofs are mathematical objects. Instead of just converting coffee to proofs, they start thinking about proofs. Even better, instead of just applying techniques and tricks to do math, they start thinking about the techniques and tricks, and how to express them in proof assistants.\nTo address the question directly, I would say that the typical experience is not that of \u201cswitching from one framework or foundation to another\u201d but rather \u201cactually learning a foundation and adopting it as a real working tool\". If you write your proofs in LaTeX then you have little direct use for foundations, but if you write your proofs in a proof assistant then the foundation and the proof development techniques are your new everyday tools.\nLet me also point to Penelope Maddy's paper What Do We Want a Foundation to Do? in which she nicely identifies the different purposes and roles that a foundation may have. While I do not entirely agree with her assessment of the role of univalent foundations, I appreciate her point that there's a difference between a foundation that studies what can be formalized in principle, and one that is supposed to be used to actually formalize mathematics. This difference may explain why the foundations of the early 20th century do not fit our current needs.", "meta": {"post_id": 596, "input_score": 8, "output_score": 12, "post_title": "Can the development of proof assistants make mathematicians switch their framework?"}}
{"input": "I noticed that Coq 8.15 (possibly 8.14) made some significant changes to divmod. In particular, Nat.divmod_0q0 seems to have been removed and the div_mod lemma with signature y<>0 -> x = y*(x/y) + x mod y has effectively been replaced with div_mod_eq which drops the y<>0 precondition.\nDoes divmod (and hence div and mod) now behave differently? Could somebody summarize the changes? It looks like some 8.15 theorems simply don't hold in 8.13 and vice-versa (with Nat.divmod_0q0).", "output": "This was changed in Coq 8.14 (MR: https://github.com/coq/coq/pull/14086), and the changes are summarized in https://coq.inria.fr/refman/changes.html#id310. Quoting:\n\nChanged: set n mod 0 = n uniformly for nat, N, Z, int63, sint63, int31 such that m = (m / n) * n + (m mod n) holds (also for n = 0)\n\nAs you can see, in many cases this gives stronger theorems with fewer side conditions \u2014 but in any case they're more uniform across nat, N and Z.", "meta": {"post_id": 600, "input_score": 11, "output_score": 11, "post_title": "What did Coq 8.15 change about divmod?"}}
{"input": "What are some useful and reliable resources for a mathematician interested in learning Isabelle/HOL? Could be online (websites) or physical (books).", "output": "Once you installed Isabelle (following these instructions), you will have all the materials listed on the tag isabelle as a local copy. Definitely go through this one for a good (field agnostic) introduction:\n\nProgramming and Proving in Isabelle/HOL\n\n(you won't need to go through the whole document)\nOnce you are familiar with its syntax, the most common tactics and Isar you will find it most helpful to read actual code. You will find good examples by pressing Ctrl and clicking on types or constants. (as an exercise, you could click on set which leads you to HOL-Set)\nSimilarly, you can find other good examples in HOL-Examples.\n\nAnother most useful resource for proofs is the AFP where you will find proofs of all sorts. Many of the theories there will come with an actual paper (not just the LaTeX generated by Isabelle).\n\nJust now, I also found this resource: Formalising Mathematics \u2013 in Praxis; A Mathematician\u2019s First Experiences with Isabelle/HOL and the Why and How of Getting Started which shows some very beautiful Isar proofs and could be useful (I only skimmed it).\nIt also has some good references that are worth to track down, for example Larry Paulson's formalization(s) of G\u00f6del's Incompleteness theorem (arxiv) one of which makes use of Isabelle/HOL's Nominal 2 framework (there is also a less powerful version shipped with Isabelle which can be found here).", "meta": {"post_id": 623, "input_score": 8, "output_score": 8, "post_title": "What are some useful resources for a mathematician interested in learning Isabelle/HOL?"}}
{"input": "Are there tools in mathlib which let you give computations of integrals which would roughly follow standard methods for solving them? For now let me restrict attention to some undergrad-level integrals, like $\\int_0^1xe^x\\,dx$, and not anything excessively crazy.\nOf course, if our goal is merely to have a proof of a statement of the form $\\int_a^bf(x)\\,dx=c$ for some known $c$, then proving this in Lean should be possible by finding a function $F(x)$, check $F'(x)=f(x)$, evaluating $F(b)-F(a)$ and appealing to FTC. But this is an approach which is very backwards compared to how this is done by hand (which, at least somewhat implicitly, involves finding such an $F$).\nMy question is, does Lean have any tools which would let you perform such \"by hand\" calculations within your code? At the very least it would require support for integration by parts and integration by substitution, and I'm not sure those are in mathlib. Ideally I would also imagine some kind of environment like the calc mode.\nIf not in Lean, are there similar tools available in other proof assistants?\nJust to clarify, I do not mean automated computation of integrals like in CAS. I just mean tools which would let you painlessly perform IBP and substitutions (and perhaps other operations) on given integrals to simplify them.", "output": "I'm going to inflate Mario's comment to an answer: You can absolutely write tactics in Lean to do this. What you'll be doing is creating a large part of a CAS, but as a Tactic. This would be largely a waste of effort to do in isolation.\nWhile Lean, especially Lean 4, would be an excellent language in which to create a CAS, this would likely be a rather independent project from mathlib. mathlib is explicitly about classical mathematics. Closed-form integration may be studied in first-year calculus, but it sits in a really odd place that straddles constructive, computational and classical mathematics. This hybrid universe of mathematics has not been studied much at all.", "meta": {"post_id": 629, "input_score": 21, "output_score": 11, "post_title": "How hard is computing integrals in Lean?"}}
{"input": "For most proof assistants the name alone is common, E.g. Coq, Lean but with Isabelle it is often seen as Isabelle/<xyz>?\nWhy the two parts?\nWhat is\n\nIsabelle/HOL\nIsabelle/FOL\nIsabelle/ZF\nIsabelle/ISAR\nIsabelle/Proof General\n\nAre there others?", "output": "Isabelle the program is designed as a \"generic\" LCF-style proof assistant/logical framework; it gives you a very minimal base logic (based on Church's simple type theory (pdf) and called Isabelle/Pure) and some infrastructure to declare new types, typeclasses (ref), constants, and judgment forms, postulate new axioms and rules, and basic tactics for performing logical reasoning therein.\nUsing this logical framework, and writing ML code (Poly/ML) to interface with it, one can develop object logics (pdf) and other applications x within Isabelle; these are then called \"Isabelle/x\".\nx = HOL, FOL, ZF, ... are object logics, they encode the rules of the particular logic in the language of Isabelle/Pure, and extend the framework with extra functionality for e.g. defining recursive functions, new tactics, theory libraries of formalizations, etc.\nx = Isar is an extension of the original Isabelle framework to allow for declarative proofs; it's now been very tightly integrated into Isabelle, but in the beginning proofs were done tactic-style, similar to how it's still done (I believe) in the HOL family of ITPs.\nx = Proof General is, I assume, the PG interface of the Isabelle prover.\nTo name a couple more interesting applications, there's Isabelle/auto2 by Bohua Zhan, it's a saturation-based theorem prover tool that enables very declarative proofs that don't require explicit tactic invocations after each claim. This allows for proofs that read quite similar to ones on paper, see e.g. here.\nI myself have tried to develop HoTT as an object logic, but it's not currently being actively worked on.\n\nI suppose this is similar to how Metamath works, but I'm not an expert on that.", "meta": {"post_id": 651, "input_score": 19, "output_score": 17, "post_title": "Why is Isabelle usually not just Isabelle but Isabelle/<xyz>?"}}
{"input": "As shown in an exercise here, we can prove the Gauss summation (that $\\Sigma_{i=0..n}{i} = n \\cdot (n + 1)/2$ ) in Isabelle/Isar using mathematical induction, as follows.\ntheorem\n  fixes n::nat\n  shows\"2 * (\u2211i=0..n. i) = n * (n + 1)\" \n  by (induct n) simp_all\n\nThe proof is a one-liner with much details hidden or handled automatically.\nHowever, this seems to be very different from the intuitive proof by Gauss that basically adds the sequence of numbers $1...n$, and a reversed copy of itself. The above Isabelle proof is a backward one, which is perfectly fine and is a textbook example of mathematical induction. But it still differs from the ideas of Gauss's famous proof, which seems to be a forward construction and then verification that it's what is wanted.\nI am just wondering:\nIs there a way to do a forward style proof that faithfully replicates Gauss's proof in Isabelle or similar proof assistants?\n(This example almost changed my perception of which proof style is simpler.)", "output": "Like @Bubbler, I think a nice forward proof can be done using a chain of equalities, to spell out each step:\ntheorem\n  fixes n::nat\n  shows\"2 * (\u2211i=1..n. i) = n * (n + 1)\" \nproof-\n  have \"2 * (\u2211i=1..n. i) = (\u2211i=1..n. i) + (\u2211i=1..n. i)\" by simp\n  also have \"\u2026 = (\u2211i=1..n. i) + (\u2211i=1..n. (n + 1 - i))\"\n    by (subst sum.atLeastAtMost_rev) simp\n  also have \"\u2026 = (\u2211i=1..n. (i + (n + 1 - i)))\"\n    using sum.distrib[symmetric].\n  also have \"\u2026 = (\u2211i=1..n. n + 1)\" by simp\n  also have \"\u2026 = n * (n + 1)\" by simp\n  finally show ?thesis.\nqed\n\nI took the liberty to start the summation at 1, not at 0, because I presumptuously expect that that\u2019s how Gauss thought about this list of numbers.", "meta": {"post_id": 682, "input_score": 8, "output_score": 10, "post_title": "How to prove Gauss summation of integers with proof assistants using forward reasoning?"}}
{"input": "I will describe a problem which I have in fact solved, but with considerable pain.  My question is whether there are better methods for dealing with similar issues.\nLean was giving me messages that did not seem to make sense.  After various manoeuvres to try to simplify and isolate the problem, I got a message like this (in which n1, r and k are all natural numbers).\ninvalid type ascription, term has type\n  (n1 * r) ^ k = n1 ^ k * r ^ k\nbut is expected to have type\n  (n1 * r) ^ k = ?m_1\n\nVisually, it appears that the term h = mul_pow n1 r k in question has precisely the expected type, so this message is confusing.  Eventually I entered set_option pp.all true, and thereby got a much longer and more explicit version of the above message.  From this I saw that in the goal, multiplication and powers were defined in the usual way for natural numbers.  However, in the term h there was all sorts of stuff about linear orders that I did not expect.  I later worked out that I had inadvertently given \u2115 an alternative monoid structure based on its structure as a lattice under the divisibility relation, and that that had been used by mul_pow when I defined h.  After removing the import that defined this unwanted instance, the problem went away.", "output": "People ask these on the Lean Zulip and because they're hard to debug my instinct is usually to try to help. Here's a checklist which I think describes my usual approach to the problem.\n\nWhen faced with issues of this nature the problem is almost always that the visually identical issues are not actually identical, like in your example; however sometimes they really are identical and the problem is a typeclass issue (e.g. the theorem you're trying to apply doesn't apply because even though the match should work, the theorem only applies to additive groups and you're trying to apply it to the naturals; unfortunately the error message you get is the same). So that's the first gotcha -- check that everything else is OK.\n\nSet pp.all true and verify (by eye, or perhaps using a text editor) that the visually identical terms really are not identical. Sometimes this is harder than it looks, because definitionally equal diamonds in mathlib (e.g. a field is a ring is a semigroup v a field is a monoid is a semigroup or whatever) might not look syntactically equal. You can look at the error message with pp.all true on and then literally copy the terms which you suspect that Lean is claiming are not definitionally equal and see if Lean can prove they're equal with rfl. If it can't, you know you've isolated the problem. Sometimes the terms really still do look equal; when this happens it can be a universe issue.\n\nSometimes the problem is easy to solve: e.g. if one term mentions _inst_4 and other one doesn't, but the other term mentions _inst_5 and the first one doesn't, then you have done something like made R a normed_ring and a division_ring, and both of these extend ring so you've given R two distinct ring structures and that's your problem. The correct approach here would be to make R a normed_division_ring.\n\nIf the terms are small then you can usually stare at them and figure out what's going on. You might have found a diamond in mathlib, and if you have then we'd be interested to know about it on the Lean Zulip; please minimise and post a fully working example.\n\nIf the terms are huge then go back to step 2, take those two terms which Lean is complaining that it can't prove are equal, and try and prove they're equal in tactic mode using the congr' tactic. This will match all the parts of the terms which do align and you might well be left with two much simpler terms which aren't definitionally equal; that's your problem.\n\nIf all else fails, remember that set_option pp.all true is just actually the following 11 options:\n\n\n* `pp.implicit true`\n* `pp.proofs true`\n* `pp.coercions true`\n* `pp.notation false`\n* `pp.universes true`\n* `pp.full_names true`\n* `pp.beta false`\n* `pp.numerals false`\n* `pp.strings false`\n* `pp.binder_types true`\n* `pp.generalized_field_notation false`\n\nYou can switch these options on one by one to see when things start diverging. The ones I usually start with are pp.notation, pp.implicit and pp.proofs.", "meta": {"post_id": 715, "input_score": 12, "output_score": 10, "post_title": "Lean: problems with visually indistinguishable instances"}}
{"input": "It seems to me that there are no real reasons to not like cumulativity (the example given here seems to not be too relevant, according to the comments), and yet most proof assistants (apart from Coq?) don't have cumulativity. Why is this so? This seems a very nice property mathematically, and is akin to how most people would think of universes intuitively.", "output": "The biggest problem in my head is that cumulativity opens the door of subtyping in type theory, which, IMO, a potential calamity to type theory. If you elaborate cumulativity to non-cumulativity + lifting, then it's probably fine, but let me say something about the subtyping-based approach.\nOnce you have subtyping, you'll start thinking about covariance and contravariance, which brings a lot more things to the type theory. Also, subtyping itself distinguishes two notions: consider $(a:A)\\in\\Gamma$, then \"the type of $a$\" and \"$A$\" are no longer the same under the context $\\Gamma$. Instead, \"the type of $a$\" is a subtype of $A$, and when you want to, for example, apply $a$ to the identity function with the type argument an implicit argument, then probably you can't solve the metavariable because $A$ won't be the most accurate solution (I thank Pavel for sharing some ideas with me about this).\nApart from that, cumulativity generates more complex universe level equations, which are harder to solve.", "meta": {"post_id": 716, "input_score": 24, "output_score": 10, "post_title": "Why is cumulativity not a given in proof assistants?"}}
{"input": "In the F* proof assistant, they use refinement types together with dependent types. Based on my impression of F*, it seems to me that refinement types are just predicates in dependent type theory that are like \"something something of type boolean should equal to true\". I think I'm probably (extremely) biased (and probably wrong), as I have only ever seen refinement types in one language.\nI am slightly more experienced with dependent types, and I'm wondering how are dependent types and refinement types related. Is there something that is only possible with refinement types, but not possible (or extremely hard to imitate) in dependent types?", "output": "Is there something that is only possible with refinement types, but not possible (or extremely hard to imitate) in dependent types?\n\nYes. Refinement types make the notion of logical or ghost term available. These are terms which can occur in types, but cannot directly be used for any computationally relevant purpose.\nConsider the following type for a function which takes a vector and returns how long it is.\nlength : \u03a0n: Nat \u2192 Vector n A \u2192 \u03a3m:Nat. m \u2261 n\n\nThis says that if you give the function a vector, it will return a number equal to the length of that vector. You can implement it like this in Agda:\nlength _ [] = (0, refl)\nlength _ (x :: xs) = let (k, pf) = length xs in \n                     (suc k, cong suc pf)\n\nwhich is a function which traverses the list and counts how many cons cells there are.\nUnfortunately, you can also write it like this:\nlength n _ = (n, refl)\n\nand just immediately return the index. In other words, you can't call the length function unless you tell it how long the list is! So there's no point to this function even existing.\nWith a refinement type discipline, you can write a function type like the following.\nlength : \u2200n: Nat \u2192 Vector n A \u2192 \u03a3m:Nat. m \u2261 n\n\nBy switching from pi to forall, I mean to indicate that the length argument is computationally irrelevant. (Note that this is not  proof-irrelevance! E.g., note the occurrence of n in m \u2261 n in the return value.) As a result, the first definition of length typechecks, but the terrible second definition won't.\nThis is super useful! (For example, termination metrics like Bove-Capretta accessibility predicates really want to be computationally irrelevant.)\nThe general theory of these things can be found in Noam Zeilberger and  Paul-Andr\u00e9 Melli\u00e8s POPL 2015 paper, Functors are Type Refinement Systems. Noam also has a set of OPLSS notes, which introduces these ideas more gently.", "meta": {"post_id": 755, "input_score": 9, "output_score": 11, "post_title": "What's the relationship between refinement types and dependent types?"}}
{"input": "When writing this answer, I had a hard time finding the lemma I needed to prove 2 \u2264 3. In Agda, when I have a goal, I can press refine and Agda will either insert a constructor or tell me she doesn't know which constructor to use, so I can pick one from the list. In Lean4 if I place a metavariable in the goal, she simply say \"don't know how to synthesize this\" (I'm paraphrasing, but it's something like that). Even without these error messages, I can click in the IDE to jump to the definition of the type, and I can read the definition to know the available constructors.\nIn Lean4 the situation is totally different. I looked up the definition of < and it's a macro or a notation definition, which I cannot jump to further definition (I tried clicking but it doesn't do anything). I looked around the code and found the definition -- it's LT, a class. So, to work with it, I need to know which instance I'm using in order to find the available constructors. However, I didn't find an easy way to lookup the instance used in my incomplete proof.\nI guess this is probably not the preferred way of programming in Lean4. I guess Lean4 wants me to work with mathematics abstractly, but I am unfamiliar with the Lean4 library and the Lean4 best practices. I can work pretty well with Cubical Agda library without prior knowledge to it by using the IDE extensively, and I wish to learn how can I do similar things in Lean4.\nPlease tell me anything I can do, even a bunch of keyboard shortcuts will help me a lot.", "output": "Here are a few ways of getting this goal without really knowing anything about what the definition of LT is etc.\nIn rough order of low-tech to high-tech, we have:\n\na tactic that tries to apply constructors of inductive types directly\nusing the simplifier (which is one of the main Lean workhorses)\na mathlib4 tactic that will search all available declarations for something that can be applied to your goal\n\nimport Mathlib.Tactic.LibrarySearch -- only for theorem c\n\ntheorem a : 2 \u2264 3 :=\nby\n  constructor\n  constructor\n\ntheorem b : 2 \u2264 3 :=\nby\n  simp\n\ntheorem c : 2 \u2264 3 :=\nby\n  library_search -- exact Nat.le_succ 2", "meta": {"post_id": 783, "input_score": 9, "output_score": 8, "post_title": "How to quickly look up what constructor/lemma I should use in Lean4?"}}
{"input": "The possibility of using proof assistants in combinatorics has been mentioned here and in more detail here.  I'm interested in automating that part of some discrete problems which reduces the search space \"without loss of generality\" by exploiting various symmetries.\nI'll mention for context a couple of problems I'm interested in:\n(1) Counting (doubly) diagonal latin squares invites reduction of a search space by the centralizer of a specific permutation. A latin square of size $n\\times n$ has the numbers $1,2,\\ldots,n$ in each row and each column without repetition.  Such a square is doubly diagonal iff the main diagonal and the anti-diagonal also have each $n$ distinct entries.  For example:\n$$\\begin{array}{|c|c|c|c|}\n\\hline\n1 & 4 & 2 & 3 \\\\\n\\hline\n3 & 2 & 4 & 1 \\\\\n\\hline\n4 & 1 & 3 & 2 \\\\\n\\hline\n2 & 3 & 1 & 4 \\\\\n\\hline \\end{array} \\;\\text{ and }\\;\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n5 & 3 & 4 & 1 & 2 \\\\\n\\hline\n4 & 5 & 2 & 3 & 1 \\\\\n\\hline\n2 & 4 & 1 & 5 & 3 \\\\\n\\hline\n3 & 1 & 5 & 2 & 4 \\\\\n\\hline \\end{array}\n$$\n(2) An absorbing Markov chain might be naively formulated using many equivalent states and transitions, and this invites for some purposes (e.g. expected steps until an absorbing state is reached) pruning the search space by symmetries of states.\nMy intuition is that proving \"without loss of generality\" cases is similar to proving program correctness.  Prolog code has often helped me with checking that \"all bases are covered,\" but I experience a certain amount of frustration with reinventing the wheel because of the weak(!) datatyping in Prolog.  So I'd be interested if setoids or some other construction would provide rigorous analysis of cases.", "output": "John Harrison had a nice paper about \u201cwlog\u201d reasoning and the logical requirements behind it.  The paper talks about various ways in which these can be automated.  This is in HOL Light, and I know there is an analogous tactic in HOL4. Given how foundationally simple HOL is, I expect  the logic works in other systems too.", "meta": {"post_id": 814, "input_score": 17, "output_score": 10, "post_title": "WLOG reasoning and Proof Assistants"}}
{"input": "I've seen in mathlib several cases where the universes are explicit, that is Type u instead of Type*. Is there any advantage in doing so and are there any (non-category theory) examples where universes have to be named explicitly in lean even though the mathematics does not run into universe issues?", "output": "Type* is just a shorthand for Type _, where the _ is a wildcard (or more accurately, a universe metavariable) that lean will try to fill in for you. Lean's options are to:\n\nCreate a new universe for you, which it will give a name like u_1.\nReuse an existing universe variable.\nUse max and similar to combine universe levels.\n\nUsually then, explicit universe variables are used either when control over the names is wanted, or when the automatic behavior does the wrong thing.\nTo demonstrate the automatic behavior a little:\n\nIn a variables line, lean will almost always create new variables; so\nvariables {\u03b1 \u03b2 : Type*}\n\nis expanded to\nuniverses u_1 u_2\nvariables {\u03b1 : Type u_1} {\u03b2 : Type u_2}\n\n\nIn something like\nexample {\u03b1 \u03b2 : Type*} (h : \u03b1 = \u03b2) : true := sorry\n\nLean is forced during elaboration of h to put both in the same\nuniverse, so it generates\nuniverses u_1\nexample {\u03b1 \u03b2 : Type u_1} (h : \u03b1 = \u03b2) : true := sorry", "meta": {"post_id": 818, "input_score": 11, "output_score": 8, "post_title": "Explicit vs implicit universes in lean"}}
{"input": "Various type theories have, over the years, explored extending the definitional equality with a variety of eta-laws and various forms of proof irrelevance.\nQuite a lot of systems manage eta for functions, because (as Thierry Coquand observed), you can eta-expand on a tit-for-tat basis, even if your equality test is not type directed. Whenever you are checking f = \\ x -> t, you may readily test \\ y -> f y = \\ x -> t instead (choosing y fresh), if f is not already eta-expanded. Crucially, if neither side is already of the form \\ x -> t, then eta-expansion makes no difference to the problem.\nAgda certainly manages eta for records, because its equality is type-directed. Two records are equal exactly if they are equal fieldwise, which makes it a delight to define the unit type as the record type with no fields. (You can do syntactic eta-expansion tit-for-tat for pairs, but not for unit. For unit, if neither side is already canonical, eta-expansion makes a big difference to the problem!)\nAre there any systems (in a good state of repair) which identify all terms in the empty type? That's a little trickier to manage, but I know some systems (because I built them) which did it. It's nice to have, because it means you can form the set of things which are not bad as dependent pairs of a thing and a proof that it's not bad: to prove that two such pairs are equal, you need only prove that the things are equal, because all inhabitants of Bad thing -> False are equal by eta for functions then falsity.\nIf you're working on a type theory for a proof assistant, have you got definitional equality for an absurd type? If so, I'd like to know (and I'm guessing Lean steps up, here). If not, what's stopping you? (I seem to remember that there was some sort of snag in Agda, but I can't for the life of me remember what it was.)", "output": "If you are happy to have your False live in a separate sort, then you can do this with strict propositions in Coq (and any other system that have those, which I guess includes quite a lot of them nowadays). Here is a small rough example:\nInductive False : SProp := .\n\nVariable Bad : nat -> SProp.\n\nInductive ok_nat : Type :=\n  | c (n : nat) (p : Bad n -> False) : ok_nat.\n\nVariable (o1 o2 : Bad 0 -> False).\n\nGoal (c 0 o1 = c 0 o2).\nProof.\n  reflexivity.\nQed.", "meta": {"post_id": 829, "input_score": 22, "output_score": 13, "post_title": "In what intensional type theories can absurdity be made definitionally proof irrelevant?"}}
{"input": "What is a universe?\nMore specifically,\n\nIs a type system without a term-type distinction, are universes just ordinary types?\nIf (1) is true, can we the system-builders freely choose which types are \"labeled\" as universes or is there a specific property that the collection of universes must satisfy?\n(An elaboration of (2)). If we label every type as a universe is it wrong or just misleading?\nWhat are universes in systems that possess a term-type distinction?\nGiven a type, is it inherently associated with exactly one universe? (The same way that a term is associated with exactly one type.)\nDo universes \"exist globally\" or are they like a \"local fiction\" (like universe levels in NFU ... that exist purely for the purpose of stratifying formulas and don't exist otherwise).\n\nNOTE: I'll reserve the word sort for use in a non-formal/meta-level sense; in this question a sort is not the type of a kind.\n\nWhat follows is my attempt to make sense of the material.\n\nAt time of writing, the universe tag wiki refers to the Wikipedia page and nLab.\nWikipedia says the following about universes in the context of type theory specifically.\n\nIn some type theories, especially in systems with dependent types, types themselves can be regarded as terms. There is a type called the universe (often denoted ${\\mathcal {U}}$) which has types as its elements.\n\nHere's how I interpret this.\n\nIn type systems that have a weak or nonexistent term-type distinction, a universe $\\mathcal{U}$ is just a type.\nIn type systems that have a term-type distinction, $\\mathcal{U}$ is a new sort of type that has types as elements. These systems now have a term-type-universe distinction.\n\nThe Wikipedia article gets most of its content from this article on homotopy type theory.\nIn this setting, it appears that a universe is a type for which we have an inference rule kind of like the following.\n$\\varphi$ is a fixed well-formed formula (that might be empty or might be an interpretation function) and $U$ is a universe.\n$$ \\frac{A : U}{\\varphi(A) \\;\\; \\text{type}} $$\nSection 4 of that article covers some concrete choices made by different proof assistants.\nThat and the section before about typical ambiguity are, to me at least, strongly reminiscent of the set theory NFU ... which can informally be obtained by taking the Russellian theory of simple types and just identifying stuff at every level of the hierarchy by neglecting type indices on defining formulas.\nHowever, in the NFU case, the different universes in the universe hierarchy do not exist globally. They spring into existence when we need to stratify a formula and then disappear.", "output": "Note: I'm approaching this from a purely syntactic perspective. I'm sure that a deeper answer could be given from the semantic perspective.\nIn all type theories I'm aware of, a universe is a type that classifies types. You mention the idea of a type-term distinction. I'm interpreting this as the presence or absence of a judgment of the form $A \\ type$, separate from $x : A$. This naturally leads to two styles of adding a universe to a type theory: universes ala Russel (Russel universes) and universes ala Tarski (Tarski universes).\nA Russel universe is some type $U$, whose elements are other types. If we have some type $\u2115$, then we can derive a the judgment $\u2115 : U$. The presence of a Russel universe allows us, if we wish, to completely eliminate any separate $A \\ type$ judgment, replacing it with $A : U$ (assuming we have some type for $U$ itself). This solution is conceptually straightforward and is used in languages like Coq and Agda.\nA Tarski universe is also some type $U$, but instead of its elements being types, they are terms, which we call type codes. Basically, instead of being able to derive $\u2115 : U$, we have some new term, which I will call $`\u2115$, for which we can derive the judgment $`\u2115 : U$. We do this for each type former. Notably, $`\u2115$ is not a type. We cannot derive $`\u2115 \\ type$ or $zero : `\u2115$. Instead, we introduce a special syntax which allows us to transform a type code into a type. This syntax is usually called $El$ (short for element), and allows us to derive that $El(a) \\ type$ given that $a : U$. In order to make $El$ useful, we introduce the \"obvious\" judgmental equalities: for instance that $El(`\u2115) = \u2115 \\ type$. If instead we made these judgmental equalities into mere equivalences, where an explicit coercion is required to transform a code into a type, we would have what are called weak Tarski universes. For a language that actually implements weak Tarski universes (though they are essentially hidden by elaboration), check out cooltt: https://github.com/RedPRL/cooltt.\nSo to answer your questions with all this in mind:\n\nYes\nA type is a universe when its elements classify other types in some meaningful way\nI would say that this is wrong, but I do not have enough knowledge of semantics to back up why it must be\nTarski universes\nNot necessarily, if we have a cumulative hierarchy of universes, we could have a rule classifying a type in any universe level\nUniverses exist globally in all type theories with them that I know of", "meta": {"post_id": 856, "input_score": 11, "output_score": 10, "post_title": "What is a universe?"}}
{"input": "I have heard that an extension to a type theory can be said to be conservative, which means it may add new formulae to the original type theory, for example new type formers and their intro/elim rules and equations, but the original type theory is left untouched in a sense that we do not create new inhabitants to existing types.\nSo here are my questions. I tried to approach the idea, but wikipedia defines conservative extension in terms of 'theory', not type theory. A (modern) type theory talks about not just theorems, but also data (like computation of mathematical objects, like numbers) using types, so:\n\nIf an extension adds new inhabitants to a data type instead of a proposition, is it considered conservative?\nIf an extension adds new inhabitants to an inhabited type, is it considered conservative?\n\nAlso, what does conservativity tell us about the extension? We usually say normalization tells us the decidability of type checking (and much more), canonicity tells us the type theory is a programming language (citation needed), etc., but what about conservativity?", "output": "Anja Petkovi\u0107 Komel in her PhD thesis Meta-analysis of type theories with an application to the design of formal proofs studied transformations of type theories among other things.\nDefinition 9.3.3 on page 66 gives the notion of a conservative transformation. This is more general than a conservative extension, which is a conservative inclusion.\nLet me give a quick summary. To understand the general definition, we need to know what a boundary is. Every type-theoretic judgement has a boundary and a head:\n\nthe boundary of $\\Gamma \\vdash A \\; \\mathsf{type}$ is $\\Gamma \\vdash \\Box \\; \\mathsf{type}$ and $A$ is the head.\nthe boundary of $\\Gamma \\vdash t : A$ is $\\Gamma \\vdash \\Box : A$ and $t$ is the head.\nthe boundary of $\\Gamma \\vdash A \\equiv B$ looks the same as the judgement itself because its head is trivial (judgemental equality is proof irrelevant), but we might write the judgement as $\\Gamma \\vdash A \\equiv B \\;\\mathsf{by}\\; \\star$, the boundary as $\\Gamma \\vdash A \\equiv B \\;\\mathsf{by}\\; \\Box$ and take $\\star$ to be a trivial token that serves as the head.\nsimilarly for $\\Gamma \\vdash t \\equiv s : A$.\n\nA boundary $\\Gamma \\vdash \\mathscr{b}$ may be filled with a head $e$ to give a judgement $\\Gamma \\vdash \\mathscr{b}\\boxed{e}$. For example, filling the boundary $\\Gamma \\vdash \\Box : A$ with $t$ produces $\\Gamma \\vdash t : A$.\nBoundaries are an important concept that is often left implicit, but they deserve to treated properly. For example, a goal in a proof assistant is precisely a well-formed boundary. The goal is solved when its head is filled to give a derivable judgement.\nGiven a type-theoretic transformation $f : \\mathcal{T} \\to \\mathcal{U}$, we say that $f$ is conservative when the following holds: for any well-formed boundary $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$ in $T$, if the boundary $f \\Gamma \\vdash_\\mathcal{U} f \\mathscr{b}$ can be filled with some $e$ to give a derivable judgement $f \\Gamma \\vdash_\\mathcal{U} (f \\mathscr{b})\\boxed{e}$, then there is $e'$ such that $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}\\boxed{e'}$.\nConcretely, suppose $f : \\mathcal{T} \\to \\mathcal{U}$ is an inclusion so that $f(e) = e$. Then conservativity of $f$ amounts to: if $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$ is well-formed in $\\mathcal{T}$ and $\\Gamma \\vdash_\\mathcal{U} \\mathscr{b}\\boxed{e}$ is derivable in $\\mathcal{U}$, then there is $e'$ in $\\mathcal{T}$ such that $\\mathcal{T}$ derives $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}\\boxed{e'}$. In particular, when we specialize this condition to term judgements and term equations, we get:\n\nif $\\Gamma \\vdash_\\mathcal{T} A \\; \\mathsf{type}$ and $\\Gamma \\vdash_\\mathcal{U} e : A$ then there is $e'$ such that $\\Gamma \\vdash_\\mathcal{T} e' : A$\nif $\\Gamma \\vdash_\\mathcal{T} s : A$ and $\\Gamma \\vdash_\\mathcal{T} t : A$ and $\\Gamma \\vdash_\\mathcal{U} s \\equiv t : A$ then $\\Gamma \\vdash_\\mathcal{T} s \\equiv t : A$\n\nYou should not take the above as precise definitions, because I skipped over the treatment of meta-variables, which matters. The details are in Petkovi\u0107 Komel's dissertation. She also gives examples of conservative transformations, such as definitional extension.\nThe concept of a conservative transformation subsumes the usual notions of conservativity that one sees in logic books, because first-order theories are special cases of type theories.\nLet us apply the concept to your specific question: if an extension adds a new element to a type, is it conservative? Concretely, suppose we have a type $A$ in theory $\\mathcal{T}$ to which we add a new constant $\\vdash c : A$ to obtain an extended theory $\\mathcal{T}'$.\nIf $A$ was empty beforehand, then this is not conservative, consider the boundary $\\vdash \\Box : A$. Otherwise, consider a closed term $t : A$ and define a further extension $\\mathcal{T}''$ which, in addition to $\\vdash c : A$, also has the equation $\\vdash c \\equiv t : A$. Example 9.3.5 of Petkovi\u0107 Komel's thesis shows that $\\mathcal{T}''$ is a conservative extension of $\\mathcal{T}$. We thus have a chain of extensions\n$$\\mathcal{T} \\to \\mathcal{T}' \\to \\mathcal{T}''$$\nwhich can be used to show that $\\mathcal{T}'$ is a conservative extension: given a boundary $$\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$$ and a filling $$\\Gamma \\vdash_{\\mathcal{T}'} \\mathscr{b}\\boxed{e},$$ we also get\n$$\\Gamma \\vdash_{\\mathcal{T}''} \\mathscr{b}\\boxed{e},$$\nand hence by conservativity of $\\mathcal{T}''$ over $\\mathcal{T}$ we find $e'$ such that\n$$\\Gamma \\vdash_{\\mathcal{T}} \\mathscr{b}\\boxed{e'}.$$\nWorking through Example 9.3.5 tells us how $e'$ is related to $e$: we just replace all occurrences of $c$ with $t$, as one would expect of a definitional extension.\nThe wheels are turning, the mechanism works.\nYou asked how the concept is useful. In chapter 10 of the dissertation, a general elaboration theorem is proved: every finitary type theory can be elaborated to a standard type theory. Conservativity plays a role in this theorem: the fact that the retrogression map (the opposite of elaboration) is conservative means that the elaboration does not reconstruct information that is not really there.\nConservativity also plays a role in the design of PAs. Suppose we use an SMT solver $S$ to solve goals in a proof assistant $P$. The situation amounts to a transformation $f : P \\to S$ that converts boundaries in $P$ to boundaries in $S$. To say that $f$ is conservative means that it is safe to use the solver $S$: for if $S$ manages to fill a boundary $f \\mathscr{b}$ then we know that the original boundary $\\mathscr{b}$ can also be filled in $P$. Of course, we need an explicit way of converting the head that $S$ found back to a head in $P$ \u2013 but that is precisely a computable witness of conservativity. And also, note that we need not translate back entire derivations, we only need to be able to reconstruct the head.", "meta": {"post_id": 857, "input_score": 8, "output_score": 12, "post_title": "What's \u201cconservativity\u201d in terms of type theory and how is it useful?"}}
{"input": "When I first learned Haskell, I was told that you use data keyword to define an \"algebraic data type\", or a \"sum type\". Then I learned the idea of catamorphism, which is related to the \"initial F-algebra\" -- that all such \"algebraic data types\" can be encoded as the colimit of its finite approximations. The name \"algebraic data type\" is an attribute to the fact that we're using an algebra to generate the type.\nThen, I saw an article by Jason Hu (in Chinese) that Haskell does not have algebraic data types, because Haskell does not enforce strict positivity. I'm not sure if this is true, because some types that are not strictly positive may also be generated \"algebraically\", for instance coinductive types can be generated by \"final F-coalgebra\".\nI tried to find some relevant resources on the internet. Wikipedia says algebraic data types are types generated by taking sums or products of basic types, which I think is true, but it doesn't mention recursive cases, while positivity can only be discussed when you have type recursion.", "output": "[Note: After some helpful off-line remarks by Mart\u00edn Escard\u00f3 I have edited the part about general recursive types. It was at best misleading by trying to paint too simple a picture.]\nI will start with the classic notion of universal algebra and develop from it algebraic datatypes. Then I will discuss how they relate to coalgebraic datatypes and recursive types found in programming languages. Hopefully this will bring some clarity to what is what.\nSignatures in universal algebra\nIn universal algebra a signature is a list of operations with their arities\n$$\\sigma = [(\\mathsf{op}_1, n_1), \\ldots, (\\mathsf{op}_k, n_k)].$$\nHere $\\mathsf{op}_i$ are symbols and $n_i \\in \\mathbb{N}$.\nUniversal algebra also has equations but those play no role here so we shall ignore them. For example, the signature for a group might be $[(\\mathsf{e}, 0), (\\mathsf{i}, 1), (\\mathsf{m}, 2)]$, for unit, inverse and multiplication, respectively.\nWe define the set $T_\\sigma$ of terms inductively as:\n\n$\\mathsf{op}_i(t_1, \\ldots, t_{n_i}) \\in T_\\sigma$ for $i \\leq k$ and $t_1, \\ldots, t_{n_i} \\in T_\\sigma$\n\nThere better be some nullary operations or else $T_\\sigma$ is empty. Also, in most applications we would include among the terms some variables, but we're ignoring that point also.\nNote that $T_\\sigma$ satisfies the isomorphism\n$$T_\\sigma \\cong T_\\sigma^{n_1} + \\cdots + T_\\sigma^{n_i}.$$\nIf we write $P_\\sigma(X) = X^{n_1} + \\cdots + X^{n_i}$ then the above is written more succinctly as $T_\\sigma \\cong P_\\sigma(T_\\sigma)$. There are many such fixed points of $P_\\sigma$, but $T_\\sigma$ is special because it is the smallest one (in a sense to be made precise below).\nInitial algebras for polynomial functors\nAt least this is how an old textbook on algebra might do it. Let us bring in some category theory. First, we replace arities as numbers with arities as sets. Thus, instead of saying that we have $n_i$ terms $t_1, \\ldots, t_{n_i} \\in T_\\sigma$ we shall say that we have a map $t : N_i \\to T_\\sigma$ where $N_i$ has $n_i$ elements (and henceforth we use $N_i$ directly, forgetting about $n_i$).\nNext, let us allow operations with parameters. Each symbol $\\mathsf{op}_i$ shall have a parameter set $S_i$ and an arity $N_i$. For instance, scalar multiplication in a real vector space would have $S_i = \\mathbb{R}$ and $N_i = \\{\\star\\}$, because it is a unary operation on vectors parameterized by a real number. An operation without a parameter set is one whose parameter set is a singleton set.\nThus a signature looks like this:\n$$\\sigma = [(\\mathsf{op}_1, S_1, N_1), \\ldots, (\\mathsf{op}_k, S_k, N_k)].$$\nIt determines a functor $P_\\sigma : \\mathsf{Set} \\to \\mathsf{Set}$ given by\n$$P_\\sigma(X) = S_1 \\times X^{N_1} + S_2 \\times X^{N_2} + \\cdots + S_k \\times X^{N_k},$$\nknown as a polynomial functor (for obvious reasons). In category-theoretic language, the set of terms $T_\\sigma$ is just the initial $P_\\sigma$-algebra. It is a fixed point of $P_\\sigma$,\n$$T_\\sigma \\cong\n  S_1 \\times T_\\sigma^{N_1} + S_2 \\times T_\\sigma^{N_2} + \\cdots + S_k \\times T_\\sigma^{N_k}.\n$$\nNote that $P_\\sigma$ may have many fixed points. For example, the final $P_\\sigma$-coalgebra is also such a fixed point.\nAlgebraic and coalgebraic datatypes\nAs a datatype, $T_\\sigma$ would be described as follows (I am dropping the subscript $\\sigma$):\ndata T : Set where\n  op_1 : S_1 \u00d7 (N_1 \u2192 T) \u2192 T\n  ...\n  op_k : S_k \u00d7 (N_k \u2192 T) \u2192 T\n\nSo this is just a different notation for an algebraic signature. It therefore makes sense to call the type determined by it an algebraic or inductive datatype, provided such a datatype really is inductive, i.e., it satisfies a suitable induction or recursion principle witnessing its initiality.\n(A side remark: the \"strict positivity\" requirement that proof assistants impose on inductive definitions is just their way of making sure that the underlying functor determined by the given signature is nice enough to have an initial algebra.)\nActually, we could describe the final coalgebra with the exact same data, by writing something like\ncodata T : Set where\n  op_1 : S_1 \u00d7 (N_1 \u2192 T) \u2192 T\n  ...\n  op_k : S_k \u00d7 (N_k \u2192 T) \u2192 T\n\nThe information, namely the algebraic signature, is the same, but the constructed type is now the final coalgebra, so it makes sene to call it a coinductive datatype, provided it satisfies suitable principles witnessing the fact that it is a final coalgebra.\nGeneral recursive types\nObserve that we may generate all polynomial functors $P_\\sigma$ using pre-existing types, the type constructors $\\times$ and $+$, and powers by pre-existing types $X \\mapsto X^S$. In fact, we get precisely the polynomial functors this way.\nGeneral recursive types arise when we allow other type constructors, for example we include function spaces $\\to$. This leads to various complications, because it is not even clear at first how to turn something like\n$$Q(X) = (\\mathsf{Nat} \\to X) + (X \\to \\mathsf{Bool})$$\ninto a functor. Is it covariant or contravariant? And once we do figure out how to turn $Q$ into a functor, it is not clear anymore whether we should be taking the initial algebra or the final coalgebra. Nevertheless, all these complications can be resolved with the aid of algebraically compact categories.\nWithout going too much into the details of general recursive types, let me say that the standard way to treat them is to model them in a category where the initial algebra and the final coalgebra coincide. These categories are typically categories of domains or predomains and require some getting used to and cannot be thought of naively as sets.\nTypes in programming languages\nA programming language might therefore support some selection of the following:\n\nalgebraic datatypes, also known as inductive datatypes,\ncoalgebraic datatypes, also know as coinductive datatypes,\ngeneral recursive types.\n\nThere may be further nuances that arise from the fact that a real-world programming language is not the same thing as a mathematical model, but let us not worry about that too much.\nProof assistants: Agda data and Coq's Inductive are algebraic datatypes. Proof assistants make provisions for coalebraic datatypes by other means. They do not have general recursive types as those allow arbitrary recursive definitions, which makes it possible to inhabit all types (and thereby ruin the logical content of types). Here is how one can use recursive types in Haskell to inhabit every type without any explicit recursive calls:\ndata Magic a = Abracadabra (Magic a -> a)\n\nevil :: a\nevil = spell (Abracadabra spell)\n  where\n    spell :: Magic a -> a\n    spell (Abracadabra x) = x (Abracadabra x)\n\nProgramming languages: they do not care about propositions-as-types, so they are free to admit general recursive types, which they often do. When the general recursive types are specialized to algebraic signatures, they may yield something that looks like an inductive datatype, or a coinductive datatype, or neither, depending on the details of the underlying semantics.\nHaskell data definitions allow general recursive types. In particular, Haskell's\ndata Nat = Zero | Succ Nat\n\ngives the inital/final solution for the functor $X \\mapsto 1 + X$, which is not the natural numbers, but rather the domain of lazy natural numbers whose elements are of the form:\n\nSucc (Succ (Succ (\u22ef Succ Zero) \u22ef))) \u2013 the natural numbers\nSucc (Succ (Succ (\u22ef Succ \u22a5) \u22ef))) \u2013 the partial numbers\nSucc (Succ (Succ (\u22ef \u22ef))) \u2013 infinity\n\nI am explaining all this to make a point: Haskell's natural numbers are both initial and final, but are computed in a different category so we cannot just pretend that they live in sets.\nOCaml and SML recursive type definitions also allow general recursive types. In particular, OCaml's\ntype nat = Zero | Succ of nat\n\ngives the initial/final algebra for the functor $X \\mapsto 1 + X$, which is the natural numbers, because it is computed in a different category.\nDisclaimers:\n\nAs Pierre-Marie points out in the comments, OCaml allows certain limited cases of coinductively defined values, such as let rec infinity = Succ infinity. However, these do not provide general coinductive types, only certain regular trees, so let us ignore them.\n\nIn the spirit of the previous disclaimer, we should be aware that Haskell and OCaml as real-world languages do not actually have a complete mathematical semantics, or at least not one that we would want to discuss here. The above remarks about recursive types in Haskell and OCaml should be understood as applying to smallish well-defined fragments of the languages.", "meta": {"post_id": 879, "input_score": 29, "output_score": 29, "post_title": "What is an algebraic data type?"}}
{"input": "I am familiar with De Bruijn indices, but not with De Bruijn levels. In my understanding, indices index variables from the top of the context/stack, whereas levels index variables from the bottom. So indices might be more attractive for dealing with bindings, but levels might be more attractive for dealing with weakening. But how does this play out from a broader perspective, for example when building an evaluator or a typechecker?\nIn what instances should De Bruijn levels be used over De Bruijn indices, and vice versa?", "output": "If we want to do evaluation in the presence of free variables (\"open\" evaluation), then De Bruijn levels are essential, precisely because of their support for cost-free weakening.\nGenerally, the best practice to implement substitutions and beta-reductions with open terms, is to have\n\nSyntactic terms, which can be evaluated to values in a given value environment. These may use any variable representation, but usually De Bruijn indices are the simplest and most efficient.\nValues, which contain closures instead of binders. Values must use levels or some other variable representation which is not relative to the size of the scope (e.g. globally fresh names are also workable).\n\nThe combination of levels and closures is what allows free weakening of values. If we have levels in plain terms, the bound variables have to be shifted during weakening. But here we don't have bound variables, we have closures instead, so there is nothing to shift.\nFor open evaluation of pure lambda terms, it looks like the following (in Haskell):\ntype Ix  = Int\ntype Lvl = Int\n\ndata Tm      = Var Ix | Lam Tm | App Tm Tm\ndata Closure = Close [Val] Tm\ndata Val     = VVar Lvl | VApp Val Val | VLam Closure\n\neval :: [Val] -> Tm -> Val\neval env t = case t of\n  Var x   -> env !! x\n  App t u -> case (eval env t, eval env u) of\n               (VLam (Close env' t), u) -> eval (u:env') t\n               (t                  , u) -> VApp t u\n  Lam t   -> VLam (Close env t)\n\nWe may also add quoting to normal forms, to get normalization-by-evaluation:\nquote :: Lvl -> Val -> Tm\nquote l t = case t of\n  VVar x             -> Var (l - x - 1)\n  VApp t u           -> App (quote l t) (quote l u)\n  VLam (Close env t) -> Lam (quote (l + 1) (eval (VVar l:env) t))\n\nnf :: [Val] -> Tm -> Tm\nnf env t = quote (length env) (eval env t)\n\nSome sources which use this basic evaluation setup:\n\nThierry Coquand: An algorithm for type-checking dependent types\nAbel, Coquand, Pagano: A Modular Type-checking algorithm for Type Theory with Singleton Types and Proof Irrelevance\nCoquand et al.: A simple type-theoretic language: Mini-TT\nDavid Christiansen: Checking Dependent Types with Normalization by Evaluation: A Tutorial\ncubicaltt\ncooltt\nMy elaboration zoo", "meta": {"post_id": 900, "input_score": 18, "output_score": 22, "post_title": "When should I use De Bruijn levels instead of indices?"}}
{"input": "W-types are said to be \"a well-founded inductive type\" that is parameterized over arities and constructors, similar to a \"tree of possible constructions\". On nlab, it is said that a lot of inductive types can be encoded with W-type.\nHowever, from that description I still don't understand what's well-foundedness in terms of the syntax of inductive types. I can see that Agda's non-mutual inductive families are \"well-founded\" because it's a rooted tree, but what about inductive-recursive and inductive-inductive types? I think these are known to be not equivalent to non-mutual inductive families and they have a more complex model. I think someone told me that strictly positive inductive families are well-founded, but what exactly makes a(n inductive) type well-founded?\nCan we encode all well-founded inductive types with the W-type?", "output": "\"Well-founded\" and \"inductive\" mean roughly the same thing. I think the reason different terminology tends to get used for W-types is that their definition looks similar to notation for ordinals (imagining that the branching of the trees is like a limit of the children). They're distinct from the other inductive types given in Martin-l\u00f6f type theory in that they are potentially infinitely wide. But they're still supposed to be (intuitively) finitely deep, which is what the \"inductive\" and \"well-founded\" refer to\nWhether or not you can encode various other sorts of inductively specified types as W-types is a complicated question.\n\nIt's pretty easy to show that you can encode plain inductive types using W-types if you have various sorts of extensionality. The typical schemas for what constitutes an 'inductive definition' are like finite polynomials, while W-types are trees built from any (infinite) polynomial definable in type theory. The main discrepancy is how the 'products' act. The finite products in inductive schemas have a canonical form, while the functions used in W-types do not, and the most obvious way to fix that is with function extensionality.\n\nHowever, you can also get by without extensionality. You only need some eta rules. The idea is to use W-types to build the right sort of trees, then define the subtype of 'canonical' trees by (W-type) induction. It happens that the recursion rule for this subtype has the judgmental behavior matching the schema for inductive definitions.\n\nYou can also encode indexed W-types using normal W-types and the identity type using a similar strategy. You use W-types to build a larger type of trees on the 'total spaces' involved, and then define a subtype of well-indexed trees by induction. This file shows how to do it.\n\nYou can (I believe) encode indexed inductive types/families using indexed W-types, including the encoded version of those, using a strategy similar to the Why Not W? paper. My Agda file above shows how to do this for a fairly simple indexed type that was mentioned in another question here.\n\nYou can encode mutual inductive and inductive-inductive types with indexed inductive types. For mutual inductive types, you just add a finite index type to turn N definitions into a single N-indexed definition. For induction-induction, you follow a similar strategy as for building indexed W-types: define mutual inductive types that contain too many values, then define the subtypes with proper indexing afterward.\n\nYou cannot encode all inductive-recursive definitions as (indexed) inductive definitions. I-R definitions were invented as a schema that would let you write down the definition of universes as a special case. However, the additional power comes from simultaneously being able to define a recursive type family. If instead you just simultaneously define a recursive function into an existing type, I believe they are encodable using a strategy like above. This might mean that if you have enough universes, you can use them with inductive definitions to encode everything you could write by just having a theory that admits inductive-recursive definitions (but with no pre-specified universes). I'm unsure about this, though.\n\nHaving a universe that classifies inductive-recursive definitions is even stronger, and isn't itself an instance of induction-recursion. It's actually inconsistent to be able to do induction on such a universe (while the I-R definable universes in 6 can have an associated induction principle).\n\nQuotient/higher inductive types can't be encoded as any of the previous sorts of definition in general. Quotients where you can compute a canonical representative for each equivalence class can be defined, but not all quotients are like that.\n\n\nI don't think strict positivity has anything to do with being inductive/well-founded (negativity does, though; see the comments). It's necessary to guarantee that 'all' inductive definitions are meaningful in various sorts of models. For instance, you can't have a classical set theoretic model of an inductive type $T \\cong 2^{2^T}$ (which is positive, but not strictly positive), because $2$ classifies the propositions, and you can't have a type equivalent to its double power type. Constructively you might be able to admit some such types, and for instance, $\u03bb \\_ \u2192 0$ and $\u03bb\\_ \u2192 1$ give you a starting point for building up your finitely-deep values. However, these sorts of non-strictly positive types can conflict with other features than just classical mathematics, so you need to be very careful.\nVarious bits of the above are subject to caveats about the details of what counts as an \"inductive definition.\" There's literature out there rigorously defining various schemata for what constitutes an (indexed) inductive(-recursive/inductive) definition (etc.). Agda (for instance) is not super rigorous, and runs a checker that lets you conveniently do things that could probably be encoded in those more rigorous schemata in a more inconvenient way. Or perhaps you couldn't, but it's still fine; or isn't.", "meta": {"post_id": 926, "input_score": 10, "output_score": 13, "post_title": "What are well-founded inductive types?"}}
{"input": "I find my mathematics and programming background$^*$ do not endow me with much understanding of type theory as it pertains to proof assistants.  To remedy this shortcoming I don't expect a Royal Road to understanding, and my strategy is mixed.\nI ordered Type Theory and Formal Proof -- An Introduction by Nederpelt and Geuvers (2014), a real hardcover book which Barendregt's Foreword claims to provide \"a gentle, yet profound, introduction to systems of types and their inhabiting lambda-terms.\"  Although the book stretches out to some four hundred pages, it gets there in chapters of twenty to forty pages, each capped by exercises to assess ones comprehension.\nI'm also installing Lean 4 and pursuing as a tutorial Mathematics in Lean hoping to see the hierarchy of types in a familiar enough perspective.\nThis is me \"hitting it high, hitting it low\".  I'm interested in other approaches and ways to expedite learning.\n\nMy math background includes formal logic, algebra, combinatorics, category theory and a good bit of numerical methods, and my programming skills are weighted toward object-oriented procedural languages (C++), Prolog, and database stuff.", "output": "I find my mathematics and programming background do not endow me with much understanding of type theory as it pertains to proof assistants.\nSo many data types, so little time\n\n\nOregon Programming Languages Summer School \u2014 July 16-28, 2012\nThe program consists of 80 minute lectures presented by internationally recognized leaders in programming languages and formal reasoning research.\n(ref) - Free access to all of the videos of the lectures.\nTechnical Lectures\n\nLogical relations \u2014 Amal Ahmed\nCategory theory foundations \u2014 Steve Awodey\nProofs as Processes \u2014 Robert Constable\nPolarization and focalization \u2014 Pierre-Louis Curien\nType theory foundations \u2014 Robert Harper\nMonads and all that \u2014 John Hughes\nCompiler verification \u2014 Xavier Leroy\nLanguage-based security \u2014 Andrew Myers\nProof theory foundations \u2014 Frank Pfenning\nSoftware foundations in Coq \u2014 Benjamin Pierce\n\n\nAs noted in a comment by Albert ten Napel\nSoftware Foundations\nThe Software Foundations series is a broad introduction to the mathematical underpinnings of reliable software.\nThe principal novelty of the series is that every detail is one hundred percent formalized and machine-checked: the entire text of each volume, including the exercises, is literally a \"proof script\" for the Coq proof assistant.\n(ref)\n\nTypes and Programming Languages\nby Benjamin C. Pierce\n(WorldCat) (Site)\nMost of the type theories derive from Lambda Calculus. This is book is Lambda Calculus on steroids.\nList of types from the book\nName                   Extends/Based on Figure                          Figure                                                   Page\nB untyped                                                                3-1 Booleans (B)                                         34\nB \u2115 (untyped)          Extends 3-1 B                                    3-2 Arithmetic expressions (\u2115B)                          41\n\u2192 (untyped)                                                             5-3 Untyped lambda-calculus (\u03bb)                           72\nB (typed)              Extends 3-1 B                                    8-1 Typing rules for Booleans (B)                        93\nB \u2115 (typed)            Extends 3-1, 8-1 B                               8-2 Typing rules for numbers (\u2115B)                        93\n\u2192 (typed)              Based on5-3 \u03bb                                    9-1 Pure simply typed lambda-calculus (\u03bb\u2192)               103\n\u2192 Unit                 Extends 9-1 \u03bb\u2192                                   11-2 Unit type                                           119\n\u2192 as                   Extends 9-1 \u03bb\u2192                                   11-3 Ascription                                          122\n\u2192 let                  Extends 9-1 \u03bb\u2192                                   11-4 let binding                                         124\n\u2192 x                    Extends 9-1 \u03bb\u2192                                   11-5 Pairs                                               126\n\u2192 {}                   Extends 9-1 \u03bb\u2192                                   11-6 Tuples                                              128\n\u2192 {}                   Extends 9-1 \u03bb\u2192                                   11-7 Records                                             129\n\u2192 {} let p (untyped)   Extends 11-7, 11-4                               11-8 (Untyped) record patterns                           131\n\u2192 +                    Extends 9-1 \u03bb\u2192                                   11-9 Sums                                                132\n\u2192 +                    Extends 9-1 \u03bb\u2192                                   11-10 Sums (with unique typing)                          135\n\u2192 <>                   Extends 9-1 \u03bb\u2192                                   11-11 Variants                                           136\n\u2192 fix                  Extends 9-1 \u03bb\u2192                                   11-12 General recursion                                  144\n\u2192 B List               Extends 9-1 \u03bb\u2192 with 8-1 Booleans                 11-13 Lists                                              147\n\u2192 Unit Ref             Extends 9-1 \u03bb\u2192 with 11-2 Unit                    13-1 References                                          166\n\u2192 error                Extends 9-1 \u03bb\u2192                                   14-1 Errors                                              172\n\u2192 error try            Extends 9-1 \u03bb\u2192 with 14-1 Errors                  14-2 Error handling                                      174\n\u2192 exceptions           Extends 9-1 \u03bb\u2192                                   14-3 Exceptions carrying values                          175\n\u2192 <: Top               Extends 9-1 \u03bb\u2192                                   15-1 Simply typed lambda-calculus with subtyping (\u03bb<:)   186\n\u2192 {} <:                Extends 15-1 \u03bb<: and 11-7 Records                15-3 Records and subtyping                               187\n\u2192 <: Bot               Extends 15-1 \u03bb<:                                 15-4 Bottom type                                         192\n\u2192 <> <:                Extends 15-1 \u03bb<: and 11-11 Simple variant rules  15-5 Variants and subtyping                              197\n\u2192 {} <:                Extends 15-1 \u03bb<: and 15-3 Records and subtyping  16-1 Subtype relation with records (compact version)    211\n\u2192 {} <:                                                                 16-2 Algorithmic subtyping                               212\n\u2192 {} <:                                                                 16-3 Algorithmic typing                                  217\n\u2192 u                    Extends 9-1 \u03bb\u2192                                   20-1 Iso-recursive types (\u03bbu)                            276\n\u2192\u2200                     Based on 9-1 \u03bb\u2192                                  23-1 Polymorphic lambda-calculus (System F)              343\n\u2192\u2200\u2203                    Extends 23-1 System F                            24-1 Existential types                                   366\n\u2192\u2200\u2203 Top                Based on 23-1 System F and 15-1 simple subtyping 26-1 Bounded quantification (kernel F<:)                 392\n\u2192\u2200\u2203 Top full           Extends 26-1 F<:                                 26-2 \"Full\" bounded quantification                       395\n\u2192\u2200<: Top \u2203             Extends 26-1 F<: and 24-1 unbounded existentials 26-3 Bounded existential quantification (kernel variant) 406 \n\u2192\u2200<: Top                                                                28-1 Exposure Algorithm for F<:                          418\n\u2192\u2200<: Top               Extends 16-3 \u03bb<:                                 28-2 Algorithmic typing for F<:                          419\n\u2192\u2200<: Top               Extends 16-2 \u03bb<:                                 28-3 Algorithmic subtyping for kernel F<:                422\n\u2192\u2200<: Top full          Extends 28-3                                     28-4 Algorithmic subtyping for full F<:                  424\n-\u21d2                     Extends 9-1 \u03bb\u2192                                   29-1 Type operators and kinding (\u03bb\u03c9)                     466\n-\u2200\u21d2                    Extends 29-1 \u03bb\u03c9 and 23-1 System F                30-1 Higher-order polymorphic lambda-calculus (F\u03c9)       450\n-\u2200\u2203\u21d2                   Extends 30-1 F\u03c9 and 24-1                         30-2 Higher-order existential types                      452\n-\u2200\u21d2<: Top              Based on 30-1 F\u03c9 and 16-1 kernel F<:             31-1 Higher-order bounded quantification (F\u03c9<:)          470\n-\u2200<: Top {}\u2190           Based on 26-1 F<: with 11-7 records              32-1 Polymorphic update                                  485  \n\n\nAdvanced Topics in Types and Programming Languages\nEdited by Benjamin c. Pierce\n(WorldCat) (Site)\n\nPractical Foundations for Programming Languages Second Edition\nby Robert Harper\n(WorldCat) (Site)\n\nThis text develops a comprehensive theory of programming languages based on type systems and structural operational semantics. (from publisher site)\n\n\nAs noted in a comment by Scott Morrison\n\nIf you have a more mathematical background, I think the first section of the \"Homotopy Type Theory\" book (freely available on line) is excellent. That section is just about dependent type theory, and one can stop before meeting the \"homotopy\" part.\n\nHomotopy Type Theory - Univalent Foundations of Mathematics\nA collaborative effort.\nContents\nI Foundations\n\nType theory\nHomotopy type theory\nSets and logic\nEquivalences\nInduction\nHigher inductive types\nHomotopy n-types\n\nII Mathematics\n\nHomotopy theory\nCategory theory\nSet theory\nReal numbers\n\nAppendix\n\u00a0\u00a0\u00a0A Formal type theory\nIndex of symbols\nThe pdf version of the book now a bit harder to find starting from Google because of bad links. So here is a valid link.\n(pdf) (LaTeX) (GitHub)\n\nProofs and Types\nby Jean-Yves Girard\n(pdf) (WorldCat) (PLS Lab)\nBased on a short graduate course on typed lambda-calculus given at the Universit\u00e9 Paris VII in the autumn term of 1986-7. (ref)\n\nSense, Denotation and Semantics\nNatural Deduction\nThe Curry-Howard Isomorphism\nThe Normalisation Theorem\nSequent Calculus\nStrong Normalisation Theorem\nG\u00f6del's system T\nCoherence Spaces\nDenotational Semantics of T\nSums in Natural Deduction\nSystem F\nCoherence Semantics of the Sum\nCut Elimination (Hauptsatz)\nStrong Normalisation for F\nRepresentation Theorem\n\n\nHandbook of Practical Logic and Automated Reasoning\nby John Harrison\n(Site) (WorldCat)\n(Code and resources)\nHOL Light was written by John Harrison (ref) who also authored \"Handbook of Practical Logic and Automated Reasoning\". Think of the book as a very detailed introduction to the code for HOL Light. (GitHub)\nThe book takes one from Boolean Logic all the way up to Interactive theorem proving.\n\nLectures on the Curry-Howard Isomorphism\nby M.H. Sorensen and P. Urzyczyn\n(WorldCat)\nContents\n\nType-free \u03bb-calculus\nIntuitionistic logic\nSimply type \u03bb-calculus\nThe Curry-Howard isomorphism\nProofs as combinators\nClassical logic and control operators\nSequent calculus\nFirst-order logic\nFirst-order arithmetic\nG\u00f6del's system T\nSecond-order logic and polymorphism\nSecond-order arithmetic\nDependent types\nPure type systems and the \u03bb-cube\n\n\nML for the Working Programmer 2nd Edition\nby L.C. Paulson\n(WorldCat) (Site)\nMost of the book is a standard book for learning a programming language.\nChapter 10 A Tactical Theorem Prover is the icing on the cake.\n\nHandbook of Automated Reasoning\nEdited by Alan Robinson and Andrei Voronkov\n(WolrdCat)\n2 volume set.\nPublished in 2001, most of it is foundational and still of value today.\n\nAs you noted recommended tutorials are worth their weight in gold.\n\nTyping rules of proof assistants\n\nLean\n\"The Type Theory of Lean\"\nby Mario Carneiro\n(pdf)\n\nCoq\nDocs \u00bb Core language \u00bb Typing rules\n\n\n\nLambda Cube\n(Wikipedia)\n\nIn mathematical logic and type theory, the \u03bb-cube (also written Lambda Cube) is a framework introduced by Henk Barendregt to investigate the different dimensions in which the calculus of constructions is a generalization of the simply typed \u03bb-calculus. Each dimension of the cube corresponds to a new kind of dependency between terms and types.\n(from Wikipedia)\n\n\n\nIf you cover all of that then you can jump into this Q&A.\nWhat are the bases for different Proof Assistants?\n\nObviously this list can go on an on. My main purpose with this answer was to get one going with their bootstraps.\n\n\nBonus section for those who do not have a background in logic.\nforall x - An Introduction to Formal Logic\nby P.D. Magnus\n(pdf) (Site)\n\nforall x is an Open Education Resource (OER) introductory textbook in formal logic. It covers translation, proofs, and formal semantics for sentential and predicate logic. (from homepage)\n\nSince this is an OER there are many follow on books based on this. (Google search)\n\n\nBonus section for those who do not  have a background in \u03bb-calculus.\nThe Lambda Calculus\nStanford Encyclopedia of Philosophy\n\"An Introduction to Functional Programming Through Lambda Calculus\"  by Greg Michaelson\n(WorldCat) (Site)\n\"The Lambda Calculus Its Syntax and Semantics\"\nby H.P. Barendregt\n(WorldCat) (Site)\n\n\nBonus section for those lost in the terminology\nStanford Encyclopedia of Philosophy\nnLab\nProofWiki\nPLS Lab\nWolfram MathWorld\nEncyclopedia of Mathematics\n\n\nBonus section for those needing the history of types.\nFunctions and Types in Logic, Language and Computation\nby Fairouz Kamareddine\n(pdf)\n\n\nBonus section of food for thought.\nCounterexamples in Type Systems\n\nThe \"counterexamples\" here are programs that go wrong in ways that\nshould be impossible: corrupt memory in Rust, produce a\nClassCastException in cast-free Java, segfault in Haskell, and so on.\nThis book is a collection of such counterexamples, each with some\nexplanation of what went wrong and references to the languages or\nsystems in which the problem occurred.\nIt's intended as a resource for researchers, designers and\nimplementors of static type systems, as well as programmers interested\nin how type systems fit together (or don't).\n\n\n\nBonus section for Prolog programmers.\nHint: SWI-Prolog\nCOT5315 Foundations of Programming Languages and Software Systems\nby Robert van Engelen and Steven Bronson\n(ref)\nProlog code that does rules of inference (think typing rules). The code no longer works with the current version of SWI-Prolog but if one knows Prolog it is not hard to update.\n\nFound this today on GitHub and gave it a try. It is a very gentle way to get started with Prolog and natural deduction. As the code is for a bachelor\u2019s project thesis don't expect much but the core prover (system.pl) seems to work. Still have to hand verify the examples.\nnatural-natural-deduction\nby Flip Lijnzaad\nRelated paper: Towards Automated Natural Deduction in Prolog (pdf)\nNote: I did not try to get the Python code working; I just edited the code into unit test and ran all the test.", "meta": {"post_id": 930, "input_score": 17, "output_score": 20, "post_title": "So many data types, so little time"}}
{"input": "In Agda, irrelevance is an annotation which marks a parameter, record field, or definition which \"will only be typechecked but never evaluated\", with the consequence that irrelevant arguments and record fields are ignored when checking equality.\nYou can define records with irrelevant fields, e.g. this \"Squash\" type whose elements are definitionally equal:\nrecord Squash {\u2113} (A : Set \u2113) : Set \u2113 where\n  constructor squash\n  field\n    .unsquash : A\nopen Squash\n\nand these fields are counted in absurd patterns, e.g.\nfoo : Squash \u22a5 \u2192 \u22a5\nfoo ()\n\nbut by default the projection .unsquash : Squash A \u2192 A doesn't actually exist, meaning you can't directly use irrelevant fields, even in an irrelevant context!\nThere is an option --irrelevant-projections which adds irrelevant projections for irrelevant record fields, but it's incompatible with --safe. Being able to use an irrelevant field in an irrelevant context seems pretty innocuous, so why is --irrelevant-projections unsafe?", "output": "--irrelevant-projections can be used to prove the irrelevance axiom mentioned in the Agda manual:\n.irrAx : \u2200 {\u2113} {A : Set \u2113} \u2192 .A \u2192 A\nirrAx x = unsquash (squash x)\n\nand conversely, irrelevant projections are justified by the irrelevance axiom:\n.unsquash' : \u2200 {\u2113} {A : Set \u2113} \u2192 Squash A \u2192 A\nunsquash' (squash x) = irrAx x\n\nIrrelevant projections and the irrelevance axiom do not compute (so e.g. unsquash (squash x) does not reduce to x). In a previous version of Agda, irrelevant projections computed, which made them inconsistent.\nAccording to Jespr Cockx, although irrelevant projections are no longer inconsistent, the irrelevance axiom is still non-conservative and implies some form of non-constructive choice.\nFor example, this function looks like a formulation of the axiom of choice:\nchoice : \u2200 {\u2113\u2081 \u2113\u2082} {X : Set \u2113\u2081} {Y : X \u2192 Set \u2113\u2082}\n       \u2192 (\u2200 x \u2192 Squash (Y x)) \u2192 Squash (\u2200 x \u2192 Y x)\nchoice f = squash \u03bb x \u2192 unsquash (f x)\n\nThe introduction of a non-conservative, non-constructive axiom which does not compute by necessity is, to the best of my knowledge, why --irrelevant-projections remains unsafe despite no longer being inconsistent.\nSee \"Why does it matter if canonicity holds for irrelevant types?\"\nfor more information on the potential implications of an irrelevant non-constructive axiom.\nAnyone interested in definitional proof-irrelevance may wish to consider using Prop instead, which can be used for the same purpose as irrelevance, but without so many caveats.\nSome of the information in this answer has since been included in the Agda manual.", "meta": {"post_id": 931, "input_score": 8, "output_score": 8, "post_title": "Why is `--irrelevant-projections` unsafe in Agda?"}}
{"input": "In the concluding statement of \"universe hierarchies\", Conor McBride calls it\n\n[...] that key lesson which I learned from James McKinna: never resort to polymorphism when initiality will do.\n\nHow can you recognize when you could use initiality instead of polymorphism, and why should you prefer to use initiality?", "output": "Initiality comes with a powerful universal property which allows you to, internally, prove statements about the constructions you perform. If you give me an element of data Nat = Z | S Nat, I can perform induction on it.\nPolymorphism on the other hand requires you to appeal to external principles such as parametricity (*) to recover similar reasoning principles. If you give me a function of type (a -> a) -> a -> a, I will know it is bound to be a natural number in disguise but will be powerless to exploit that fact.\nAdditionally in predicative systems with a tower of universes, initiality is typically size-respecting whereas an encoding via polymorphism will land you a definition that lives one level up in the hierarchy due to the universal quantification. If you want to iterate these constructions, that's bad news.\n(*) caveat: there are type theories attempting to internalise parametricity", "meta": {"post_id": 939, "input_score": 15, "output_score": 21, "post_title": "Why should you \"never resort to polymorphism when initiality would do\"?"}}
{"input": "From my rough understanding, an (external/internal) realignment property says that given a type $A$, a proposition $p : \\Omega$, and a partial isomorph $B : p \\to \\sum_{A'} A' \\cong A$, we can extend it to a total one, such that it is strictly equal to $B$ given that $p$ is true.\nSterling et al. used this technique when building models for cubical type theories. However, in Sterling's thesis, he also extensively used realignment to adjust various definitions when setting up Tait's computibility for MLTT.\nI cannot (yet) fully appreciate the power of realignment. It seems to resemble the cubical gluing operation, so I'd expect it to be useful there. But is it necessary (or to put it another way, does it greatly simplify various arguments) for non-univalent type theories?\nMore specifically, even if assuming the realignment property holds in some topos helps, we still need to prove this property when actually instantiating a concrete topos, right? I cannot locate such a proof anywhere in Sterling's thesis, perhaps there's something that I overlooked? Any help is appreciated!", "output": "This is a very good question.\nWhat is realignment for?\nOne application of realignment that is particularly useful is to construct a cumulative hierarchy of universes; what cumulativity means in the \"objective\" sense is that you have a hierarchy U1 >-> U2 >-> U3 ... where each coercion is injective, and you have a choice of type operations at each level i like Pi_i x:A. B(x)  such that the coercion from Ui to Uj commutes with the chosen type operations. It is semantically very hard to get this to happen!\nRealignment is, however, much stronger than cumulativity; so if you have a hierarchy of universes linked by injective coercions but you have chosen type operations that do not commute with these operations, realignment can be used to replace all these type operations with ones that do commute with the coercions.\nThe cumulativity example is nice --- since if you have realignment in a sheaf topos (which we, in fact, do have in every sheaf topos as I will explain in the second half of my answer) then we have a strict interpretation of full MLTT with cumulative universes. Thus we can use a very naive denotational semantics to prove a variety of independence results for MLTT quite easily. This contradicts almost a decade of regrettable misinformation and confusion on the topic of universes in sheaf topoi.\nIn addition to cumulativity, realignment is, as you point out, the critical ingredient for constructing univalent universes in models of homotopy type theory. It is also useful for synthetic Tait computability, as you mention. Can we find uses for it in \"ordinary\" work-a-day type theory? My work recently has focused on finding applications for the modalities of synthetic Tait computability that have nothing to do with Tait computability --- for instance, the cost aware logical framework and my recent account of type based security/information flow control with Bob Harper. In these cases, we can imagine a type theory where realignment would play some useful role internally --- i.e. a new \"glue\" connective that lets you make a type that restricts up to definitional equality to some other type underneath a certain security level or phase, etc. It's unclear how useful this will be, but it could be useful.\nWhat models of realignment do we have?\nYour other question about whether you can in fact validate realignment in semantic models is also excellent. The state of play is that so long as your ambient mathematical universe is boolean (as $\\mathbf{Set}$ is), then any Grothendieck topos has a hierarchy of universes satisfying realignment. This is proved by Daniel Gratzer, Mike Shulman, and myself in our recent manuscript Strict universes for Grothendieck topoi --- the results were complete at the time that I claimed \"realignment is fine in topoi\" in my thesis, but the paper was not ready to be released unfortunately.\nThere are a number of open questions remaining.\n\nHofmann and Streicher give a simple construction of universes in any presheaf topos, and this is perfectly constructive. However Andrew Swan has proved that this particular universe construction cannot satisfy realignment for all monomorphisms unless the ambient mathematical universe is boolean. Meanwhile Gratzer, Shulman and Sterling have given a different construction of universes that applies in any Grothendieck topos, but their construction is very classical and employs both LEM and AC. Is there a modification of the GSS construction that works constructively and hence over a non-boolean base topos? We expect that the base topos will nonetheless need to satisfy a weak / constructively acceptable choice principle such as WISC.\n\nA less ambitious but also reasonable question is whether there is a universe construction specifically for realizability topoi that satisfies realignment, assuming perhaps that we are looking at $\\mathbf{RT}(\\mathbb{A})$ for a pca $\\mathbb{A}$ internal to a boolean topos like $\\mathbf{Set}$.\n\n\nAs far as I am aware, both of these questions are open. Daniel and Mike and I tried really hard to find a constructive version of our work, but got stuck.", "meta": {"post_id": 942, "input_score": 10, "output_score": 11, "post_title": "What is realignment and is it useful in non-univalent theories?"}}
{"input": "You can encode ordinals in Coq as\nInductive ord := O | S (n: ord) | Lim (s: nat -> ord).\n\nSuppose you use the following encoding instead\nCoInductive stream A := {\n  head: A ;\n  tail: stream A ;\n}.\nInductive ord := O | S (n: ord) | Sup (s: stream ord).\n\nCan you make an induction principle go through?\nI tried in both Coq and Agda but the obvious induction principle doesn't seem to work\n{-# OPTIONS --guardedness #-}\nmodule ord where\n\nrecord stream (A : Set) : Set where\n  coinductive\n  constructor _::_\n  field\n    head : A\n    tail : stream A\n\nrecord Forall {A : Set} (P : A \u2192 Set) (x : stream A) : Set where\n  coinductive\n  constructor _:>_\n  field\n    forhead : P (stream.head x)\n    fortail : Forall P (stream.tail x)\n\ndata ord : Set where\n  o : ord\n  s : ord \u2192 ord\n  sup : stream ord \u2192 ord\n\nopen stream\nopen Forall\n\nind : (P : ord \u2192 Set) \u2192 P o \u2192 ((x : ord) \u2192 P x \u2192 P (s x)) \u2192 ((x : stream ord) \u2192 Forall P x \u2192 P (sup x)) \u2192 (x : ord) \u2192 P x\nind P onO onS onSup = loop where\n  loop : (x : ord) \u2192 P x\n  loop o = onO\n  loop (s x) = onS x (loop x)\n  loop (sup x) = onSup x (gen x) where\n    gen : (y : stream ord) \u2192 Forall P y\n    forhead (gen y) = loop (head y)\n    fortail (gen y) = gen (tail y)\n\n(Agda termination checker complains about the recusive calls to gen and loop.)", "output": "It makes sense to want something like this, but Agda's termination/productivity checker does not actually validate this interpretation of the types. The reasoning behind your induction principle is presumably:\n\n$\\mathsf{stream}\\ A$ is an infinitely long stream of whatever $A$ is.\n$\\mathsf{ord}$ is a well-founded tree that delegates to $\\mathsf{stream}$ to achive an infinitely wide branching factor.\n\nHowever, Agda accepts the following definition:\nbad : ord\nbads : stream ord\n\nbad = lim \u03bb where\n  .head \u2192 zero\n  .tail \u2192 bads\n\nbads .head = bad\nbads .tail = bads\n\n$\\mathsf{bad}$ is not a well-founded tree, and contradicts the induction principle:\nP : ord \u2192 Set\nP zero = \u22a4\nP (suc o) = \u22a4\nP o@(lim s) = o \u2261 head (tail s) \u2192 \u22a5\n\nlemma : \u22a5\nlemma =\n  ind P _ (\u03bb _ _ \u2192 _)\n    (\u03bb s fp eq \u2192 subst P (sym eq) (forhead (fortail fp)) eq)\n    bad refl\n\nThe issue is that coinduction in Agda (and I suppose also Coq) does not really act in a 'compositional' way. You can't just say that $\\mathsf{ord}$ is the initial algebra of the functor $1 + o + \\mathsf{stream}\\ o$; it actually has some more complicated specification owing to $\\mathsf{stream}$ being defined coalgebraically.\nThere are some (not great) justifications you could use for this behavior. The above functor is not strictly covered by the more rigorous foundational schemas for (co)inductive definitions, so in that sense it requires some interpretation, and that interpretation could well be one that matches Agda's checker. Also, if the two types were mutually defined, it's a lot less clear whether it should mean something primarily algebraic or coalgebraic. But it's unfortunate that some mixed (co)inductive specifications just can't be written in Agda/Coq.\n(Note, I'm not very familiar with coinduction in Coq; just assuming based on the question that it behaves similarly to Agda.)\nAddendum: if you want to more reliably predict what definitions Agda will accept for types involving coinduction, it helps to imagine that they are actually picking out certain well-defined values of an analogous domain-theoretic definition (where coinduction is more 'lazy'). But even that is not completely straight forward. For instance, using\nbad = lim bads\n\nabove would be rejected.", "meta": {"post_id": 961, "input_score": 12, "output_score": 8, "post_title": "Is induction over mutually inductive coinductive types possible?"}}
{"input": "The Markov's principle states that if a Turing machine does not run forever, then it halts. Equivalently, if I have a function $f : \\mathbb N \\to \\mathrm{Bool}$, such that I have proved that $\\neg\\neg \\exists n. f(n)=\\mathrm{true}$, then I can find $\\exists n. f(n)=\\mathrm{true}$.\nThis principle is sometimes considered as constructively valid, since we can give a program that just enumerates $\\mathbb N$ and searches for the solution, and if our meta-logic is classical, we can see that this program halts, which means that it is constructively true for each concrete function $f$.\nWhat are the best practices to incorporate this principle into various proof assistants? The most na\u00efve approach is, of course, assume it as an axiom. But I would also like to have computational properties. So for each concrete function $f$, I can do this in Agda:\nf : Nat -> Bool\nf = blah blah\n\nf-good : \u00ac \u00ac \u2203[ x \u2208 Nat ] f x \u2261 true\nf-good = blah blah\n\nprivate\n  {-# TERMINATING #-}\n  helper : Nat -> \u2203[ x \u2208 Nat ] f x \u2261 true\n  helper n with f n in eq\n  ... | true = n , eq\n  ... | false = helper (suc n)\n\nmagic : \u2203[ x \u2208 Nat ] f x \u2261 true\nmagic = helper 0\n\nI have to seal helper as private, because it indeed does not terminate on some input. But given f-good, we can be (meta-classically) sure that magic will halt.\nHowever, this has some drawbacks:\n\nIf the normalization strategy is wrong, then this program will not terminate. For instance, if Agda chose to first fully expand all the branches before picking one, then it will keep entering the false branch.\nIf I try to state the general Markov's principle instead of on concrete $f$'s, then Agda will indeed loop in certain cases.\nI might accidentally make some unsafe leaks.\n\nIs there any better way of doing this? Also, is there any proof assistant that actually supports this with a flag or something?", "output": "There are different ways to implement Markov's principle, and they're not equivalent. I know of at least three techniques to implement MP.\nMP as a loop\nI personally find this to be a hack inherited from untyped realizability. As you observed already, it breaks the good metatheoretical properties of your system, in particular strong normalization. It also requires some form of MP in the metatheory, which is not a blocker but not great either. I guess that if your favourite proof assistant is some variant of PRL this is not so much a problem, but if you only swear by MLTT this is not going to do the job.\nMP via Dialectica\nIt is well-known that the Dialectica interpretation realizes MP. The underlying implementation is very different from an unbounded loop. Morally you can picture it as some form of delimited continuations triggered by the access to bound variables in the calculus. I wrote a Dialectica model of CoC in my PhD, and we have an unpublished account of a Dialectica model of MLTT with Andrej Bauer, but it's not very implementable. It relies on a structure of finite multisets that enjoy definitional equations and it is not even clear we can get a decidable type system out of it.\nMP by static exceptions\nThe third technique is, I believe, a simplification of the Dialectica approach that is easy to explain computationally and implement. Essentially, it consists in adding a special kind of exceptions to the ambient type theory. In order to preserve consistency you have to forbid exceptions at toplevel though, and ensure that they are always statically caught.\nThere are two ways to understand this, either in direct style or through a program translation. These two papers only handle a weak logical system, but the technique can actually be lifted to MLTT.\nThe resulting theory can be described as the composition of an exceptional model with a strict presheaf model. Both processes are relatively simple compilation phases so in the end you can just compile away MLTT + MP into a reasonable variant of MLTT. The result is described in this paper.\nSince the presentation is synthetic, you can actually dump the syntactic model phase altogether and simply add the corresponding combinators with the reduction rules inherited from the underlying computational model. Dynamic exceptions are fairly easy to add through rewrite rules so I guess it is only a matter of correctly implementing the static try-catch handlers.", "meta": {"post_id": 972, "input_score": 12, "output_score": 15, "post_title": "Incorporating Markov's principle in various proof assistants"}}
{"input": "I've been looking at syntactic multicategories for mechanizing some type theory stuff.\nBut multicategories are pretty messy to work with in a two sorted definition like the usual dependently typed encoding.\nStating the operations alone without the laws is a mess. You need extra Forall and concat helpers too to iterate over g the list of argument Homs.\nIn practice you'd also add another layer of finickyness with setoids.\nClass Multicategory := {\n  Obj: Set ;\n  Hom: list Obj -> Obj -> Set ;\n\n  id A: Hom [A] A ;\n  compose {B C}:\n    Hom B C ->\n    forall (g: Forall (fun o => { a & Hom A o }) B),\n    Hom (concat g) C\n}.\n\nThe two sorted definition as a monad in a multispan is more sensible but loses some ease of use.\nI'm thinking about trying the single sorted definition or the colored operad approach.\nI'd be curious if anyone has any concrete experience with mechanizing this sort of thing.", "output": "[I'm posting this by proxy for Astra, who is having technical difficulties posting on this site.]\nHey! I recently completed a large scale formalisation in which I independently rediscovered cartesian multicategories as being the exactly correct and most economic way of encoding the structure of non-dependent type theory. (I refer to these as Simple Contextual Categories.) This notion is completely central to all of my constructions.\nMy project was to formalise Altenkirch's paper Categorical reconstruction of a reduction-free normalisation proof. The main idea is that simply typed lambda calculus is the language of cartesian closed simple contextual categories. We obtain a normalisation proof by looking at two such (CCSC) categories: presheaves and twisted glueings. I think that the main contribution of the formalisation is a particularly nice formulation of simple contextual categories, which seems to be what you're asking about. The paper on this formalisation is currently in the process of being written, but the code has been online for some time now.\nIn order to read the definition, first read the first 28 lines, plus the definition of derive, in the following file:\nhttps://github.com/FrozenWinters/stlc/blob/main/lists.agda\nSubsequently, you can go directly to the definition of a simple contextual category in this file:\nhttps://github.com/FrozenWinters/stlc/blob/main/contextual.agda\nThe main definition is in lines 28-63 of the second file, and we then go on to develop the theory of weakenings and variables that is inherent to every simple contextual category.\nThe bulk of the project consists of constructing examples of such structures.\nI believe that there is a dependent analogue of this construction; this is a current research question on which some progress has been made.", "meta": {"post_id": 985, "input_score": 11, "output_score": 8, "post_title": "Ergonomic use of multicategories in proof assistants"}}
{"input": "Is there a theory of typing proof objects or proof trees?\nPartially inspired by this question and answer, I was wondering about separating type checking into two phases: the first phase infers a full proof tree and the second phase checks if the proof tree is well formed and conclusions follow from premises.\nPotentially, this sort of separation allows a smaller kernel. Also the kernel can be written as a total function within a prover of similar power without extra axioms. I think it's the driver which infers the proof trees which would require extra axioms.\nWhat I mean is you can provide explicit witnesses for typing judgements.\nFor the STLC, something like this:\n$$\\frac{\\begin{split} q_0 \\colon [ \\Gamma \\vdash e_0 \\colon \\tau_0 \\rightarrow \\tau_1 ]  \\\\\nq_1 \\colon [ \\Gamma \\vdash e_1 \\colon \\tau_0 ]\n\\end{split}}{ q_0 q_1 \\colon [ \\Gamma \\vdash e_0 e_1 \\colon \\tau_1 ] } $$\n$$\\frac{q \\colon [ \\Gamma , x \\colon \\tau_0 \\vdash e \\colon \\tau_1 ]}{ \\lambda x \\colon \\tau_0. q \\colon [ \\Gamma \\vdash \\lambda x \\colon \\tau_0 . e \\colon \\tau_0 \\rightarrow \\tau_1 ] } $$\n$$\\frac{x \\colon \\tau \\in \\Gamma}{ x \\colon \\tau \\in \\Gamma \\colon [ \\Gamma \\vdash x \\colon \\tau ] } $$\nThe meat and potatoes is when you have more complicated features like dependent types. You'd want explicit witnesses for judgemental equalities something like:\n$$ \\frac{\\begin{split}q \\colon [ \\Gamma , x \\colon \\tau \\vdash e \\colon  \\tau' ] \\\\\nq' \\colon [ \\Gamma \\vdash \\colon e' \\colon \\tau ]\\end{split}}{\\beta(x \\colon \\tau \\mathrel{:=} q' , q) \\colon [ \\Gamma \\vdash (\\lambda x\\colon \\tau. e) e' \\triangleright_\\beta [x \\mathrel{:=} e'] e ]} $$\n$$ \\frac{\\begin{split} q \\colon [\\Gamma \\vdash \\tau \\equiv \\tau'] \\\\\nq' \\colon [\\Gamma \\vdash e \\colon \\tau]\n\\end{split} }{ \\textbf{J}(q, q') \\colon [ \\Gamma \\vdash e \\colon \\tau' ]} $$\nAnyhow, it quickly gets very redundant, confusing and unreadable.\nI'm confused about several aspects of this.\n\nYou want proof trees to be trivial to check but you also want them to not be redundant. What should it mean for it to be \"trivial\" to infer a property of the root of the proof tree like the environment or type?\n\nIt's really confusing to notate proof trees. Is there a better way of writing this silliness?\n\nAre there existing references or work in this direction?\n\nIt's hard to come up with a reasonable meaning for proof trees. I guess you can see them as terms annotated with explicit environments and types?", "output": "What you are describing already happens to a large extent in a proof assistant with a trusted kernel.\nSide remark: I prefer to say \"derivation\" instead of \"proof tree\" because derivations need not be about proofs, they can also be about constructions.\nUser input typically requires elaboration to a more explicitly annotated type theory (the process of elaboration is intertwined with the process of type-checking), let us call it the core type theory. The core type theory is designed in such a way that the judgements already encode the proof trees, except for judgemental equalities.\nFor example, in the simply-typed $\\lambda$-calculus with explicitly annotated types of variables ($\\lambda (x : A) . e$ instead of just $\\lambda x . e$), the derivation of $\\Gamma \\vdash e : A$ can be read off immediately from $e$. There is no point in storing any extra information anywhere, as it is all there already. One only has to process $e$ to either reconstruct the derivation or decide that $e$ does not encode a valid one. This is the meaning of \u201cterms as proofs\u201d.\nProofs of judgemental equalities are typically not recorded for two reasons:\n\nThey are proof irrelevant: it does not matter which proof is provided, as long as there is one. Consequently, if we have the means to reconstruct them when they exist, we need not store them.\n\nIt is often deemed that derivations of judgemental equalities are too large to be stored. Instead, the core type theory is designed to have decidable equality checking so that the missing derivations can be reconstructed algorithmically. In a sense we are storing the derivations in time rather than in space.\n\n\nTo specifically address your questions:\n\nThere are techniques for making derivations less redundant, such as bidirectional type-checking which synthesizes some information. You are correct that redundant information is a nuisance because we need to check that the redundant copies agree.\n\nWith the exception of judgemental equalities, the derivations are already encoded by the terms, at least when the core type theory is explicit enough. (See the notion of standard type theory in the reference given in the next point.)\n\nThe idea that the derivation is directly reconstructible from the proof terms is manifested as an inversion principle, see Theorem 3.24 in Finitary type theories with and without contexts by Philipp Haselwarter and myself, and the subsequent paragraph.\n\nAs already said, when the theory is nice enough the derivation is encoded by the judgement, so its meaning can be taken to be the same as the meaning of the judgement. If the theory is not nice enough, then there is another theory which is nice enough that covers the original one, in which case the meaning of the former can be derived from the meaning of the latter. This is the point of the Elaboration theorem in Chapter 10 of Anja Petkovi\u0107 Komel's doctoral dissertation.", "meta": {"post_id": 994, "input_score": 7, "output_score": 9, "post_title": "Typing proof trees or proof objects"}}
{"input": "I'm working up from elaboration zoo and noticed that you don't use fixed point if you've got type level computation. It causes unification/equality check to hang up. Now, this means that I need inductive data structures of some kind if I want to compute on type-level.\nWhat would be the easiest way to introduce inductive types?\nW-type has a description that could be interpreted, but it is potentially cumbersome in practice.\nW : {A : Set} \u2192 (B : A \u2192 Set) \u2192 Set\nsup : (a:A) \u2192 {B} \u2192 (B a \u2192 W B) \u2192 W B\nelim : {B} \u2192 (C : W B \u2192 Set) \u2192 ((a:A) \u2192 (f : B a \u2192 W B) \u2192 (c : (b : B a) \u2192 C (f b)) \u2192 C (sup a f)) \u2192 (w:W B) \u2192 C w\n\nFor instance, with nat we'd have:\nNat : W 2 {empty, unit}\n\nzero : Nat\nzero = sup 0 absurd\n\nsucc : Nat \u2192 Nat\nsucc n = sup 1 (\u03bb_. n)\n\nElimination without pattern matching would be inconvenient, but doable.\nI have a hunch that there's not many ways to put this together. It's some variation of W-type that I would have to implement. But which variant?", "output": "In principle, every indexed inductive type can be defined from $\\bot$, $\\top$, $\\mathsf{Bool}$, $\\Pi$, $\\Sigma$, $\\mathsf{W}$, $\\mathsf{Id}$ and universes, with exactly the expected eliminators and computation rules: https://jashug.github.io/papers/whynotw.pdf. But this has a lot of encoding overhead.\nIf we want ergonomic support for inductive types, that's not simple at all and we have a number of design choices. There's a basic trade-off between ergonomics and implementation complexity.\n\nA very basic choice is between eliminators and recursive definitions (including fixpoints). Eliminators are formally the easiest, but can be very inconvenient in practice. Recursive definitions tend to require a lot more effort to implement.\nCoq has inductive signatures, case splitting and fixpoints. As you mention, fixpoints diverge if we try to naively compute them. In Coq, termination checking marks an argument of a fixpoint as decreasing, and fixpoints only compute when applied to a canonical value in the decreasing argument position.\nMini-TT allows recursive definitions, without termination checking. The obvious downside is that users have to perform termination checking instead (when necessary), but it's much easier to implement. However, we still have to avoid divergent unfolding. The Mini-TT solution is essentially to disallow unfolding of recursive names in certain contexts, so that e.g. we can't prove that a renamed version of a recursive function is convertible to the original version. Compare Coq, where fixpoint bodies can be compared for conversion. However, comparing recursive function bodies is not super useful in practice, because $\\beta\\eta$-conversion of definitions is far less common than extensional (pointwise) equality of functions. We usually want function extensionality, and richer $\\beta\\eta$-conversion by itself is not sufficient.\nAgda has recursive definitions and termination checking, and no fixpoints and no conversion checking for recursive function bodies. Agda has by a fair margin the most powerful termination checker and the most powerful pattern matching elaboration. This is often very convenient, but the implementation is not for the faint of heart: https://dl.acm.org/doi/10.1145/3236770\n\nAnother variation is the range of allowed inductive types. Here, the easy choice is to stick to the inductive types which are reducible to W-types; these are roughly Dybjer's classic inductive families: https://www.researchgate.net/publication/226035566_Inductive_families.\nGoing beyond this is another way of complicating implementations. Induction-recursion and induction-induction are the most notable generalizations. These can be also implemented using eliminators or recursive definitions, but right now only Agda supports them. In Agda, induction-recursion and induction-induction arise as a \"side effect\" of positivity checking, termination checking and mutual definitions, but nothing prevents us from doing an eliminator-based implementation which is more amenable to small core syntaxes.", "meta": {"post_id": 999, "input_score": 11, "output_score": 13, "post_title": "Easy ways to introduce inductive types"}}
{"input": "In A Cubical Language for Bishop Sets, the authors write:\n\nAs a consequence of its fundamentally untyped nature, formalizing a\ntheorem in Nuprl does not imply the correctness of the corresponding\ntheorem in standard classical math-ematics (the global mathematics of\nconstant or discrete sets), nor even in most forms of constructive\nmathematics (the local mathematics of variable and cohesive sets). It\nis worth noting that the problem is not located in the presence of\nanti-classical principles (which are interpretable in logic over a\nvariety of topoi), and rather arises from the commitment to untyped\nontology.\n\nWhat is meant by this statement? I would wager that the NuPRL team disagrees, as some of their publications are about mathematical theorems, including: Formalizing Category Theory and Presheaf Models of Type Theory in Nuprl and Constructing Analysis and Experimental Mathematics using the NuPRL Proof Assistant.\nIs there an example of a theorem that is provable in NuPRL, but that a mathematician would reject as false? Also, it shouldn't be conjured up in ZFC versus MLTT, as theorems like $1 \\in 2$ are generally understood and happily ignored.", "output": "As an author of this paper, I think I would write it a little bit differently if I had the chance today. Before I begin, let me echo Andrej's comment that Nuprl is a very significant moment in the history of type theory and even constructive mathematics, and none of what I say (then or now) is meant to detract from that. I obviously have immense respect for Nuprl and the people who built it, who were in many ways pioneers in type theory and in interactive proof assistants. Much of what we have today (like Coq and Agda) owes its present shape to the ideas developed by the PRL Group.\nTo me the difficulty with relating Nuprl to mathematics is basically one of methodology. As Andrej says, Nuprl's Computational Type Theory is based on \"truth in one model\"; as a result, there are many things that are true in this specific model that are false in the category of sets, false in many categories of presheaves, and false in many categories of sheaves. This is not the fault of (e.g.) realizability semantics, but rather the fault of confounding syntax and semantics. Both are important, but semantics benefits from multiplicity --- and the multiplicity of semantics is embodied in syntax. We can therefore expect strange results if we say that syntax is just a way to speak about one single example of semantics.\nSo my aim is not to say \"realizability is bad\" --- realizability is clearly very good. But I think it is bad on balance to base a proof assistant on one single model (bad in ways that COULD NOT have been anticipated [clarification: by that community] in the early 1980s when this was going on!) because it limits the applicability of your results.\nBecause Nuprl incorporates axioms that are not true in ordinary math, nor in the relative ordinary math of topoi, we cannot take a Nuprl proof about groups and use it as evidence to a \"proper mathematician\" for the truth of that statement about groups in a way that applies to that mathematician's work. This limits the ability to communicate and re-use results, but that is to me the entire point of mathematics.\nI want to end by saying that my perspective on mathematics is not the only one. Nuprl is much inspired by the ideas of L.E.J. Brouwer who took a very different viewpoint --- a proof in Brouwer's style about groups also does not necessarily lead to evidence that a mathematician would accept for the truth of that statement about groups. But Brouwer's perspective was that all the mathematicians were wrong, and that only he was right. If that was actually so, then one could not blame him for doing his proofs in a way that was not backward compatible.\nTherefore, the question that Nuprl raises is nothing less than: is mainstream mathematics wrong? Back when I was building tools based on Nuprl, I believed that normal mathematics was wrong. I no longer believe that though.\nLastly, I want to say that the above applies to Nuprl when viewed as a general-purpose language for mathematics --- for the reasons I describe, it cannot be a general-purpose language in that sense. But this is not the only thing Nuprl has been: Nuprl has also been a \"synthetic domain theory\" (an implementation of the slogan \"domains are just types, don't worry about continuity\" --- albeit quite different from the one pursued by the category theorists). Nuprl's synthetic domain theory aimed to let you reason about programs that use partial functions in ways that don't terminate, like LCF. One can argue today whether their approach was the best or not, but I think this is a very interesting aspect of Nuprl that is less well known than it should be. Nuprl qua synthetic domain theory doesn't claim to be general purpose, only useful for a specific purpose. So I think that's a good way to think about Nuprl that respects the great accomplishments of that team.", "meta": {"post_id": 1012, "input_score": 15, "output_score": 16, "post_title": "Can mathematical formalizations in NuPRL be trusted as correct in the greater mathematical practice?"}}
{"input": "I've heard about a folklore claim that\n\nIf all terms of \u2115 are literals, all closed terms admit canonical form.\n\nIn MLTT-style type theories.\nI am assured that it's true for Bool if one also assumes consistency, namely the canonicity for empty type. Unfortunately, I don't know much about the general case, and couldn't find references. I suppose that it's related to encoding terms by natural numbers or something similar...\nDoes anyone know a rigorous formulation or a proof (or a disproof)?", "output": "Your question is quite vague, so let me give you both an intuition on why this ought to be true, and a counterexample.\nAs for intuition, let me show how supposing only canonicity\nfor $\\mathbb{N}$ (natural numbers) one can deduce canonicity for $\\mathbb{B}$ (booleans). Suppose we are given a closed boolean $\\vdash b : \\mathbb{B}$, and consider\n$$\\mathtt{if}~b~\\mathtt{then}~0~\\mathtt{else}~0$$\nThis is a natural number in the empty context, so it must reduce to a canonical form. But to do so it must be the case that $b$ also reduces to\na canonical form, because this is the only way for the $\\mathtt{if}$ statement to reduce!\nAbstracting a bit, if we can construct a context $\\vdash C[\\cdot] : T \\Rightarrow T'$, that is a term with a hole such that whenever a term $\\vdash t : T$ is plugged in said hole the whole term is of type $T'$, and such that $C[t]$ has a canonical form only if $t$ has, then canonicity for $T'$ implies canonicity for $T$.\nBut there are perfectly valid cases where such a context cannot be built, and this is intentional! My favorite example is the sort of (proof-irrelevant) propositions $\\mathtt{SProp}$. As shown in a recent article (and its predecessor), you can prove canonicity for natural numbers in MLTT extended with $\\mathtt{SProp}$ without proving canonicity of inhabitants of strict propositions \u2013 you only have to assume consistency. The trick is to control the way one can use those to build relevant terms (in our case, natural numbers) so that no context like the above can be built. The idea is roughly to allow for an eliminator from $\\mathtt{SProp}$ to $\\mathbb{N}$ only for the false proposition. This ensures that a natural number in the empty context cannot be stuck on this eliminator since that would mean that one has an inconsistency in the theory \u2013 the term on which the eliminator is applied would be a proof of falsity in the empty context.", "meta": {"post_id": 1026, "input_score": 13, "output_score": 19, "post_title": "Does the canonicity of natural number imply that of all types?"}}
{"input": "Mostly when I read about impredicativity I see people bemoaning its downsides.\nBut it's not clear to me why I would want impredicativity in the first place.\nImpredicativity is useful for analyzing impredicative systems. And impredicative Pi types can be used to encode recursive datatypes without going up a universe. However, from what I understand Cedille has to go through a few contortions to have induction principles over impredicative encodings of datatypes.", "output": "There are some tricks that only work when you have access to an impredicative universe. They tend to construct \"the smallest object\" of some kind, without an explicit construction, i.e. a textbook impredicative encoding.\nA fancy one that comes to my mind is Mendler encoding. In a nutshell, given some function P : (Prop -> Prop) -> (Prop -> Prop) in CIC, this encoding constructs the \"free strictly positivization\" of P. This can be used to compose inductive predicate constructions in a modular way, since one effectively abstracts over the syntactic criterion of strict positivity.", "meta": {"post_id": 1037, "input_score": 17, "output_score": 8, "post_title": "What are the advantages to impredicativity?"}}
{"input": "I've been trying to prove this in Agda:\nlemma : (P : \u2115 \u2192 Set)\n  \u2192 ((n : \u2115) \u2192 Dec (P n))\n  \u2192 Dec (\u03a3[ n \u2208 \u2115 ] \u00ac (P n))\n\nIn words:  We have an infinite sequence of decidable propositions.  Can we conclude that either a) all the propositions are true or b) you can find a counterexample?\nI'm 70% sure this is not a conclusion you can draw without some extra axiom.  Is there a name for this, or some papers I can read?  If you need an extra axiom, what's a good one to go with?\n(I came to this after working for a bit on Konig's Lemma, which I also suspect can't be proven without some additional assumptions.)", "output": "This is equivalent to solving the Halting Problem (perhaps relativized to an oracle), so this is a very strong assumption.\nFix a Turing machine M, suppose P n is the statement that M does not halt after being run n steps. This is decidable by simply running the machine. However, \u03a3[ n \u2208 \u2115 ] \u00ac (P n)) asks whether it is possible to find an n such that M halts in n steps. That is undecidable for a general Turing machine M.\nAs a general principle, being able to solve the Halting Problem relative to any oracle, is equivalent to the Limited Principle of Omniscience (LPO) as mentioned by Trebor. K\u00f6nig's Lemma (KL) for general finitely branching trees is known to be equivalent to this, provided one has an enumeration of the nodes. In the most general possible setting, one might first need a weak form of choice \u2014 namely, to choose one from each of a countably infinite sequence of nonempty finite sets \u2014 and then use LPO to prove the full form of K\u00f6nig's Lemma.\nHowever, the Weak K\u00f6nig's Lemma (WKL), which is limited to trees of finite $0,1$-sequences is known to be strictly weaker. In fact, it is related to the Lesser Limited Principle of Omniscience (LLPO) in the sense that for any infinite decidable tree T of finite $0,1$-sequences there is an infinite sequences of LLPO instances such that any simultaneous solution to all of these can be used to compute an infinite path through T.", "meta": {"post_id": 1039, "input_score": 7, "output_score": 8, "post_title": "Decidability for an infinite sequence of decidable propositions"}}
{"input": "I have the following problem. I have a type $\\alpha$, function $f : \\alpha \\to \\mathbb{N}$ and predicate $P : \\alpha \\to \\mathrm{Prop}$ and I want to prove that for all $a : \\alpha, P a$.\nHow could prove this with induction on the value $f a$, i.e. cases where $f a = 0$ and $f a > 0$?\nMy current trial in Lean looks like this:\nlemma foo (\u03b1 : Type) (f : \u03b1 \u2192 \u2115) (P : \u03b1 \u2192 Prop)\n  (base : \u2200 a : \u03b1, f a = 0 \u2192 P a)\n  (ind : \u2200 a : \u03b1, f a > 0 \u2192 \u2203 b : \u03b1, f b < f a) :\n  \u2200 a : \u03b1, P a\n\nIn the inductive case, I want to get the hypothesis P b.\nI tried to use nat.strong_induction_on, but I can't figure out what to apply it on.\nAm I doing something wrong?", "output": "Here's how to prove Andrej Bauer's corrected statement using the induction tactic:\ntheorem foo (\u03b1 : Type) (f : \u03b1 \u2192 \u2115) (P : \u03b1 \u2192 Prop)\n  (ind : \u2200 a, (\u2200 b, f b < f a \u2192 P b) \u2192 P a) :\n  \u2200 a, P a :=\nbegin\n  intro a,\n  induction hn : f a using nat.strong_induction_on with n ih generalizing a,\n  apply ind,\n  intros b fb_lt_fa,\n  rw hn at fb_lt_fa,\n  exact ih _ fb_lt_fa _ rfl,\nend\n\nWe're using lots of its optional features at once here:\n\nhn : _ lets us remember that the variable we're inducting on is equal to f a\nusing nat.strong_induction_on tells it to use a non-default recursion scheme\ngeneralizing a ensure that a is in a binder in our inductive hypothesis ih\n\nNote that we don't need base.", "meta": {"post_id": 1049, "input_score": 8, "output_score": 10, "post_title": "Strong induction on \u2115 with function \u03b1 \u2192 \u2115"}}
{"input": "Sometimes, trying to use rw in Lean, we get an error saying\nmotive is not type correct\n\nWhat does this mean? Often simp_rw succeeds, so what we want to rewrite can be rewritten and the end: why rw is not enough?", "output": "This tends to show up when trying to rewrite a term that appears as a dependent argument.\n\nTo understand this, let\u2019s see how rw actually works, by way of a small example:\nI\u2019m using list.last with type\nlist.last {\u03b1 : Type u} (l : list \u03b1) : l \u2260 list.nil \u2192 \u03b1\n\nas an example for a function with a dependent argument: The second argument is a proof that mentions the first argument l.\nLet\u2019s try to prove the following lemma:\nopen list\n\nexample (\u03b1 : Type*) (l : list \u03b1) (h1 : l \u2260 nil) (x : \u03b1) (hx : l = [x]) :\n  last l h1 = x :=\nbegin\n\nWe might want to start to rewrite with that equality, but\n  rw hx,\n\nfails with\nrewrite tactic failed, motive is not type correct\n  \u03bb (_a : list \u03b1), l.last h1 = x = (_a.last h1 = x)\n\nAnd the error message sheds more light into why it fails: The rw tactic takes the goal, which is\nl.last h1 = x\n\nfinds the occurrences of the left-hand side of the equality, i.e. l, and then tries to equate the goal as we have it with the goal where l is replaced by a parameter _a. Adding one set of parenthesis clarifies things maybe a bit:\n  \u03bb (_a : list \u03b1), (l.last h1 = x) = (_a.last h1 = x)\n\nNormally rw can use this function to turn the equality given to it (hx) into an equality between the existing goal and the goal you\u2019d expect after the rw, but it doesn\u2019t even get that far: The expression above is not type-correct! In _a.last h1, the h1 is still l \u2260 nil, but it now should be _a \u2260 nil.\n\nUnfortunately, rw is too limited to take care of this automatically. Work-arounds are, as you say\n\nuse simp_rw, which is able to generalize goals even in these cases.\nBut has it\u2019s own shortcomings \u2013 you cannot rewrite with equalities like l = head l @ tail l that mention the LHS on the RHS. This is because simp_rw is based on simp, and as a simplification rule, this would be bad.\n\nuse simp only [hx] {single_pass := tt}, which is similar to simp_rw, but doesn\u2019t complain about such looping rewrites.\n\nIf the equality has just a variable on one of its side (as it does here), then\nsubst hx\n\nworks nicely.\n\nIf the dependent argument that gets in the way is a local hypothesis (such as the hx here), it can help to move it into the goal;\nrevert h1, rw hx, intro h1,\n\nworks here, and may work in your case. But it may not always be possible to revert all relevant hypotheses.\n\nIf none of these help, you can try to extract a helper lemma where the thing you want to replace is a variable, use subst to prove that lemma quite directly, and then use that lemma.", "meta": {"post_id": 1062, "input_score": 16, "output_score": 15, "post_title": "What does the \"motive is not type correct\" error mean in Lean?"}}
{"input": "I was looking at the MSC2020 database and I find it hard to identify a field suitable for works about proof assistants and formalised mathematics. 03B70 (\"Logic in Computer Science\") might be the closest one, but it does not look optimal to me. My questions are\n\nDo you think a better subject class exists?\nIf we end up agreeing that there is none (and that one might be needed), is there a way to propose this to the AMS?", "output": "TL;DR: Based on the descriptions in the MSC2020 alone, I would use the new section 68Vxx, specifically the subject classification 68V20 (or maybe 68V15) if one is formalizing mathematics in a proof assistant.  (However, 68 is CS, so if one is concerned that their work be recognized as pure mathematics, then maybe it shouldn't be the primary classification of the work.)\n\nThere appears to be a new section in the 2020 MSC Subject Classification:\n\n68Vxx Computer science support for mathematical research and practice\n\n68V05 Computer assisted proofs of proofs-by exhaustion type {For rigorous numerics, see 65Gxx; for proofs employing automated or interactive theorem provers, see 68V15}\n68V15 Theorem proving (automated and interactive theorem provers, deduction, resolution, etc.) [See also 03B35]\n68V20 Formalization of mathematics in connection with\ntheorem provers [See also 03B35, 68V15]\n68V25 Presentation and content markup for mathematics\n68V30 Mathematical knowledge management\n68V35 Digital mathematics libraries and repositories\n68V99 None of the above, but in this section\n\n\n\nNote some of the above subject classifications also cross reference 03B35, but this classification (from 2010) seems quite vague:\n\n03B35 Mechanization of proofs and logical operations [See also 68V15]\n\nEven, 03B70, the subject classification you mentioned (which seems to be the de facto standard in the small, biased selection  of arXiv papers I've looked at) also cross references 68-XX:\n\n03B70 Logic in computer science [See also 68-XX]\n\nLast, the computer science section 68-XX also mentions the availability of the classifications -04 for \"papers containing software, source code, etc. in a specific mathematical area\", e.g.\n\n14-04 Software, source code, etc. for problems pertaining to algebraic geometry\n\n\nThere is a small discussion on the Lean Zulip about this new section.\n\nHere are papers on arXiv using 68V20 and 68V15.\n\nI'd also love to get an answer from someone involved in the creation of 68Vxx or from anyone who feels that 68Vxx doesn't work for them.", "meta": {"post_id": 1071, "input_score": 16, "output_score": 14, "post_title": "Proof assistants and formalised mathematics in the MSC database"}}
{"input": "I keep seeing these phrasing in some proof assistants/elaborators and their issues/internal discussions (e.g. Github search results in cooltt), that seems not that related to the actual proofs/programs. So I guess it is some implementation/semantic specific stuff and \"fibration\" seems related to Pi types, and \"cofibration\" to intervals?\nI'm also curious about their intuitions/origins. It's always cool but kinda confusing to see these categorical constructs in the implementation for some naive code readers like me.", "output": "Let me provide a slightly watered up supplement of Gratzer's answer.\nIn the beginning, mathematicians need to study spaces by considering how other spaces cover(*) them. For example, if you have a line and a circle, you can wind up that line around the circle completely. In mathematical terms, you are given $\\mathbb R$ and $\\mathbb S = \\{z\\in\\mathbb C: |z|=1\\}$. You can wind it up with $x\\mapsto e^{ix}$. This is good because locally, i.e. in a small area $U$ around each point, this map looks like the projection function $\\mathbb Z \\times U \\xrightarrow{\\pi_2}U$.\nI'll steal a picture here:\n\nNote that if you restrict your attention on a small region around one of the black arrows (the space \"above\" a point in $\\mathbb S$), it just looks like $\\mathbb Z$ copies of the same thing overlaid on that region.\nNow, what's good about it? Generally, we want to study the geometry of the base space $\\mathbb S$ by \"projecting\" down from the total space $\\mathbb R$. So we need the covering to be relatively well-behaved. For instance, when we are studying paths, we may want to require the property that, no matter how I walk along the base space, you can follow along in the total space, while keeping yourself strictly above me. As an example (lots of examples!), if I start at $\\color{blue}1$, and you start at $\\color{red}{-1}$; when I go counterclockwise along $\\color{blue}{1 \\to i \\to -1 \\to 1}$, you must follow along. And you can: $\\color{red}{-1\\to-\\frac34\\to-\\frac12\\to0}$.\nActually, a notion that came earlier is fiber bundles, which puts a stricter requirement: If you look at a small enough region, then the whole thing just looks like $F \\times B \\to B$. This is slightly easier to work with, but it is too restrictive in our use cases.\nTo formalize the notion of fibration, we can use a continuous function $[0,1]\\to B$ to represent a path in $B$. Also, we have a point $*\\xrightarrow0 [0, 1]$. (Using an arrow from a singleton space to another space is a common category speak to \"pinpoint\" a point in space; Here we want to get the point $0$. Also, I use $*$ instead of the categorical $1$, because it clashes with the real number $1$ here.) We start with a path $q:[0, 1]\\to B$, and a \"covering\" $p:E\\to B$ (our picture above is a covering $\\mathbb R \\to \\mathbb S$). We also specify our starting point in $E$ with an arrow $*\\to E$. This starting point must lie above our starting point of the path $q$ in the base space. So it can be written in a commutative diagram:\n$$\n\\begin{matrix}\n*&\\to&E\\\\\n\\downarrow & & \\downarrow\\\\\n[0, 1]&\\color{blue}{\\xrightarrow q} & B\n\\end{matrix}\n$$\nBy saying that \"you can follow along the path $q$ in $E$\", we really mean that you can lift the path into $\\hat q : [0, 1]\\to E$, and the commutativity of the diagram ensures that this lifted path lies \"above\" the original one. So:\n$$\n\\begin{matrix}\n*&\\to&E\\\\\n\\downarrow &\\color{red}{\\nearrow} & \\downarrow\\\\\n[0, 1]& \\color{blue}{\\to}& B\n\\end{matrix}\n$$\nIn this case, we say that the arrow $p$ lifts against the arrow $*\\xrightarrow 0 [0,1]$. But of course, we may\nwish to use other arrows than $0$. So in general, if $p$ lifts against a certain set of arrows,  we call $p$ a fibration.\nOn the other hand, we can consider the dual notion. Since fibrations are really good coverings, by reversing the arrows, we get that cofibrations are really good inclusions.\nWe invert our diagram:\n$$\n\\begin{matrix}\nY&\\leftarrow&B\\\\\n\\uparrow &\\color{red}{\\swarrow} & \\uparrow\\\\\nX& \\color{blue}{\\leftarrow}& A\n\\end{matrix}\n$$\nHere $X \\to Y$ is some other given arrow which we want to \"colift\" against. This is called an extension property.\nThe archetypical example is when $X \\to Y$ is $S^{[0,1]} \\xrightarrow {p_0} S$, which maps each path in the space of paths in $S$ to its starting point. Suppose $A = *$, and $* \\to B$ is a point of $B$, then the commutative square gives\n$$\n\\begin{matrix}\nS&\\leftarrow&B\\\\\n\\uparrow &\\color{red}{\\swarrow} & \\uparrow\\\\\nS^{[0,1]}& \\color{blue}{\\leftarrow}& *\n\\end{matrix}\n$$\n\nA map $\\color{blue}{H : * \\to S^{[0,1]}}$, which is equivalent to a path $\\color{blue}{[0,1] \\to S}$.\nA map $B \\to S$, mapping $B$ inside $S$.\n\nThe extension property gives the diagonal arrow $\\color{red}{B \\to S^{[0,1]}} \\cong B \\times [0,1] \\to S$, which describes how $B$ \"moves\" continuously in $S$. $\\color{blue}H$ gives the trajectory of a single point of $B$. So the extension property roughly says that, whenever you have the starting position of $B$, and describe how a single point moves inside $S$, then you can always \"drag $B$ along that trajectory\". Another way to put it is you can always extend the orbit of the point to the orbit of the whole of $B$.\nOf course, extension of a point sounds trivial. But we can immediately create another example: If you replace $A = *$ with $A = 2$, the space with two points, then the extension property says that given the trajectory of two points, you can always extend the trajectory to the whole of $B$.\nThis is very useful in type theory. Suppose you have proved three equalities:\n$$\n\\begin{matrix}\na&&d\\\\\n\\|& & \\|\\\\\nb& =& c\n\\end{matrix}\n$$\nIn cubical type theory, this is represented as three paths, similar to $[0,1] \\to X$. To compose them, you invoke a version of the extension property: Let $B$ be the path $b=c$, and you have the trajectory of the two endpoints $b$ and $c$: they move to $a$ and $d$, respectively. Now \"dragging $B$ along\", you will end up with a path $a = d$. This is how path composition is proved (**).\n(*): Covering spaces is a related term, but I don't want to talk too much about that. I'm using this word to represent intuition.\n(**): In some flavors of CuTT, at least. There are type theories that try to achieve this with different methods.", "meta": {"post_id": 1073, "input_score": 17, "output_score": 9, "post_title": "What are \"fibration/cofibration\" in type theory and what are their intuitions?"}}
{"input": "I have often seen the claim that in a classical setting, well-foundedness of a relation > defined as the absence of an infinite descent x_0 > x_1 > x_2 > ... is equivalent to the constructive definition using an accessibility predicate as in Coq's Acc:\nInductive Acc (x: A) : Prop :=\n  Acc_intro : (forall y:A, R y x -> Acc y) -> Acc x.\n\nDefinition well_founded := forall a:A, Acc a.\n\nwhere R : A -> A -> Prop.\nIt seems it requires some version of choice but I don't know which is weakest one has to assume to construct such a proof.", "output": "This is not a Coq proof, but it's a proof sketch using the axiom of dependent choice and LEM to exhibit an equivalence. I believe this is the weakest choice principle you can get away with, but I cannot find a reference. It's too long for a comment unfortunately.\nEdit: it's no longer just a paper proof! @kyo dralliam has formalized the below argument in Coq and gone further to show that the choice additional axioms (dependent choice, LEM) are also necessary as well as sufficient.\n\nLet's define $\\mathsf{idc}(A,R,a)$ to be the proposition which states that there is an infinite descending chain in $(A,R)$ starting with $a$:\n$$\n\\mathsf{idc}(A,R,a) = \\exists f : \\mathbb{N} \\to A.\\ f(0) = a \\land (\\forall n.\\ f(n) \\mathrel{R} f(n + 1))\n$$\nWe will show that $\\neg \\mathsf{idc}(A,R,a) \\iff \\mathsf{Acc}(A,R,a)$. The $\\Leftarrow$ direction follows by induction on $\\mathsf{Acc}$ and requires no non-constructive principles. The inductive step can be summarized as \"if $\\mathsf{idc}(A,R,a)$ holds, there is some $b$ such that $a \\mathrel{R} b$ and $\\mathsf{idc}(A,R,b)$ holds.\"\nFor the other direction, let's use LEM and assume $\\neg \\mathsf{Acc}(A,R,a)$ to derive a contradiction. We will achieve this contradiction by constructing an infinite descending chain and this is where we will use the axiom of dependent choice.\nTo that end, consider the type $B(a) = \\{x : A \\mid a \\mathrel{R^*} x \\land \\neg \\mathsf{Acc}(A,R,x)\\}$ and write $S$ for the restriction of $R$ to this type. Let's prove that $S$ is entire, that is if $b : B(a)$ then there exists some $c$ such that $b \\mathrel{S} c$. I've deferred this lemma to the bottom of the answer, to not clutter up the proof. Since $(B(a),S)$ is entire and non-empty with $a : B$ by assumption, the axiom of dependent choice gives us an infinite descending chain within $B(a)$: $b_0 \\mathrel{S} b_1 \\mathrel{S} b_2 \\mathrel{S} b_3 \\dots$. By definition of $S$, this induces a chain $b_0 \\mathrel{R} b_1 \\mathrel{R} b_2 \\dots$. Moreover, by definition of $B(a)$ we can extend this chain so that it starts with $a$. We now derive our contradiction from $\\neg\\mathsf{idc}(A,R,a)$.\n\nLemma. $(B(a),S)$ is entire.\nProof. Assume we are given $b : B(a)$. By assumption, $\\neg \\mathsf{Acc}(A,R,b)$. Unfolding this, we see that it's equivalent to the following:\n$$\n\\neg (\\forall b. a \\mathrel{R} b \\to \\mathsf{Acc}(A,R,b))\n$$\nBy LEM again, we obtain $c : A$ such that $a \\mathrel{R} c$ and $\\neg \\mathsf{Acc}(A,R,c)$. Therefore, $c : B(a)$ and $b \\mathrel{S} c$ as required.", "meta": {"post_id": 1077, "input_score": 16, "output_score": 16, "post_title": "Well-foundedness: classical equivalence of no infinite descent and accessibility"}}
{"input": "I am new to the categorical semantics for dependent type theories, so it is surprising for me to see nLab introduces so many variants of categorical models, including comprehension categories, display map categories, category with attributes, category with families, natural models and contextual categories.\nI wonder what are the motivations for these different models? For example, IMHO, I think category with families is one of them that is quite close to the syntax and developed quite early, I am curious what are the disadvantages of CwF that lead to the development of other notions?", "output": "I would divide these models into three general groups.\n\nStructures that are more \"categorical\", arising naturally from categories \"in nature\" without the need for strictification theorems.  In particular, in these models substitution/pullback is only functorial up to isomorphism.  These include display map categories and comprehension categories.  The variations among this group arise in part from how much of the rest of the structure of type theory one wants to retain, e.g. whether a dependent type is treated as a separate notion, or is identified with its projection map, and whether that projection map is equipped with a structure or just a property.\n\nStructures that faithfully represent the entire syntax of type theory, with strict substitution, and including the fact that contexts are inductively generated as lists of types.  These are contextual categories, a.k.a. C-systems; there's really only one notion here, although it can be named and defined in different-looking ways.\n\nStructures that faithfully represent the syntax of type theory, with strict substitution, but with a separate class of things called \"contexts\" that aren't necessarily inductively generated as lists of types.  These are categories with families, categories with attributes, and natural models, which are also really just equivalent ways to define the same thing, so I would say there's really only one notion here too.\n\n\nArguably the difference between the last two points is just a difference in the type theory they correspond to: in case (3) the type theory has a judgment $\\Gamma \\,\\mathsf{ctx}$ with rules like\n$$\\frac{\\Gamma\\,\\mathsf{ctx} \\qquad \\Gamma \\vdash A\\,\\mathsf{type}}{\\Gamma,x:A \\,\\mathsf{ctx}}$$\nwhile in case (2) there is no such judgment, and an ordinary rule like\n$$\\frac{\\Gamma\\vdash a:A \\qquad \\Gamma \\vdash b:B}{\\Gamma \\vdash (a,b):A\\times B}$$\nis actually regarded as a schema of rules parametrized by an external natural number length of context:\n$$\\frac{\\cdot\\vdash a:A \\qquad \\cdot \\vdash b:B}{\\cdot \\vdash (a,b):A\\times B}\n\\qquad\n\\frac{x:X\\vdash a:A \\qquad x:X \\vdash b:B}{x:X \\vdash (a,b):A\\times B}\n\\qquad\n\\frac{x:X,y:Y\\vdash a:A \\qquad x:X,y:Y \\vdash b:B}{x:X,y:Y \\vdash (a,b):A\\times B}\n\\qquad\\cdots$$\nHowever, even when using type theories without a context-judgment, structures of type (3) occupy an intermediate stage between those of type (1) and type (2).  Namely, the way to make (1) into (2) is to first apply a strictification theorem to make it into (3), and then apply the \"contextual coreflection\" to make it into (2).  In particular, any type theory without a context judgment can be interpreted in a (3) structure via this contextual coreflection.", "meta": {"post_id": 1086, "input_score": 18, "output_score": 17, "post_title": "What are the motivations for different variants of categorical models of dependent type?"}}
{"input": "What is bidirectional type checking and why would I want to implement it?\nIt feels like the name is a bit of a misnomer and the syntactic separation kind of resembles stuff like ANF, call by push value or CPS?", "output": "Bidirectional type checking is a popular technique for implementing type checking/inference algorithms. AFAIK it originated from the paper \"local type inference\".\nThe idea of bidirectional type checking follows from the following observation: there are two ways to typecheck a program, type checking, where type information is provided externally, and the program checks if some term matches the type, and type inference, where type information is missing, and the algorithm must deduce it from only the term itself.\nObviously, inference is much harder than checking. In fact, in many complex type systems only the latter is decidable. However, requiring type annotation everywhere is very awkward. And here comes bidirectional type checking: its ultimate goal is to propagate type information/annotation through the syntactic structure of terms.\nThere are two modes in bidirectional type checking, synthesis and checking. synthesis corresponds to type inference, and no external information is available. So an annotation is usually needed here. On the other hand, checking mode, as its name indicates, receives a type from the outside world and performs type checking, so usually no type annotation is needed.\nAt nodes like function application f(t), where the subterms' types are related, bidirectional type checking can propogate type information from one to the other, reducing the number of annotations needed. For example, if f synthesis to a type A -> B, then t can be checked with A.\nYou mentioned CPS/ANF/CBPV. IMO they are not related to local type inference. Local type inference shares with these transformations/NFs the same nature that it reveal certain ordering on terms. However, CPS/ANF reveal evaluation order, while what bidirectional type inference reveals is how type information flows, which is something different.", "meta": {"post_id": 1090, "input_score": 18, "output_score": 23, "post_title": "What is bidirectional type checking?"}}
{"input": "Agda has a reflection mechanism (not equality reflection or reflexivity, but something related to metaprogramming based on goals and contexts to generate terms) and people have developed some libraries that simulate Coq tactics, for example acata.\nHowever, some people still don't consider Agda to have tactics (for example in Kevin Buzzard's \"the future of mathematics\" talk, the Q&A section).\nOn the other hand, Coq and Lean are considered to \"have tactics\", but what I see is just another syntax for writing reflection code like in Agda. I think it's because I don't understand tactics in these languages good enough. So, what particular feature do they support so that they can be considered to \"have tactics\"? I have a clear understanding of what reflection is (like in Agda -- the compile time reflection with access to the type-checking monad and bidirectional type checking functions), but I am unsure what do people mean by \"tactics\".", "output": "Consider a reasonable type theory $T$ with decidable checking. Think of it as the core type theory implemented in the kernel of a proof assistant, i.e., with fully elaborated and annotated judgements that nobody wants to write with bare hands.\nA central task of formalization is inhabitation of a type: given a derivable judgement $\\Gamma \\vdash A \\; \\mathsf{type}$ (the goal), the user provides an expression $e$ (the solution), and the kernel checks whether $\\Gamma \\vdash e : A$ is derivable. How can we do this in a practical way?\nIt is not practical for the user to write down a fully annotated term $e$. Instead, they write down an expression $c$ in a surface language $V$ (the vernacular) which is elaborated or evaluated by the proof assistant to give a fully annotated expression $e$. In order to account for the possibility of a user error, the proof assistant can be seen as a map\n$$\\textstyle\nP : \\prod_{\\Gamma, A} \\, (\\Gamma \\vdash A \\; \\mathsf{type}) \\to V \\to \\{\\mathsf{error}\\} + \\sum_{e} \\, (\\Gamma \\vdash e : A).$$\nWe read the above as: given a derivable type $A$ in context $\\Gamma$ (the goal) and an expression $c$ of type $V$ (the suggested solution), the proof assistant either recognizes that $c$ evaluates to an expression $e$ such that $\\Gamma \\vdash e : A$, or it reports an error.\nActually, apart from errors, there could also be other computational effects, such as non-termination, state, user interaction, etc. We thus generalize the above to a suitable computational monad $M$:\n$$\\textstyle\nP : \\prod_{\\Gamma, A} \\, (\\Gamma \\vdash A \\; \\mathsf{type}) \\to V \\to M(\\sum_{e} \\, (\\Gamma \\vdash e : A)).$$\nWith the general setup explained, we can address the question. What kind of proof assistant one gets depends on the design of the vernacular $V$ and the computational monad $M$. Some possibilities are:\n\nAgda: to give the user the impression that they are working with $T$ directly, we design the vernacular $V$ to look like an abridged version of $T$ and call the expressions of $V$ (proof) terms. The computational monad $M$ supports interactive construction of terms through \u201dholes\u201c that represent subgoals. (Agda also support meta-programming through which the user can implement proof search and other techniques that people call \u201ctactics\u201d.)\n\nCoq and Lean: make the vernacular $V$ look like a command-based programming language. The commands are called tactics. The monad $M$ incorporates the current goal as state that is manipulated by the user through tactics. The command-based language allows general recursion and construction of non-terminating programs.\n\nIsabelle/HOL: make the vernacular $V$ look like a meta-level programming language. There is a built-in abstract data type of judgements, which is controlled by a trusted kernel. The user writes programs that evaluate to judgements. There is a library of useful functions for constructing judgements, called tactics. The user is free to implement their own functions, as well as Tetris.\n\n\nReflection is the ability for the vernacular $V$ to access the abstract syntax trees of computed judgements. Again, there is more than one way of doing this:\n\nAgda reflects judgements into a datatype of abstract syntax trees (this is also known as reification)\nCoq has matching on goals and hypotheses\nLean metaprogramming also allows one to decompose judgements\nIsabelle/HOL has reflection too, though the abstract data type of judgements. In addition, there is also a mechanism for matching on goals and hypotheses similar to the one for Coq.\n\n(If someone can provide better descriptions and links to reflection for Lean and Isabelle, that would be grand.)\nReflection is not to be confused with Boolean reflection, which is a proof technique for (ab)using the equality checker as a decision procedure.  Please ask a separate question if you'd like to know more.", "meta": {"post_id": 1103, "input_score": 18, "output_score": 18, "post_title": "What's the difference between reflection and tactics?"}}
{"input": "Is there a library (for any proof assistant) which provides formalized definitions of unsolved problems? To clarify, I mean some collection that correctly defines unsolved problems in the language of a proof assistant, without attempting to provide any solutions.\nBy unsolved problems, I mean one which haven't been solved by any human mathematician (like the Riemann hypothesis) rather than just ones which have been proven but not formalized in a proof assistant.", "output": "The closest to what you are asking for might be the Formal abstracts project whose aim is to formalize the statements (but no proofs) of results from papers.\nFormalizing statements without proofs is not as trivial as one might think at first. Some challenges are:\n\nThe statement of the proof might require formalization of a complicated constructions. For instance, consider a theorem about cohomology \u2013 presumably we need to define cohomology before we can state it.\n\nWithout proofs and without any applications of the theorem there is no sanity check as to whether the statement is formalized correctly. There is real danger of mistakes going undetected.\n\nNot only should one formalize the statements itself, but also keep track of provenance, i.e., information of where in the non-formalized world the theorem originated.\n\n\nSimilar challenges will present themselves with a library of open problems.", "meta": {"post_id": 1125, "input_score": 16, "output_score": 10, "post_title": "Formalizations of unsolved problems"}}
{"input": "I'm reading Girard's Proofs and Types, and in section 4.4 he writes:\n\nLemma: t is strongly normalisable iff there is a number \u03bd(t) which bounds the\nlength of every normalisation sequence beginning with t.\n\nHe later uses this lemma in his proof that the STLC is strongly normalizing in section 6.2.3:\n\n(CR 3) Let t be neutral and suppose all the t' one step from t are\nreducible. Let u\nbe a reducible term of type U; we want to show that t u is reducible. By\ninduction hypothesis (CR 1) for U, we know that u is strongly normalisable;\nso we can reason by induction on \u03bd(u).\n\nThe lemma uses a version of Konig's Lemma which IIUC is not provable constructively.  Is there a constructive proof of strong normalization for the simply typed lambda calculus?  Specifically something I can (try to) translate into Agda?", "output": "As Dan Doel says in the comments, the reason K\u0151nig's lemma is used is that Girard's definition of strong normalization is constructively too weak. It's defined as the non-existence of infinite reduction sequences.\nWe can use the following definition instead: a term is strongly normalizing if every one-step reduct of it is strongly normalizing (inductively). In Agda:\ndata SN {\u0393 A} (t : Tm \u0393 A) : Set where\n  sn : (\u2200 {t'} \u2192 t ~> t' \u2192 SN t') \u2192 SN t\n\nNote that if a term is normal then it has no one-step reduct so SN t holds vacuously.\nThis is also called accessibility of t with respect to ~>. I believe it originates from Aczel's An Introduction to Inductive Definitions. This is a common SN definition in formalizations. The oldest formalized example that I know is in Altenkirch's thesis.\nThe advantage of SN t is that we can do induction on it. Operationally, we can make a \"recursive call\" on any one-step reduct of a strongly normalizing term. You could also look at my Agda formalization of Girard's Chapter 6, which uses this SN.", "meta": {"post_id": 1135, "input_score": 10, "output_score": 17, "post_title": "Constructive proof of strong normalization for simply typed lambda calculus"}}
{"input": "Andrea Vezzosi said Cubical Agda made cubical subtyping coercion-based (inS and outS) because it's a bad idea to introduce more subtyping relations which complicates the system. Also Agda did not have cumulative universes (it is added now).\nSo does Agda support any subtyping relations? Without using rewriting and cumulativity options of course.", "output": "The current subtyping rules that Agda uses are for sized types (when --sized-types is enabled) and cumulativity (when --cumulativity is enabled). Currently neither of these options are allowed under the --safe flag, so with --safe there is no subtyping in Agda.\nThere used to be subtyping rules for irrelevant and erased function types as well, but these were removed recently (see This successful Pull Request which was merged into Master on GitHub) because they were causing performance problems and anyway only save the user a simple eta-expansion.", "meta": {"post_id": 1148, "input_score": 5, "output_score": 10, "post_title": "What subtyping rules does Agda support?"}}
{"input": "Ulf Norell wrote this in his PhD thesis (figure 1.6):\n\nThis contradicts my stereotype on pi & sigma types, where pi parameter should be contravariant and sigma parameter is covariant. Why is Agda designed this way? Is it actually implemented this way?\nP.S. I have two guesses why pi has invariant parameter:\n\nThat contravariant parameter makes the type theory complex, because you keep track of the actual type and the type in the telescope -- two types -- of a binding. Without contravariant pi parameter we can think of them as the same thing.\nAccording to Conor McBride, it's Zhaohui Luo's Extended CoC that made this design choice for a simpler set-theoretical semantics (that subtyping is modelled by subsetting).\n\nWhich of these is true? Or are they both unrelated to the Agda design choice?", "output": "There are subtleties here, when type annotations are present, depending in quite a brittle way on where they must be placed. (I'm half-remembering conversations about this with Zhaohui Luo.) Suppose we demand (as many do) type annotations on lambda, with a typing rule like this\nS : Type_i     x : S |- t : T\n-----------------------------\n\\ x : S -> t  :  (x : S) -> T\n\nIn particular, we may readily have\n\\ X : Type_1 -> X : (X : Type_1) -> Type_1\n\nNow, if cumulativity were contravariant in the input type, we should allow\n(X : Type_1) -> Type_1  <=  (X : Type_0) -> Type_1\n\nand, indeed, one might imagine that the identity function for Type_1 should work perfectly well for those of its inhabitants which happen also to live in Type_0. But there's a snag, if you want cumulativity to be, in a suitable sense, reducible to universe inclusion. The typing rule for lambda will only ever give\n\\ X : Type_1 -> X\n\nfunction types with the domain Type_1, because that is what the domain type annotation says, and we're stuck with it. There is a function that does thejob we want, but it's\n\\ X : Type_0 -> X\n\nand that ain't the function we first thought of.\nNow, if you're me, you arrange never to synthesize the types of lambda abstractions, only to check them, removing the need for the type annotation. Then, both Type_0 -> Type_1 and Type_1 -> Type_1 will accept \\ X -> X. In the bidirectional setting, the \"change of direction\" rule says that to check that T accepts a term whose synthesized type is S, then an S thing should be able to do all a T thing's jobs. That clear directedness yields great temptation to relax from equality to a subsumptive notion of subtyping, and in that setting contravariance in function domains is not a problem.", "meta": {"post_id": 1152, "input_score": 9, "output_score": 8, "post_title": "What's the benefit of having pi and sigma types with an invariant parameter?"}}
{"input": "Fifty years ago, few would have imagined that the process of verifying the correctness of a known proof of a mathematical theorem might be so costly that the mathematical community would hesitate to do it more than once. Of course, nowadays, there is a growing list of theorems whose proofs require an enormous amount of computation.  In many cases, these theorems amount to calculating the first few values of an infinite sequence of values, and maybe not many people would care to have those computations formally verified.  In other cases, however, there is an attractive theorem statement that many might want to see a formal proof of. The Kepler conjecture is an obvious example; perhaps less well known to the proof-assistant community is the ternary Goldbach problem, solved by Harald Helfgott, who reduced the problem to a feasible finite computation.  A complete proof therefore requires doing this computation; to give some idea of how big this computation is, let me quote a paragraph from Helfgott's preprint:\n\nIn December 2013, I reduced $C$ to $10^{27}$. The verification of the ternary Goldbach conjecture up to $n \\le 10^{27}$ can be done on a home computer over a weekend,\nas of the time of writing (2014). It must be said that this uses the verification of the\nbinary Goldbach conjecture for $n \\le 4 \\cdot 10^{18}$ [OeSHP14], which itself required computational resources far outside the home-computing range. Checking the conjecture\nup to $n \\le 10^{27}$ was not even the main computational task that needed to be accomplished to establish the Main Theorem\u2014that task was the finite verification of zeros of $L$-functions in [Plab], a general-purpose computation that should be useful elsewhere.\n\nProofs like these seem to pose a significant knowledge management problem if we envisage a future in which the majority of interesting mathematical results are archived in some machine-checkable library. It seems that if we want to check the proof of a new theorem that relies on an existing theorem whose proof is extremely expensive, then we are faced with the choice of either redoing the expensive computation, or taking it on faith that someone else checked it and that we haven't made a bookkeeping error somewhere along the way.  If there are only a few expensive theorems then maybe there isn't a problem, but if expensive theorems proliferate, and we want to rely on our records of whether they were checked or not, then it seems that the chances of mistakenly thinking that something was proved (when it really wasn't) start to become non-negligible.  This is especially true if we have software \"upgrades\" every couple of years that could introduce bugs, or at least confusion over the exact meanings of various terms and theorems.\nWith the above remarks as background, I have a twofold question.\n\nDo existing systems have a plan for managing this kind of issue? I see that there is another question about recompilation avoidance that seems related, but doesn't seem to be quite the same.\n\nTypically, how much computational overhead is there when we take a conventional computation and redo it in a proof assistant?  (Here, I'm not talking about the human effort in doing the re-programming, although I recognize that that is also a major issue.) The answer is probably, \"It depends,\" but I'm wondering if there is any rule of thumb. In this regard, I found the Ph.D. thesis of Muhammad Taimoor Khan, Formal Specification and Verification of Computer Algebra Software, to be very interesting. For certain types of computation in Maple, one can apparently answer this question fairly easily by running Khan's software.  Are there other packages available that allow someone with conventional programming experience but little experience with proof assistants to write \"normal-looking code\" (or something close to it) that carries out a formally verified long computation, and thereby time how much longer it takes than conventional code does?", "output": "Not as general as Andrej's answer but more a data-point for your interest. We've been looking at how to construct formal proofs about neural network-enhanced systems in proof assistants. This runs into exactly the problem you describe: the act of verifying the network can be very expensive (hours/days) which disrupts the interactive component of the proof assistant.\nAs described in Figure 1 in our recent paper, we've taken the external book-keeping approach, i.e. create a cache which stores the result of the verification of the network. The proof assistant then checks the cache instead of reverifying the network each time. The cache maintains consistency by also storing the location and the hash of the network, which can be used to detect when the proof has become invalid (for example due to further training of the network).", "meta": {"post_id": 1161, "input_score": 31, "output_score": 8, "post_title": "What work has been done on computationally intensive proofs?"}}
{"input": "I wrote some code that used a [inhabited \u03b9] assumption, and then used arbitrary \u03b9 in the proof. The mathlib linter then complained\nThe following `inhabited` instances should be `nonempty`. argument 2: [_inst_3 : inhabited \u03b9]\n\nI can change the types as suggested, but what do I do with the arbitrary \u03b9 expression in my proofs?", "output": "What you're seeing here is the behavior of the inhabited_nonempty linter:\nimport tactic\n\nlemma foo {\u03b1} [inhabited \u03b1] : \u2203 x : \u03b1, x = x :=\nbegin\n  exact \u27e8default, rfl\u27e9\nend\n\n#lint only inhabited_nonempty\n/- Checking 1 declarations (plus 1 automatically generated ones) in the current file with 1 linters -/\n\n/- The `inhabited_nonempty` linter reports: -/\n/- USES OF `inhabited` SHOULD BE REPLACED WITH `nonempty`. -/\n\nYou can use the inhabit tactic to solve this problem\nimport tactic\n\nlemma foo {\u03b1} [nonempty \u03b1] : \u2203 x : \u03b1, x = x :=\nbegin\n  inhabit \u03b1,\n  exact \u27e8default, rfl\u27e9\nend\n\n#lint only inhabited_nonempty  -- All linting checks passed!\n\nNote this has the advantage over classical.arbitrary of not introducing the axiom of choice:\n#print axioms foo  -- no axioms\n\nwhile being less cumbersome than the nonempty.elim \u2039nonempty \u03b1\u203a $ \u03bb a, suggested in the other answer.", "meta": {"post_id": 1168, "input_score": 3, "output_score": 10, "post_title": "Like arbitrary, but for nonempty instead of inhabited"}}
{"input": "This document showed that Lean's impredicative universe of strict propositions breaks normalization (of proofs) in a way that canonicity and logical consistency are unaffected, because the counterexample lives in Prop. I wonder how much of trouble it will be to have this counterexample, like both syntactically (in terms of Lean type checking) and semantically (in terms of Lean metatheory)?\nTo me, it seems that you simply need to prevent reduction for terms in Prop and assume them to be equal. This will at least recover the termination of type checking (well, at least refute this particular counterexample of termination of type checking). This can solve the obvious trouble pointed out by the paper.\nPierre-Marie P\u00e9drot said that:\n\nImpredicative SProp breaks SN only when you can eliminate the SProp equality into a non-SProp sort. Otherwise, it's fine.\n\nI'm also unsure about that. I think it is established that equalities in SProp cannot be eliminated into non-SProp (like boolean in SProp has true = false, but if we can eliminate this into non-SProp booleans, we get logical inconsistency), so I assumed this is disallowed in Lean, but still the paper I linked in the beginning claimed that normalization is broken by their counterexample.", "output": "To address a most important point, as suggested by Andrej Bauer, Lean's intended model is one where types are sets.  Mario Carneiro showed in his master's thesis that Lean has such a set-theoretic model (specifically using ZFC plus infinitely many large cardinals).  So that means Lean is consistent (assuming no bugs in its implementation), and a theorem in Lean means what a classical mathematician would think it means (once they interpret types as sets and understand universes).\nThe failure of normalization however means that one can't give a more computational model of Lean, which isn't a large deal since Lean is mostly used as a classical mathematics proof assistant.  Also, it should be pointed out that Lean's reduction (in Lean 3 at least) is painfully slow anyway.  It can barely add numbers with #reduce, so non-termination and slow termination are basically the same in the eyes of the user.  This is discussed in this Zulip thread.\nA related issue which you also address, comes down to elimination outside of Lean's Prop.  Unlike your expectation, Lean does have a limited form of large elimination called (syntactic) subsingleton elimination.\nAs a quick aside, my understanding is that there are two interpretations of definitional equality in Lean.  One is what is actually implemented in Lean.  Mario's thesis has an approximation of that which is decidable, but it breaks transitivity.  Broken defeq transitivity implies broken subject reduction.  The other definitional equality which appears in Mario's thesis is a more ideal one.  It satisfies transitivity and subject reduction, but it is undecidable.\nThe reason for this failure of subject reduction in Lean's implemented type theory, to my understanding, is that Lean has proof irrelevance (similar to Coq's SProp) and a syntactic form of subsingleton elimination as described in this document.  In particular, the latter lets you eliminate \"syntactic subsingletons\" such as true, false, and, =, and acc (which is related to well-founded) to types in Type.  These are called \"syntactic subsingletons\" since they have at most one type constructor.  As Pierre-Marie P\u00e9drot pointed out, this leads to the issues with definitional equality that I mention above.  On Zulip, Mario gave a MWE of how to use proof irrelevance of Prop and the syntactic subsingleton elimination of acc to explicitly break transitivity of defeq, break subject reduction, and fool Lean tactics into producing false terms (but which the kernel still correctly rejects).\nThese issues are relatively small for Lean users.  Again, they don't break consistency or change the set-theoretic model, just make some things more annoying for users.  Lean users rarely notice these issues are there (and likely couldn't point to them at all), but they do occasionally come up as discussed here.  Mario, however likes to point out that it is possible to run into the same issues even in a system without explicitly broken definitional equality.  In particular, it is easy to construct examples where a = b and b = c are fast to compute definitionally, but a definitional proof of a = c is quite slow and will time out.  While not technically a failure of definitional equality, it is the same sort of issue in practice.  One must resort to an alternate proof of a = c in some cases.\n(Also, it should be pointed out that even Coq has some issues with subject reduction.  I think Mario would say the Coq issues are similar in scope to those Lean in that they aren't a large issue, but should be watched out for.  But I think Coq users would counter that they have a plan to fix SR in Coq whereas it is an accepted trait of Lean.  I also have the sense that the SR issues in Coq are more compartmentalized to certain Coq features.)\nNote: I got a lot of my information from this thread on the Lean Zulip.\nThe big question of course now is why does Lean have proof irrelevance and syntactic subsingleton elimination if they knowingly lead to these (small) issues?  And what big benefits does proof irrelevance and syntactic subsingleton elimination provide to Lean users?  I'm trying to figure that out, and then I'll update this answer when I do.", "meta": {"post_id": 1183, "input_score": 16, "output_score": 16, "post_title": "How much of trouble is Lean's failure of normalization, given that logical consistency is not obviously broken?"}}
{"input": "I read this CSTheory SE post, which suggests that it is often not clear what variant of MLTT or CIC is being referred to. But I would like to know the proof-theoretic strengths of the various underlying foundational systems for more 'constructive' proof assistants such as Agda, Coq, Lean. For example, this paper gives the proof-theoretic strength of MLTT[1984]. But Coq is not based on MLTT[1984], but on some (evolving?) variant of CIC. This introduction seems to mix both type theory and Coq, and omits details \"for the sake of simplicity\", so I don't really know what is the whole system...\nCan anyone present a precise table comparing the proof-theoretic strengths of the most common systems underlying Agda, Coq, Lean? In particular, I am interested in what happens when features that might be considered impredicative are included or excluded. So far I have seen two, one being impredicative types (e.g. impredicative Prop) and the other being impredicative inductive/recursive constructions. I am also interested in what happens when LEM is included or excluded, because it seems to me that we are unable to provide non-circular (ontological) justification for having both impredicative Prop and LEM. (Related.)\nFor instance, what is the strength of MLTT[1984]+LEM? Does it exceed that of ZBQC \u2245 MOST? What about CIC and CIC+LEM?", "output": "Here are a few comparisons of dependent type theories with fragments of Peano arithmetic and set theories that I know of:\nPredicative systems (the MLTT family)\nPredicative systems are comparable with small fragments of second-order arithmetic. Their strength is measured by which inductive definitions are allowed:\n\nWhen there is only a type of natural numbers, we get the strength of ATR\u2080, in the sense that both theories prove the same \u03a0\u2070\u2082 statements about arithmetic (proved in the paper you mention). ATR\u2080 is a small fragment of PA2 which plays an important role in reverse mathematics.\nAdding W-types makes the system stronger than \u0394\u00b9\u2082-CA but weaker than \u03a0\u00b9\u2082-CA, which are slightly larger fragments of PA2.\nInduction-recursion is still a bit stronger, but is expected to be bounded above by KPM+ (see here) which is still weaker than \u03a0\u00b9\u2082-CA, I believe. This is more or less the theory of Agda.\n\nThe order can be refined by taking into account the number of predicative universes : MLTT\u2081 < MLTT\u2082 < ... < MLTT\u03c9 < MLTT\u2081 + W < ... MLTT\u03c9 + W < MLTT\u2081 + IR < ...\nA detailed table that also includes ordinal notations and references can be found on the nLab.\nExtensionality axioms such as function extensionality, proposition extensionality, UIP, univalence, equality reflection... do not add any logical strength to the system. The full law of excluded middle, however, is another story: now any type is either inhabited or not, which basically makes Bool an impredicative universe of propositions \u2014 and impredicative systems are much stronger. In fact, MLTT + LEM has the same strength as CIC + LEM.\nImpredicative systems (System F, CC, CIC)\nThe introduction of an impredicative universe adds a tremendous amount of logical strength to a system. We jump from small fragments of second-order arithmetic to systems which are at least as strong as the full PA2.\n\nSystem F has the strength of second-order arithmetic, in the sense that an integer function is provably total in PA2 if and only if it can be defined in system F (standard reference).\nCC, the calculus of constructions (which has an impredicative universe Prop which sits inside the predicative universe Type) has the strength of higher-order arithmetic.\nCC\u03c9, which adds a hierarchy of predicative universes, is stronger than Zermelo set theory (proved here), but weaker than ZF set theory (as witnessed by the normalization proof).\n\nNote that the strength comparison theorems are about definable functions, not provable arithmetic statements as before. Indeed, all of these systems have no inductive types at all, so even the integers need to be encoded with impredicativity. And while you can define very complex functions on these impredicative integers, predicates cannot tell them apart: you can't even prove 0\u22601. If we want to do such things, we need to add inductive types with large elimination, which results in CIC, the calculus of inductive constructions.\n\nIf we restrict large elimination in the impredicative universe to sub-singleton inductive types, then we can lift functions that are defined on impredicative integers to predicative, inductive integers while remaining compatible with classical set-theoretic semantics. This is more or less the theory of Coq.\nIf we allow large elimination for all inductive types in the impredicative universe, then the system becomes incompatible with classical logic.\nFinally, if you add LEM and a variant of the axiom of choice to CIC, then it becomes comparable with ZFC set theory (proved here)", "meta": {"post_id": 1201, "input_score": 21, "output_score": 26, "post_title": "Proof-theoretic comparison table?"}}
{"input": "Off and on I have heard of the jargon \"commuting conversion\" but I don't really know what it means.\nI've heard commuting conversions are problematic but I don't know why.", "output": "Syntactically, commuting conversions are part of the $\\eta$-rules for left-invertible types -- i.e., types which have pattern-matching eliminators. So if $e : A + B$, the $\\eta$-equation for it looks like:\n$$\n\\newcommand{\\c}[1]{\\mathsf{#1}}\nC[e] \\equiv \\c{case}(e, \\c{inl}\\,x \\to C[ \\c{inl}\\,x], \\c{inr}\\,y \\to C[ \\c{inr}\\,y]) \n$$\nThis rule says that if you have a term with an $e : A+B$ occuring in it somewhere (this is the $C[e]$), then it is the same as matching on $e$ and running $C[\\c{inl}\\, x]$ in the left branch and $C[\\c{inr}\\, y]$ in the right branch.\nThe algorithmic difficulty this poses is that this rules is extremely nondeterministic and non-syntax-directed -- if you have a term $t$, you can break it into a context $C$ and expression $e$ such that $C[e] = t$ in potentially many different ways. Moreover, since the context $C$ is duplicated in each branch of the case statement, the context in each can be independently transformed some more into terms which look radically different from one another.\n\nSemantically, commuting conversions arise from the fact that the sum type can be viewed as a coproduct. Since coproducts are a kind of colimit, they satisfy both an existence property, and a uniqueness property.\nThe existence property of coproducts says that for each $X$ and $Y$, we have a type $X + Y$ and injection maps $\\c{inl} : X \\to X + Y$ and $\\c{inr} : Y \\to X + Y$ such that for each $f : X \\to Z$ and $g : Y \\to Z$, we have a map $[f, g] : X + Y \\to Z$ such that $[f,g] \\circ \\c{inl} = f$ and  $[f,g] \\circ \\c{inr} = g$.\nYou can see that these two equations correspond to the $\\beta$-rules for sums:\n$$\n\\c{case}(\\c{inl}\\,z, \\c{inl}\\,x \\to e_1, \\c{inr}\\,y \\to e_2) \\equiv [z/x]e_1\n$$\n$$\n\\c{case}(\\c{inr}\\,z, \\c{inl}\\,x \\to e_1, \\c{inr}\\,y \\to e_2) \\equiv [z/y]e_2\n$$\nThe uniqueness property tells us this map is unique. In particular, uniqueness means that if you have a map $h : X+Y \\to Z$, then $h \\equiv [h \\circ \\c{inl}, h \\circ \\c{inr}]$. (This is easy to see if you draw the coproduct diagram for this.)\nThis is exactly the $\\eta$-law, once you remember that composition of morphisms and substitution of terms are the same thing:\n$$\n\\newcommand{\\c}[1]{\\mathsf{#1}}\nC[e] \\equiv \\c{case}(e, \\c{inl}\\,x \\to C[ \\c{inl}\\,x], \\c{inr}\\,y \\to C[ \\c{inr}\\,y]) \n$$\n\nColimits without the uniqueness property are called \"weak colimits\", and come up fairly often in category theory. But it is still weird that $\\eta$ for projective tuples and lambdas is easy, but that it's so hard for sum types.", "meta": {"post_id": 1202, "input_score": 13, "output_score": 11, "post_title": "What is a commuting conversion and why are they problematic?"}}
{"input": "It is possible (with some cleverness) to prove false from type-in-type.  For instance, Girard proved false in Martin Lof's original system (as described in the introduction of An Intuitionistic Theory of Types).  This answer has a good account of the formal details.\nMost systems allow type-in-type, through a configured opt-in and sometimes even by default (Pebble, Cayenne, some Trellys languages).\nHas anyone ever \"proven\" an incorrect theorem with type-in-type, without intending to show that the system is unsound?", "output": "I did this myself!\n(At least, if you interpret \"incorrect\" as \"probably not true\" rather than \"demonstrably false\".)\nIn the early days of homotopy type theory, we were starting to use multiple universes in ways that, I believe, had rarely been done before in type theory.  Since Coq didn't support universe polymorphism yet, the only way to formalize some results was using type-in-type.  In particular, I used it when formalizing some of the initial work on modalities that eventually became the RSS paper.\nOne of the results I proved, at that time, was that an (idempotent monadic) modality is left exact if and only if the universe of modal types is modal.  This was a really striking application of, and justification for, the notion of left exactness.  In particular, it implies that the modal types for a left exact modality form a model of type theory on their own, which is useful when constructing higher-topos models of type theory since any topos is a left exact localization of a presheaf topos.\nUnfortunately, when we later went back to write up and formalize these results correctly using universe polymorphism, I realized that the proof was invalid.  When universe levels are tracked, what I had proven was that the type of modal types in one universe $U$ was null with respect to all $U$-small modally-connected types; but since $U$ is not $U$-small, that doesn't imply $U$ is modal \u2014 unless we assume the modality is generated by a $U$-small family, i.e. that it is $U$-accessible.  (In this form, the result is Theorem 3.11 in RSS.)\nI don't have a counterexample to the statement in the non-accessible case, but I doubt it is true.  In fact, for a non-accessible modality there isn't necessarily a canonical way to extend a modality on one universe to a modality on larger universes, so if we have a lex modality on $U$-small types, it's not even obvious what it means to say that $U$ is modal.\nEver since this experience, I've been unconvinced by any argument along the lines of \"type-in-type is okay as long as you don't do anything stupid\".  I'm willing to grant that what I did was stupid; the point is that it's possible to do stupid things by accident.", "meta": {"post_id": 1219, "input_score": 31, "output_score": 43, "post_title": "Has anyone ever accidentally \"proven\" a false theorem with type-in-type?"}}
{"input": "Is impredicativity useful for program extraction in Coq? For example is there some kind of realizability argument that depends on impredicativity?\nOf course it doesn't seem to be necessary for program extraction, as Agda manages to extract programs to Haskell, but I suspect there might be some kind of theoretical tradeoff.", "output": "I have occasionally thought about this question. My inconclusive conclusion is that impredicativity hinders program extraction. Let me try to give an argument in the context of realizability. I am going to crank up the technical level of the discussion, because the concepts involved are, well, a bit technical (but they also embody some pretty concrete intuitions).\nAssume we're working in a realizability topos over a pca $\\mathbb{A}$. Now, that's a pretty big and complicated category, which among other things contains $\\mathsf{Set}$ as a full subcategory: given any set $X$, we get an object $\\nabla X = (X, {\\Vdash_{\\nabla X}})$ in the topos whose realizability relation is trivial: $r \\Vdash_{\\nabla X} x$ holds for all $r \\in \\mathbb{A}$ and all $x \\in X$. From a computational point of view the realizers of $\\nabla X$ are maximally uninformative because every realizer $r$ realizes every element of $X$.\nSo if the realizability topos can contain such computationally trivial objects, perhaps we should try to define precisely what it means for an object to be computationally trivial in general.\nHere is a bad attempt: say that an object $S$ of the topos is computationally trivial if every morphism $S \\to \\mathsf{Bool}$ is constant, where $\\mathsf{Bool} = 1 + 1$ is the object of Booleans. The idea is right, namely $S$ should be considered computationally trivial if we can't extract any interesting bits of information from it, but the execution is wrong, because sometimes we can extract less than one bit. (For instance, we can't decide whether any given machine halts, but we can semi-decide that it halts.)\nGiven any disjoint non-empty subsets $T_0, T_1 \\subseteq \\mathbb{A}$ define the test object $[T_0, T_1]$ to be the set $\\{0,1\\}$ with the realizability relation\n$$r \\Vdash_{[T_0, T_1]} b \\iff (r \\in T_1 \\land b = 1) \\lor (r \\in T_0 \\land b = 0).\n$$\nIn words, $1$ is realized by the elements of $T_1$ and $0$ by the elements of $T_0$. Examples:\n\nDecidable test: $T_0 = \\{\\mathtt{false}\\}$ and $T_1 = \\{\\mathtt{true}\\}$ yields the booleans,\nSemi-decidable test: $T_0 = \\{\\ulcorner M \\urcorner \\mid \\text{machine $M$ does not halt on input $\\ulcorner M \\urcorner$}\\}$ and $T_1 = \\{\\ulcorner M \\urcorner \\mid \\text{machine $M$ halts on input $\\ulcorner M \\urcorner$}\\}$.\nWe can make much weaker notions of test than decidability and semi-decidability. Write $\\mathsf{PA} \\models \\phi$ if the sentence $\\phi$ in the language of Peano arithmetic is true, and define $$T_0 = \\{ \\ulcorner \\phi \\urcorner \\mid \\mathsf{PA} \\models \\phi\\}$$ and $$T_1 = \\{ \\ulcorner \\phi \\urcorner \\mid \\mathsf{PA} \\models \\lnot\\phi \\}.$$ This sort of test is very weak. It is not about deciding arithmetical sentences, but rather expresing conditions with arithmetical formulas. Indeed, suppose $r \\Vdash_{[T_0, T_1]} b$. Then $r$ encodes some arithmetical sentence $\\phi$ such that $b = 1 \\Leftrightarrow \\mathsf{PA} \\models \\phi$ \u2013 so telling which bit $r$ encodes is as hard as computing arithmetical truth.\n\nA morphism $f : S \\to [T_0, T_1]$ can be thought of as a two-valued test.\n\nDefinition: An object $S$ of the realizability topos is computationally trivial if for all test objects $[T_0, T_1]$ all morphisms $S \\to [T_0, T_1]$ are constant.\n\nWhat the definition says is that one cannot extract any non-trivial information from $S$, for any notion of two-valued test.\n\nProposition: For any set $X$, the object $\\nabla X$ is computationally trivial.\n\nHere is the punchline: impredicativity is intimately tied to completeness, bu completeness prevents extraction of any useful information.\n\nTheorem: A complete lattice is computationally trivial.\n\nProof. We already know that $\\nabla \\{0,1\\}$ is computationally trivial. Now consider any complete lattice $L$, a test object $[T_0, T_1]$ and a morphism $f : L \\to [T_0, T_1]$. Given $x \\in L$, define $g : \\nabla \\{0,1\\} \\to L$ by\n$$g(p) = \\sup \\{ y \\in L \\mid y = x \\land p = 1 \\}$$\nThe map $f \\circ g : \\nabla \\{0,1\\} \\to [T_0, T_1]$ is constant, therefore\n$$f(\\bot) = f(g(0)) = f(g(1)) = f(x),$$\nwhich shows that $f$ is constant. $\\Box$\nP.S. A comment for the connoisseurs: all the above says is that all morphisms from a complete lattice to a modest set are constant.", "meta": {"post_id": 1221, "input_score": 11, "output_score": 14, "post_title": "What is the role of impredicativity in program extraction?"}}
{"input": "Every community can have inside jokes among it, and April Fool\u2019s jokes are a common variety, but proof assistants are particularly susceptible to April Fool\u2019s jokes. They have strong guarantees of correctness so one could claim both that these claims are wrong or that one found irrefutable proof of something crazy.  More so, they are very labor-intensive, making one possibly overly excited for claimed magical tools which speed up the field.\nWhat are some good April Fool\u2019s day jokes about proof assistants?\n\nFor those whose might vote to close this, let me at least say there is precedence for this question.  Also this question is not itself a April Fool\u2019s joke. :)\n\nOne answer per joke please.  Keep it to the subject of this website.  Provide a link if possible.  I assume this will be made community wiki.", "output": "Brunhilde/Rhein 2015, as announced on the Isabelle mailing list was a fully working German translation of Isabelle/HOL, including keywords. It also came with a homepage, example documents and suggestive screenshots such as this one:", "meta": {"post_id": 1229, "input_score": 6, "output_score": 10, "post_title": "What are good examples of April fools jokes in the proving assistant community?"}}
{"input": "When defining an inductive type, there is a famous \"positivity\" restriction on the constructor types. For example, an inductive type $\\mathsf D$ has constructor $\\mathsf c : F(\\mathsf D) \\to \\mathsf D$, where $F(\\mathsf D)$ is the type of the constructor argument, and it can contain $\\mathsf D$ itself. Then the positivity restriction basically states that the proof assistant needs to see that $F$ is a functor $\\mathsf{Type} \\to \\mathsf{Type}$(1). This is crucial because otherwise the induction principle wouldn't make sense: $$\\forall (P : \\mathsf D \\to \\mathsf{Prop}).\\left(\\forall (\\vec x : F(\\mathsf D)). \\mathtt{fmap}_{F} P \\, (\\vec x) \\to P(\\mathsf c\\, \\vec x)\\right) \\to \\forall x. P(x)$$ where $\\mathtt{fmap}_F$ is the map on the morphisms associated with the functor $F$. The proof checker, based on the consideration above, implements some syntactic criteria to ensure functorality, which become known as positivity. On the other hand, the \"strictly positive\" criterion, as I view it, is added to avoid size issues similar to that of set theory.\nThis has led me to wonder if such a criterion can be relaxed on propositions. In languages having definitionally irrelevant (or related weaker notions of) propositions, the propositions can be roughly viewed as a subsingleton, i.e. a subset of the singleton. This avoids all the size issues. Also, the functorality condition degenerates because of the irrelevance. So perhaps for inductive propositions, we can allow $F : \\mathsf{Prop} \\to \\mathsf{Prop}$ to simply be monotonic. So if the proof checker can deduce (from some syntactic criteria, or by a user supplied proof) that $F(p) \\to F(q)$ whenever $p \\to q$, then it should allow this inductive definition. In proof assistants embracing classical logic (at least in $\\mathsf{Prop}$), this can be much more powerful. The proof checker might just run a small SAT solver to see if the definition is monotonic.\nThe question is: does this leads to anything interesting (including possible paradoxes when combined with other features in the type theory)? Pointers to existing literature are also welcome.\n(1) : Let's ignore universe levels for now. $\\mathsf{Type, Prop}$ are just suggestive notations.", "output": "Impredicative strict Prop supports initial F-algebras for all F : Prop \u2192 Prop functors. In Coq:\nRequire Import Coq.Unicode.Utf8.\n\nRecord Functor : Type := mkFunctor\n  {ob : SProp \u2192 SProp; hom : \u2200 (A B : SProp), (A \u2192 B) \u2192 ob A \u2192 ob B}.\n\nRecord Alg (F : Functor) : Type := mkAlg\n  {carrier : SProp; phi : F.(ob) carrier \u2192 carrier}.\n\nArguments mkAlg {F}.\nArguments carrier {F}.\nArguments phi {F}.\n\nDefinition InitAlg (F : Functor) : Alg F :=\n  mkAlg (\u2200 (P : SProp), (F.(ob) P \u2192 P) \u2192 P)\n        (\u03bb fx P c, c (F.(hom) _ _ (\u03bb g, g P c) fx)).\n\nDefinition InitAlgInit (F : Functor)(A : Alg F) (x : (InitAlg F).(carrier)) : A.(carrier) :=\n  x (A.(carrier)) (A.(phi)).\n\nAs you mentioned, all equations can be omitted here because of the irrelevance, and there's no need to consider \"dependent\" elimination. For ordinary Prop, we would need an axiom of irrelevance to do the same. Impredicativity is generally required, else \u2200 (P : SProp), (F.(ob) P \u2192 P) \u2192 P) doesn't fit in SProp.\nI couldn't come up with examples of interesting applications of non-strictly positive SProp-s, maybe someone else here can.", "meta": {"post_id": 1239, "input_score": 10, "output_score": 10, "post_title": "Replacing (strict) positivity with monotonicity on propositions"}}
{"input": "Consider the simple indexed inductive type\nInductive Single : nat -> Set :=\n| single_O : Single O\n| single_S {n} : Single n -> Single (S n).\n\nIntuitively, I thought that Single n has a unique value for each n : nat.\nI started by trying to prove that forall s : Single O, s = single_O.\nHowever, the usual tactics inversion, destruct, and induction did not work:\nLemma single_O_unique (s : Single O) : s = single_O.\n    inversion s.  (* No effect *)\n    Fail destruct s.\n    Fail induction s.\n\nThe error messages were:\nAbstracting over the terms \"n\" and \"s\" leads to a term\nfun (n0 : nat) (s0 : Single n0) => s0 = single_O\nwhich is ill-typed.\nReason is: Illegal application: \nThe term \"@eq\" of type \"forall A : Type, A -> A -> Prop\"\ncannot be applied to the terms\n \"Single n0\" : \"Set\"\n \"s0\" : \"Single n0\"\n \"single_O\" : \"Single 0\"\nThe 3rd term has type \"Single 0\" which should be coercible to \n\"Single n0\".\n\nSo I resorted to a manual match expression:\n    refine match s with\n    | single_O => _\n    | single_S _ => _\n    end.\n\nResulting in the following proof context:\ns: Single 0\n\n(1/2)\nsingle_O = single_O\n\n(2/2)\nIDProp\n\nwhich was puzzling, but easy to prove:\n    - reflexivity.\n    - exact idProp.\nQed.\n\nQuestions:\n\nWhy was inversion unable to recognize that s could only be single_O and substitute accordingly?\n\nWhy did the refine tactic produce the subgoal IDProp?\n\nIs there a way to get inversion or destruct to work in this case?  Or, what would a better way to prove s = single_O?\n\n\nFull example:\nInductive Single : nat -> Set :=\n| single_O : Single O\n| single_S {n} : Single n -> Single (S n).\n\nLemma single_O_unique (s : Single O) : s = single_O.\n    inversion s.  (* No effect *)\n    Fail destruct s.\n    Fail induction s.\n    refine match s with\n    | single_O => _\n    | single_S _ => _\n    end.\n    - reflexivity.\n    - exact idProp.\nQed.", "output": "Or, what would a better way to prove s = single_O?\n\nI would define a function that, given a nat n, computes the canonical proof Single n.\nFixpoint Canonical (n : nat) : Single n := match n with\n  | O => single_O\n  | S n => single_S (Canonical n)\nend.\n\nYou can then easily prove that any Single n proof is equal to the canonical one by induction. Here the abstraction won't fail because the equality is already generic over n.\nLemma single_canonical (n : nat) (s : Single n) : s = Canonical n.\nProof.\ninduction s.\n - reflexivity.\n - simpl; f_equal; assumption.\nQed.\n\nYour original lemma is then a direct corollary.\nLemma single_O_unique (s : Single O) : s = single_O.\napply single_canonical with (n := O).\nQed.", "meta": {"post_id": 1246, "input_score": 11, "output_score": 8, "post_title": "Proving uniqueness of an instance of an indexed inductive type"}}
{"input": "I just started with Lean and with this nice SE.\nIn the official web book/tutorial, when explaining definitions https://leanprover.github.io/theorem_proving_in_lean/index.html they ask to complete this (partial) definition of curry (in paragraph 2.4):\ndef curry (\u03b1 \u03b2 \u03b3 : Type*) (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := sorry\n\nI tried\ndef curry (\u03b1 \u03b2 \u03b3 : Type*) (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := \u03bb  x y, f (x,y) \n\nand it seems to be accepted by the system.\nIs this definition correct? If yes, how do I use it?\nI expected something like\ncurry x y f    or      curry f x y\nshould work, but they don't.\nActually the given (partial) definition is of type \u03b1 \u2192 \u03b2 \u2192 \u03b3 so it expects just 2 arguments of type \u03b1 and \u03b2 respectively, but no function f.\nThank you for your help", "output": "If Lean checks your code you know it is correct.  (Well, that and the fact there is only one curry function up to functional equivalence.)\nNow as for how to use curry you just defined, notice the type signature says you need to put in the following things in order:\n\n\u03b1 : Type*\n\u03b2 : Type*\n\u03b3 : Type*\nf : \u03b1 \u00d7 \u03b2 \u2192 \u03b3\nx : \u03b1\ny : \u03b2\n\nSo you have to use it as, e.g.\ncurry \u2115 bool \u2124 f n b\n\n\nNote, it is cumbersome to have to write the types since you can usually get them from the types of f, n, and b in this case.  So usually (and the tutorial will get to this soon), you write this function using curly braces for the arguments which are implicit:\ndef curry' {\u03b1 \u03b2 \u03b3 : Type*} (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := \u03bb  x y, f (x,y)\n\nIn this case, you can write this as\ncurry' f x y\n\nor in rare cases when you need to supply the types:\n@curry' \u03b1 \u03b2 \u03b3 f x y", "meta": {"post_id": 1249, "input_score": 9, "output_score": 9, "post_title": "How to define curry in Lean"}}
{"input": "How are the Coq math-comp account and repositories related?\n\nDetails\nOne of my side goals is to try to keep the tags on this site meaningful and useful.\nToday I ran into this question:\nHow to prove has_esp when using mathcomp.analysis?\nI do not use Coq so take that in mind.\nWhen I saw\n\nPage of mathcomp.analysis: https://github.com/math-comp/analysis\n\nI thought it was odd as the tag coq-mathcomp was used, but then a specific repository was given. In looking at at the mathcomp account (https://github.com/math-comp), which is different from the matchcomp repository (https://github.com/math-comp/math-comp) I did see a separate repository for analysis (https://github.com/math-comp/analysis).\nI know that sometimes that with GitHub accounts multiple repositories can be loaded into a tool by various means with a package management system being common.\nI also downloaded the code as noted on the web page (https://github.com/math-comp/math-comp/releases) and searched for an analysis directory and found none thinking that analysis would be included in the download on the web page.\nSo it seems that some useful information is glossed over on the web page.\nMy current thought to my question above is\n\nmath-comp/math-comp - What most users mean when the mention Coq and math-comp\nmath-comp/analysis - Analysis code not installed with math-comp/math-comp\nmath-comp/algebra-tatics - Another repository not installed with math-comp/math-comp\nmath-comp/math-comp.github.io - The website https://math-comp.github.io/\n\n...\nNeedless to say for those that use it often, it is not confusing, but to some one new it is as confusing as the different Lean accounts. (ref)\nThe primary goal of this question is to have an answer that makes it clear to some one not familiar with Coq math-comp account and repositories to understand what they need to install, be it one of the repositories or a combination of them.\nUsing that answer, it should be clear enough to know how to add tags related to math-comp as they are needed. That does not need to be addressed in the answer, but it will be used as a criteria for accepting an answer.\n\nNote: For the question How to prove has_esp when using mathcomp.analysis?, I created, added and edited the tag coq-mathcomp-analysis", "output": "As Meven wrote, the Mathematical Components repositories originate from the work of Georges Gonthier and his team proving first the Four Color Theorem, then the Odd Order Theorem.\nIn Coq people do not install \"repositories\", they install \"packages\" (most often through the opam package manager, but this can also be through the Nix package manager or through a distribution like Debian). The MathComp library contained in the main repository (math-comp/math-comp) is already divided into multiple packages:\n\ncoq-mathcomp-algebra\ncoq-mathcomp-character\ncoq-mathcomp-field\ncoq-mathcomp-fingroup\ncoq-mathcomp-ssreflect\n\nBut there are many more packages that have the coq-mathcomp- prefix and are also available to users. These packages are developed in separate repositories (most of them under the math-comp organization, but now the fourcolor repository has migrated to the coq-community organization).\nRoughly speaking, the criteria for a package to be inside or outside the main repository is supposed to be its quality and maturity, but also its reusability (that's why odd-order and fourcolor are outside).\nAll of these packages and the ones that depend on them form what one could call the \"MathComp sub-ecosystem of the Coq ecosystem\".", "meta": {"post_id": 1276, "input_score": 8, "output_score": 13, "post_title": "Explanation of Coq math-comp repositories"}}
{"input": "In the init/logic.lean file of the Lean 3 standard library, nonempty and inhabited are defined. It seems like these two classes are very similar - you can instantiate either of them given an element of the type. In what situations do you use one over the other?", "output": "The difference between nonempty and inhabited is that nonempty A : Prop but inhabited A : Sort (max 1 u) (if A : Sort u). This makes inhabited A isomorphic as a type to A, while nonempty A is the propositional truncation of A, equivalent to \u2203 x : A, true. To extract a value from inhabited A is inhabited.default, but extracting a value from a proof of nonempty A is the axiom of choice and is noncomputable.\nGenerally, you should use nonempty A if you only need the \"mere fact\" that A is not empty, while inhabited A is used if you need to access a specific default value for totalizing a function or so. Mathlib has a linter to ensure that you don't use inhabited in place of nonempty for proving theorems unless you need it in the statement of the theorem.", "meta": {"post_id": 1286, "input_score": 13, "output_score": 15, "post_title": "Lean \"nonempty\" vs \"inhabited\""}}
{"input": "I have this function:\nDefinition bexp x y := bexp_r x y [true].\n\nAnd I have this goal:\nvalue (bexp [] y) = 0 ^ value y\n\nI want to replace the call to bexp with the underlying bexp_r.\nI tried to use the simpl tactic but it did not replace it.", "output": "The tactic you are looking for is probably unfold. If you write unfold bexp it will replace it with its body.\nIn certain cases, unfolding the definition will unfold too much and if you want more control over it you can prove unfolding lemmas instead and rewrite with them.", "meta": {"post_id": 1297, "input_score": 6, "output_score": 10, "post_title": "How to replace a function by its body"}}
{"input": "The docs on lambdas in Agda provide two forms of lambda: a curly brace based version, and the where syntax. But while writing some programs, I stumbled across a third version: one pattern, no braces, no where. I thought it would behave the same as a single pattern and braces, but here's a counter example to that:\nnatid : \u2115 \u2192 \u2115\nnatid = \u03bb x \u2192 x\n\ng0 : ( \u03bb x \u2192 natid x ) \u2261 ( \u03bb x \u2192 natid x )\ng0 = refl\n\ng1 : ( \u03bb { x \u2192 natid x } ) \u2261 ( \u03bb x \u2192 natid x )\ng1 = refl\n\ng2 : ( \u03bb x \u2192 natid x ) \u2261 ( \u03bb { x \u2192 natid x } )\ng2 = refl\n\ng3 : _\u2261_ {_} {_ \u2192 _} ( \u03bb { x \u2192 natid x } ) ( \u03bb { x \u2192 natid x } )\ng3 = refl\n\n-- g4 : ( \u03bb { x \u2192 natid x } ) \u2261 ( \u03bb { x \u2192 natid x } )\n-- this fails with error message:\n     _a_91 : Agda.Primitive.Level  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,28-29 ]\n     _A_92 : Set _a_91  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,28-29 ]\n     _94 : _A_92  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,8-25 ]\n\n     \u2014\u2014\u2014\u2014 Errors \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n     Unsolved constraints\n\nSo my question is, what does a lambda with no braces around a single pattern mean, and why does the type inference of Agda treat g1/g2/g4 differently.", "output": "The key thing about \"pattern\" lambda is that it's a fake: each such thing elaborates as the local invocation of a new (i.e., generative) top-level helper function. The type declaration for that helper function must be somehow synthesized, and it is indeed important to allow it to be a dependent type. It looks to me as if allowing the extra degrees of freedom on both sides of the equation results in an underconstrained inference problem.\nIn response to the question in the comment, the g3 example gives _ -> _ as the type at which equality is demanded, which constrains the type of the function to be non-dependent. That seems to be enough information to ensure a most general solution.\nMeanwhile, in reference to the examples which do check out, note that g0 requires only syntactic identity to go through; the other three work only thanks to eta-expansion and laziness. You should really think of each pattern lambda as some generated foo, which has a definition given by what's in the braces. Neither foo nor \\ x -> natid x reduces. Fortunately, as they have function type, Agda will generate a fresh variable standing for a hypothetical input and apply both functions to it, at which point they do both compute. But foo computes only because it is lazy in its input.\nSo here's the classic gotcha:\nyay : (\\ f -> _\u2261_ {_}{Bool -> Bool} f f) (\\ { false -> true ; true -> false })\nyay = refl\n\nchecks, but\nnay : _\u2261_ {_}{Bool -> Bool}\n        (\\ { false -> true ; true -> false })\n        (\\ { false -> true ; true -> false })\nnay = {!refl!}\n\ndoes not.\nHow so? In yay only one auxiliary definition is generated, and it is seen to be the same as itself by name. In nay, two auxiliary definitions are generated, differing only in name, but eta-expansion is not enough to make the names compute away.", "meta": {"post_id": 1305, "input_score": 8, "output_score": 8, "post_title": "Curly Braces and Lambdas in Agda"}}
{"input": "Theorem search\n  {P : nat -> Prop} (dec : forall n, {P n} + {~P n})\n: ~~(exists n, P n) -> {n | P n}.\nAdmitted.\n\nI don't think this is provable in Coq without additional axioms, and it is provable when assuming LEM : forall A : Prop, A \\/ ~A. However, I don't think it proves LEM. What are the weakest axioms that prove search? I'm not very familiar with all the versions of the axiom of choice, but I guess this is probably equivalent to one.", "output": "This is an axiom in itself, known as Markov's principle. A more compact but equivalent way to state this axiom is\nAxiom MP : forall (f : nat -> bool), ~~ (exists n, f n = true) -> exists n, f n = true.\n\nThe original phrasing of the question hardwires a bit of choice that is provable in CIC, known as constructive definite description.\nLemma description : forall (f : nat -> bool), (exists n, f n = true) -> {n | f n = true}.\n\nThis theorem is a consequence of singleton elimination.\nMarkov principle is not provable in CIC, but it enjoys a special status amongst semi-classical principles. As already observed, it is a consequence of classical logic in Prop. As such it does not endanger the computational properties of extraction. Even better, you can actually natively extend CIC with it without breaking strong normalization and canonicity. See for instance this other question for more details.", "meta": {"post_id": 1332, "input_score": 9, "output_score": 11, "post_title": "What axioms do I need to search the naturals?"}}
{"input": "I have used Coq's bidirectionality hints (placement of & in a call to Arguments) to some effect, mostly by trial and error. Despite familiarity with bidirectional type checking, I can't connect the explanation given in the documentation with what I actually observe. So, precisely what change does & make to the checking of an application? To start, I understand the default bidirectional typing rule for applications to be, more or less (read clockwise, with \u2208 denoting type synthesis and \u2264 denoting subtyping (including unification)):\nf \u2208 \u2200 (x1 : A1) ... (xn : An), B\nM1 \u2208 A1'    ...    Mn \u2208 An'\nA1' \u2264 A1    ...    An\u2032 \u2264 An\n----------------------------------\nf M1 ... Mn \u2208 B[M1/x1, ..., Mn/xn]\n\nSome questions I'd like to be answered as corollaries of the explanation:\n\nIs Arguments foo & x y. equivalent to Arguments foo x y.?\nIs Arguments foo x y &. equivalent to Arguments foo x y.?\nIs Arguments foo {x} & y. equivalent to Arguments foo & {x} y.?\nWhat happens for partial applications (of Curried functions)?\nIs the Arguments f ... & ... . mechanism reasonably complete for bidirectional typing strategies you may want/expect?", "output": "I'm familiar with bidirectionality hints in Coq but with superficial knowledge of bidirectional typing, so some details may be off.\nAs our running example consider this binary function with a bidirectionality hint in the middle:\nParameter f : forall (x : A), B x -> C x\nArguments f x & y.\n\nBidirectionality hints affect type checking of an application.\nNormally, function application is associated with a synthesis (\u2191) rule, which turns into checking (\u2193) via the standard subsumption/unification rule (denoted \u2291, but in Coq it's more like an =, except that coercions make it asymmetric again):\nf : forall (x : A), B x -> C x\na \u2193 A     b \u2193 B a\n----------------- app-syn\n  f a b \u2191 C a\n\ne \u2191 T'     T' \u2291 T\n----------------- syn-check\n      e \u2193 T\n\nThey combine into the following rule for checking an application f a b:\nf : forall (x : A), B -> C x\na \u2193 A     b \u2193 B a    C a \u2291 T\n---------------------------- (1)\n         f a b \u2193 T\n\nNotably, the unification with the required type T happens at the end.\nA bidirectionality hint moves that unification earlier, to the &:\nf : forall (x : A), & B x -> C x      (* \u2190 f with a bidir. hint after 1 argument *)\na \u2193 A     C a \u2291 T     b \u2193 B a\n----------------------------- (2)\n         f a b \u2193 T\n\nOf course, rules (1) and (2) are equivalent when read purely declaratively. The difference becomes observable when those rules are interpreted in a stateful context. In Coq, we are actually type checking terms containing metavariables (holes), which get instantiated by unification, possibly creating redexes, hence the order in which premises are evaluated matters.\nIn this example, if we type check f ?a b, containing a hole ?a, then the standard type checking algorithm (rule 1) will try to check b \u2193 B ?a first, which may get stuck on that hole. If we take the bidirectionality hint into account (rule 2), we first unify C ?a \u2291 T, which may hopefully instantiate ?a so that subsequently checking b \u2193 B ?a succeeds.\n\nAs a concrete example, consider the following definitions. So is a predicate on Booleans which is inhabited by I when the Boolean is true, and uninhabited when the Boolean is false. From a proof of So b, So_eq extracts a proof that b = true. We use So as a means of proof automation when using auto_fin \u2013 we can cast a concrete natural number m into a bounded natural number of type Fin.t n whenever m <? n computes to true.\nRequire Fin.\nRequire Import PeanoNat.\n\nDefinition So (b : bool) : Prop := if b then True else False.\n\nDefinition So_eq {b : bool} : So b -> b = true :=\n  match b with\n  | true => fun _ => eq_refl\n  | false => False_rec _\n  end.\n\nDefinition auto_fin {n} m (prf : So (m <? n)) : Fin.t n :=\n  Fin.of_nat_lt (proj1 (Nat.ltb_lt _ _) (So_eq prf)).\n\nWe want to apply auto_fin as follows, checking it against a finite type with concrete bound (5 in this case). However, from the error message we can see that this fails because the implicit argument n (which we can see should eventually be 5) has not been instantiated by the time we check the prf argument I.\nFail Check auto_fin 3 I : Fin.t 5.\n\n(* The term \"I\" has type \"True\" while it is expected to have type \"So (3 <? ?n)\" *)\n\nTo fix this, we want to make sure that n will propagate from the type we're checking against to the type of prf before we check prf. A bidirectionality hint can provide this behaviour:\nArguments auto_fin {n} m & prf.\n\nThe old and new type checking rules for auto_fin are, respectively, as in the following preformatted block. With the instantiations n = ?n, m = 3, prf = I, T = Fin.t 5, we see that the new rule is able to solve for ?n by unifying Fin.t ?n with Fin.t 5, and then proceed to reduce So (3 <? 5) to True, against which I checks. The key feature is that solving n by unification against the result type allows for reduction in the later argument prf.\nOld:\nn \u2193 nat    m \u2193 nat    prf \u2193 So (m <? n)    Fin.t n \u2291 T\n------------------------------------------------------\n                @auto_fin n m prf \u2193 T\n\nNew:\nn \u2193 nat    m \u2193 nat    Fin.t n \u2291 T    prf \u2193 So (m <? n)\n------------------------------------------------------\n                @auto_fin n m prf \u2193 T\n\n\n\n\nIs Arguments foo & x y. equivalent to Arguments foo x y.?\nIs Arguments foo x y &. equivalent to Arguments foo x y.?\n\n\nAs explained above, bidirectionality hints move a unification step from the end to the middle of an application. So (1) no, (2) mostly yes. Only \"mostly\" because with dependent types the \"arity\" of a function might depend on the arguments, so in full generality there is no way to specify a bidirectionality hint (essentially a number of arguments) that is equivalent to no hint.\n\n\nIs Arguments foo {x} & y. equivalent to Arguments foo & {x} y.?\n\n\nType checking happens after implicit arguments have been elaborated into metavariables (foo y becomes @foo ?x y). Furthermore you can always ignore implicit arguments with the @foo syntax. So these hints differ.\n\n\nWhat happens for partial applications (of Curried functions)?\n\n\nSince the effect of a bidirectionality hint is to insert a unification after some number of arguments, this works smoothly with currying. If a function is applied to fewer arguments than its hint, then it will be checked as if there was no hint.\n\n\nIs the Arguments f ... & ... . mechanism reasonably complete for bidirectional typing strategies you may want/expect?", "meta": {"post_id": 1343, "input_score": 15, "output_score": 8, "post_title": "How do Coq's bidirectionality hints (`&`) affect type checking?"}}
{"input": "Question\nIs CiC stronger than CoC, in terms of proof strength?\nContext\nTo illustrate the kind of confusion I am in, and what I'd like to learn from the answer, here is part of my inner monologue:\nIf I have understood correctly, CiC is CoC plus inductive definitions. An inductive definition places one or more types into the context, plus some constants and a recursor function. So is it just syntactic sugar to formulate assumptions or is it kind of an axiom, that recursors exist for every possible inductive definition? Can you do inductive definitions within a proof and if so, how would this look like in coq or lean?\nExample: Let FTT be a rephrasing of the Feit\u2013Thompson theorem, that does't refer to the integers or any other infinite structure, e.g. \"Every (Tarski-)finite non abelian simple group has a non trivial involution\". Thanks to Gonthier et alt. FTT is clearly provable in coq/CiC. I assume the proof uses the integers. Is FTT\nstill provable if we did not define the integers before? If yes, can we create the integers on the fly when we need them in the proof? Or is there a procedure to eliminate the use of the integers from the proof?", "output": "Meven's answer explains that without inductive types, you cannot hope to recover a natural number object with proper induction principles. You can interpret this as saying that you lose some mathematical expressiveness.\nHowever, in the Calculus of Constructions (CC) you can still define the Church integers\n$N := \\forall (X : \\mathrm{Prop}) , X \\to (X \\to X) \\to X$,\nand even though they don't come with an induction principle, you can still define exactly the same functions as terms $N \\to N$ as in CC + Inductives. So, in a way, you do not get any proof-theoretic strength out of inductives in an impredicative system.\nNote that it is not true at all for predicative systems, such as MLTT, where inductive types are the main source of proof-theoretical strength.\n\nLet's make this claim more precise: by CC, I mean the original system of Coquand, which can be expressed as a PTS with two sorts $\\mathrm{Prop}$ and $\\mathrm{Type}$, the first being impredicative.\nFor simplicity, I will only consider one inductive type, the integers. I use CC+Nat to mean CC extended with an inductive type $\\mathbb{N} : \\mathrm{Prop}$ with large elimination (and adequate computation rules):\n$$\n\\frac{P : \\mathbb{N} \\to \\mathrm{Prop} \\quad t_0 : P\\ 0 \\quad P_S : \\Pi\\ n. P\\ n\\to P\\ (S\\ n) \\quad n : \\mathbb{N}}{\\mathrm{natrec}(P, t_0, t_S, n) : P\\ n}\n$$\n$$\n\\frac{K : \\mathrm{Type} \\quad P_0 : K \\quad P_S : K\\to K \\quad n : \\mathbb{N}}{\\mathrm{Natrec}(K, P_0, P_S, n) : K}\n$$\nOne can show that any term of type $N \\to N$ (where $N$ still means the Church integers, not $\\mathbb{N}$!) in CC+Nat corresponds to an integer function using the normalization theorem. I will first explain that any such function is provably recursive in higher-order Peano arithmetic without choice (PA\u03c9), and then I will explain that any function that is provably recursive in PA\u03c9 can be expressed as a term in CC. Thus, CC+Nat and CC can define exactly the same functions on the Church integers.\n\nFrom CC+Nat to PA\u03c9\nThis direction requires analyzing a normalization proof (this one doesn't treat large elimination, but we can follow Werner on this). Given any typing derivation in CC+Nat, our goal is to prove that the term is normalizing within PA\u03c9 (we cannot do it uniformly for all derivations, as the proof strength of PA\u03c9 is not sufficient).\nFirst, we can show through simple syntactic consideration that in CC+Nat, terms can be stratified into kinds (inhabitants of $\\mathrm{Type}$), predicates (inhabitants of a kind), and objects (inhabitants of a predicate of kind $\\mathrm{Prop}$), and that nothing complex happens at the level of kinds \u2014 they can't be abstracted over, and they can be described by the following syntax:\n$$\nK ::= \\mathrm{Prop}\\ |\\ \\Pi (\\alpha : K) . K\\ |\\ \\Pi (x : T) . K\\qquad \\text{where $T$ has type Prop}\n$$\nThe standard reducibility proof assigns a set-theoretic interpretation to each kind:\n$$\n\\begin{split}\n\\mathcal{V}(\\mathrm{Prop}) & = SAT \\quad \\text{The set of saturated sets of terms} \\\\\n\\mathcal{V}(\\Pi (\\alpha : K_1) . K_2) & = \\mathcal{V}(K_1) \\to \\mathcal{V}(K_2)\\\\\n\\mathcal{V}(\\Pi (\\alpha : T) . K) & = \\mathcal{V}(K_2)\n\\end{split}\n$$\nUnfortunately, we cannot reproduce this definition in PA\u03c9, because it \"lives in kind \u03c9+1\".\nBut we are only interested in proving normalization for a specific derivation, and only finitely many kinds appear in that derivation \u2014 so we can restrict ourselves to a subtheory with bounded kinds, in which case the interpretation is definable in PAn for some n.\nThen one proceeds to assign an element $[P]$ of $\\mathcal{V}(K)$ to every predicate $P$ of kind $K$ by induction on the syntax, in the usual fashion (see this summary for instance. Integers and predicates defined by recursion are treated in Werner's thesis).\nFinally, one shows by induction on the typing derivations that if $\\vdash t : A$, then $t \\in [P]$ and thus $t$ is normalizing.\nWe have that if $\\vdash f : N \\to N$ in CC+Nat, then PA\u03c9 proves that for any Church integer $\\overline{n}$, $f\\ \\overline{n}$ is normalizing (since it does not involve more kinds than $f$). After some simple considerations on normal forms, we can show that it normalizes to an integer $\\overline{m}$, and thus we showed that the function coded by $f$ is recursive.\n\nFrom PA\u03c9 to CC\nGirard's representation theorem shows that any function that is provably recursive in PA\u03c9 can be expressed as a term in System F\u03c9. The idea is to first use a double negation translation and \"Friedman's trick\" to reduce the problem to higher-order Heyting arithmetic HA\u03c9, and then translate HA\u03c9 into F\u03c9 by erasing integers.\nFinally, there is an obvious embedding of System F\u03c9 in CC.\n\nExtensions\nThis proves that CC+Nat and CC can define the same functions on the Church integers. What about stronger systems?\nWerner actually treats a full scheme of inductives with large elimination for small constructors (also known as \"Impredicative-Set\"). I would expect that the same technique carries through for the full scheme, but inductives are somewhat messy.\nWhat about a universe hierarchy? We could hope that the full CIC (CC with a universe hierarchy and inductives) has the same proof-theoretic strength as CC\u03c9 (CC with a universe hierarchy). However, I don't know any normalization proof for CIC \u2014 we cannot do the stratification by kind anymore, so it is not clear whether a similar technique works.", "meta": {"post_id": 1360, "input_score": 13, "output_score": 10, "post_title": "Calculus of (inductive) Constructions: Do inductive definitions increase proof strength?"}}
{"input": "I've been playing with Lean, trying to prove the next lemma:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\n\nHere is the proof I've currently written:\nbegin\n  intro h,\n  cases classical.em C; cases classical.em A; cases classical.em B; try {right, assumption};\n  {right, apply h, intro hA, contradiction},\n  {sorry},{sorry},{sorry}\nend\n\nFirst {} seems okay; here is the Tactic state right before the contradiction:\n1 goal\nABC: Prop\nh: (A \u2192 B) \u2192 C\nh_1: \u00acC\nh_2: \u00acA\nh_3: \u00acB\nhA: A\n\u22a2 B\n\nSo, I have that h_2: \u00acA and hA: A, which should lead to the contradiction. Applying contradiction in Coq thus leads to completing the goal.\nBut here in Lean something strange occurs: in Tactic state window I get the notification goals accomplished , whileas in Messages I get contradiction tactic failed with the next state:\nstate:\nA B C : Prop,\nh : (A \u2192 B) \u2192 C,\nh_1 : \u00acC,\nh_2 : A,\nh_3 : \u00acB,\nhA : A\n\u22a2 B\n\nSo it seems like h_2 somehow has been changed, and this might lead to that message, although the goal is completed. Probably I've used contradiction tactic in the wrong way, but I don't see that for now. Can anyone help me to figure things out and to get rid of the message?\nThanks in advance!", "output": "The issue is the ; at the end of the long line. This is causing the {right, ...} block to be applied to all goals, meaning that the {sorry} blocks never have a chance to run, and the error you are getting at contradiction is in the second invocation on the block on the second goal. Change the ; to , to fix the issue:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nbegin\n  intro h,\n  cases classical.em C;\n    cases classical.em A;\n    cases classical.em B;\n    try {right, assumption}, -- <- note the comma\n  {sorry},\n  {sorry},\n  {right, apply h, intro hA, contradiction},\n  {right, apply h, intro hA, contradiction},\nend\n\n(Actually, as you can see the provided proof only works on goals 3 and 4; it is the first two that fail.)\nAlternatively, you can keep the ; but use try to apply the block anywhere it works:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nbegin\n  intro h,\n  cases classical.em C;\n    cases classical.em A;\n    cases classical.em B;\n    try {right, assumption}; -- <- note the semicolon\n    try {right, apply h, intro hA, contradiction},\n  {sorry},{sorry},\nend\n\n\nOr if you are feeling lazy (and are using mathlib)\nimport tactic.tauto\nlemma l1_cl (A B C : Prop) : ((A \u2192 B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nby tauto!", "meta": {"post_id": 1366, "input_score": 7, "output_score": 11, "post_title": "In Lean, contradiction tactic failed but actually goal accomplished"}}
{"input": "Most theorem provers simply axiomize inductive types (or equivalently W types) in the abstract which is fine.\nBut I'm curious about explicit constructions of inductive types within the theory.\nI suppose I'm interested in both inductive types and \"weak inductive types\" (weak initial algebras) such as impredicative encodings. I think the term I've most found in my searches has been \"weak natural numbers object.\" Inductive types can be better but weak inductive types are cool too.\nI know you can use impredicative universes as in System F to encode inductive types. Or you can just accept a universe bump. And apparently if you internalize a small amount of parametricity you can construct appropriate induction principles.\nBut I'm pretty sure I've heard you can construct inductive types other ways. I think maybe you need to assume some classical principles? I've read some stuff on transfinite induction I still don't really get. I think once you have a base inductive type of ordinals you can construct other inductive types in those terms?\nI have a hunch you can abuse impredicative proof irrelevant propositions and natural numbers to construct inductive types but I don't really have anything solid here.\nAlso for some reason I think it's easier to construct free monads the hard way instead of inductive types? I'm not sure this is an important issue.\nI don't think W types and polynomial endofunctors directly solve the problem. They provide some clarifying language for how to talk about inductive types but they're not quite an explicit construction.\nThere's a paper \"Induction Is Not Derivable in Second Order Dependent Type Theory\" I don't understand yet but I think all this means is you need to assume more axioms than second order dependent type theory?", "output": "In the HOLish settings, these types (starting with the natural numbers) are indeed constructed from first principles; they're certainly not axiomatised.  Harrison had an early (1995) paper on how to do this, and the technology has developed from there.\nHarrison's construction doesn't use ordinals, but encodes its types as trees underneath and then uses an inductive relation and the HOL type definition principle to prune away values that are not desired.\nThe more capable and more recent technology in Isabelle/HOL (\u201cbounded natural functors\u201d) does have some fancy cardinality reasoning behind it.  There's a nice 2012 LICS paper on this tech by Traytel et al.", "meta": {"post_id": 1376, "input_score": 9, "output_score": 10, "post_title": "Construction of inductive types \"the hard way\""}}
{"input": "A neutral/normal term in the lambda calculus is typically defined\ndata nf = Lam of nf | Neu of ne\ndata ne = Var of int | App of ne * nf\n\nNow the question is what to do about constructors and eliminators such as pairs, projections, natural numbers, or the recursor.\n\nWe could view them all neutral terms because they are just variables pair, fst, snd, zero, succ, rec that happen to have some semantic meaning. For example:\n\ndata ne = Var of int | App of ne * nf \n| Pair of nf * nf | Fst of nf | Snd of nf\n| Zero | Succ of nf | Rec of nf * nf * nf\n\n\nBut this question considers an alternative, namely to view constructors (pairs, natural numbers) as normal terms and eliminators (projections, recursor) as neutral terms, which also seems to make sense considering Lam is the constructor for function types and App the eliminator. For example:\n\ndata nf = Lam of nf | Neu of ne\n| Pair of nf * nf \n| Zero | Succ of nf\ndata ne = Var of int | App of ne * nf\n| Fst of ne | Snd of ne \n| Rec of nf * nf * ne\n\n(Edit) It seems in the first approach, terms are only normal up to $\\lambda$-reductions, whereas in the second, terms are in fully normal form.\nIs there an advantage to one approach over the other?", "output": "A neutral term is a variable, or an eliminator stuck on a neutral term. Basically a stack of eliminators applied to a variable.\nThese terms are called neutral, and distinguished from other normal forms, because they have not reduced to a constructor, but they cannot reduce further (because the eliminators are stuck on a variable).\nWith this in mind, your second definition of the type of normals and neutrals is the correct one. I would say this approach has the advantage of actually representing normal forms. I'm not sure why you would want a language with pairs where fst (pair zero zero) does not reduce.\nI would also caution that constructors like pair and zero, and eliminators like fst and rec really are not \"just variables\", anymore than lam is just a variable. Of course they are \"variables\" in the sense that they are strings of characters, but as you say, they have semantic meaning distinct from variables. When I used \"variable\" earlier in this answer, I meant a thing with the semantics of a variable, that can be the subject of substitution.", "meta": {"post_id": 1397, "input_score": 7, "output_score": 9, "post_title": "What is a neutral term?"}}
{"input": "The chess computer which beat the human world champion in 1997 had a huge database of openings inbuilt into it. However my understanding of Deep Mind's alpha zero is that it is capable of generating these openings by itself.\nPerhaps a theorem-proving analogy would be the following. The research groups trying to develop systems which do mathematics automatically -- is it true that right now they would have to teach such a system a proof of the infinitude of primes first (as \"training data\" or whatever)? And we're not yet at the stage where the systems can figure out a proof of this 2000+ year old result by itself? Or am I out of date?", "output": "I doubt you are out of date.  I think given a reasonable interpretation of this question, this is well beyond current (2022) systems.\nNonetheless, I will also argue your question needs some clarification as to what \"by itself means\".  Rhetorically, if GPT-f or the Isabelle Hammer could solve the infinitude of primes proof, would this count?\nGPT-f (in Lean or Metamath) is a trained model based on human proofs.  Assuming the training data didn't include the proof of infinitude of the primes, but still included much of Lean's (or Metamath's) library, would that count?  There are probably other proofs about the prime numbers and other theorems about sets being infinite.  Also, GPT-f takes advantage of Lean's tactics which also take a lot of math knowledge to create.  Is all this too much knowledge to be able to count for it being able to come up with the proof \"by itself\"?\nReducing the knowledge slightly are systems found in the papers Reinforcement Learning of Theorem Proving and Learning to Reason in Theories Without Imitation  These systems don't use any proof data.  They only use theorem statements, and definitions, and (sometimes) built-in tactics.  Would this count?\nSecond, the Isabelle Hammer is a mostly hand engineered tool (with a bit of machine learning), but a very powerful one.  Nonetheless, a lot of it's engineering is based on our understanding of first order logic and common mathematical objects like the integers.  Also, it again can use existing definitions and previous theorems in the library.  Does this count as \"by itself\"?\nAnd just to be clear, I don't know for sure if either GPT-f or the Isabelle Hammer can do this.  I doubt it, but one would have to check.  Nonetheless, my point was more of a thought experiment, since I think even if they could, it would wouldn't count.\nNow one could theoretically design a system which starts from \"nothing\" and tries to prove theorems.  Here \"nothing\" could mean a few things:\n\nIt could start from some axiom system like Lean's axioms, as well as just the definitions needed to state the infinitude of primes.  And it has the infinitude of primes as a goal.  It can't use human-written tactics.  It has to just use the base logic.\nSame as (1) but now it is not given any definitions or goals.  It just has to come up with interesting math on its own.\nSame as (2), but now we don't even have a logical framework.  It has to develop mathematics on its own.\n\nI am confident we can't do any of those in 2022.  And I question whether we can do (2) or (3) ever without somehow cheating and imparting mathematical knowledge.  Imagine for example how humans learned math throughout history.  I'm not an expert on this, but I imagine many real world concerns came into play:  Division of property for inheritance (both geometric land plots and discrete objects and animals), engineering, understanding physics, money and accounting, and so on.  Even the idea of primes might have been linked to our understanding that some things are made of multiple other things and then it is natural to ask how many basic things are there.\nNow, we could train an agent in a realistic multi-agent simulation environment with physics, an economy, geometry, etc with the hope that it learns math, but to code that up we are putting in our math knowledge.  Alternatively, we could feed a model with a lot of real world data (scraped from say the Internet), but again I would worry that this data already contains a lot of math knowledge.\nMy personal thought on this particular challenge is that the infinitude of primes should be replaced with a harder problem, and eventually we should work up to an AI which can develop unmistakably new mathematics or solve difficult unsolved problems.  Then there is no trouble throwing in data, throwing in existing math knowledge, giving the model goals, and throwing in real world data.  Then we don't have to worry as much about if we are cheating.  We just have to worry about building the future AI mathematician.\n\nUpdate: I want to update my answer with why I think my version (1) of the problem posed (which is the easy case) is much more difficult than chess, go, or Atari games, but also has a flavor of being possible with technologies on the horizon.\nRecall, in version (1) the learning agent knows the axiomatic rules of a theorem prover and knows the goal (prove that there are infinitely many primes) including all the necessary definitions leading up to that goal.  However, it doesn't have access to other theorems of mathematics, other definitions, or advanced tactics. At first, this feels just like the scenario of Alpha Zero where we start with the rules of chess and a goal of winning.  But note there are a few key differences, and in my opinion, one of the most key differences is that two-player games like Alpha Zero make it easy to develop a curriculum of progressively harder problems.  The agent plays itself, and to make progress it only needs to learn just enough to beat itself.  Combine that with a good learning tool (neural networks), a good search algorithm (Monte Carlo tree search), and enough computation, it can advance from random play to grand master.  But theorem proving doesn't have this.  Theorem proving is a one player game, also known as a puzzle.  However, even in many puzzles it is possible to develop good curriculums.  To solve Sokoban, many agents just start solving small random levels, progressively getting better on harder levels.  Similarly, for the Rubik's cube.  Moreover, the Rubik's cube has a nice trick.  You can start from a solved cube, shuffle it up, and preform every step of that shuffling in reverse to have a solution.  In this way it is possible to get lots of training data at many levels of difficultly (distance from the solution).\nTheorem proving seems different.  Sure if one already has a vast library of theorems statements (as in much of the previous work on AI and theorem proving), then it is possible to use those theorems as a curriculum for reinforcement learning.  The model will first solve the easiest problems by random chance, learn from those problems, and then be able to solve harder problems.  We can even improve this, as many papers do, by learning from partially solved proofs.  The real question however is if any of this is still possible without a bank of starting problems to get any traction?\nThere is however still a path forward.  The agent could just randomly explore the space of proofs, combining axioms and rules until it proves something.  Then it could learn from those proofs.  As it get's better, it can find harder and harder proofs.  But unlike the Rubik's cube, it is hard to find a random proof and harder still to find an \"interesting one\".  Indeed, now we are getting into hard problems in reinforcement learning such as learning to explore an open world with few or no explicit goals.  In such exploration, teh agent must be driven by \"curiosity\", but what makes an experience \"novel\" in that it should be remembered and learned from in the future, or sought out?  And in mathematics, what makes a mathematical fact interesting?\nNow one might see the benefit of a more holistic approach.  Maybe a theorem is \"interesting\" if it is useful to prove other theorems?  Maybe agents shouldn't just prove theorems, but also conjecture them?   Maybe agents shouldn't just focus on exploring the space of theorems, but also focus on other mathematical concepts like computation?  (Recall, a lot of math has been developed just to perform computations practically.) And soon we may want to be venturing outside the world of pure math?  (And this is why I'm not sure if Euclid's theorem is a good test problem or not, since it's simplicity requires avoiding real world knowledge for fear we might be tainting our model with facts about the prime numbers and arithmetic.)\nBut even if we stay inside the world of pure math, we still probably need our agent to develop its own definitions.  For example, Euclid's proof uses the product of all the first n primes, which is a special case of an iterated function.  What on Earth could make an algorithm find and explore such a concept?  And why is that concept any more \"interesting\" or \"natural\" or \"useful\" for the task at hand than any other definition in the infinite space of all definitions?  If the agent just starts enumerating definitions it will get all sorts of minutia like f(x) = x+(1+x)+1.  We need a way to pair this down to the most \"useful\", \"interesting\", \"fruitful\" ideas.\nOne work which has shown a lot of promise in this direction is DreamCoder, which isn't for proving theorems, but is for writing code.  Not only does it learn to write code with no human code examples, but it does so by building its own library of functions.  It is very careful to only keep the best most useful functions in the library to avoid an overload of choice.  And in doing so it learned to derive the map, fold, and unfold operators from just pure simply typed lambda calculus.  This is pretty remarkable!  However, even in this case DreamCoder was only able to do this since it was given a carefully designed curriculum of problems (given as input-output pairs where the agent was supposed to find a computer function matching that pair).  (In fairness to DreamCoder it used a really small curriculum, but it was a curriculum nonetheless.)\nSo the real question is, can we develop a good notion of \"interesting\", \"useful\", \"fruitful\" mathematics allowing us to start from \"nothing\" and make headway in mathematics by building our own curriculum of mathematics getting us closer and closer to Euclid's theorem?  And if we can do it, is it worth the effort?", "meta": {"post_id": 1403, "input_score": 10, "output_score": 10, "post_title": "auto-generating the proof of infinitude of primes"}}
{"input": "In Lean, some definitions must be marked as noncomputable, for example if they depend on the law of the excluded middle or other nonconstructive choice principles.  Usually, the reason for noncomputability is obvious.  Occasionally, however, Lean will tell me that something must be marked as noncomputable even though it seems perfectly constructive to me.  The example that I am facing at the moment involves long and complicated definitions spread over a number of files, so I cannot give a self-contained explanation here.  Are there good general methods for diagnosing this kind of issue?", "output": "The purpose of the noncomputability checker is to (try to) determine whether or not the VM compiler will succeed in making executable bytecode, which can be evaluated more efficiently (by #eval for instance). One thing to keep in mind is that \"computability\" is not a property that's verified by the Lean kernel, so if you're not planning on evaluating your definitions, the easiest thing to do is to just put noncomputable theory at the top of your modules :-). Lean will still compile definitions that pass the noncomputability checker when in this mode -- it just won't complain at you if you neglect to annotate noncomputable definitions with noncomputable.\nThat said, there are a few tricks to get things to be computable. Overall, what the noncomputability checker is trying to do is determine whether there is a noncomputable function being used within the computationally relevant part of the definition. The following are computationally irrelevant: anything that is Prop-valued (that includes set), anything that is Type-valued, proofs, and the parameters/indices that are the first arguments to a recursor or a constructor. (Note that a definition can depend on the law of the excluded middle and still be computable -- the way it usually would be noncomputable is if you're depending on the classical decidable instance for em in the computationally relevant part of the definition.)\nSometimes there are functions that use noncomputable arguments in a computationally irrelevant way. You can try using the @[inline] attribute for this since the noncomputability checker will inline definitions with this attribute while checking.\nSometimes it's that you're accidentally using a theorem/lemma instead of a def. Only def's are compiled, so technically theorems are noncomputable (in the next release of Lean 3 there will be a more user-friendly error message for this).\nSometimes it's that you're using a noncomputable instance somewhere. If you can figure out what it is, you can write a computable instance for the given types. The @[inline] trick can also be useful here if those instances aren't used in a computationally relevant way. (This shows up in mathlib for some constructors for homomorphisms, though while these constructors are computable, they're not really usefully computable.)", "meta": {"post_id": 1407, "input_score": 5, "output_score": 9, "post_title": "Lean: dubious noncomputability"}}
{"input": "Some time ago I asked this question on Stack Overflow but got no answer:\nhttps://stackoverflow.com/questions/60521384/tracing-tactics-in-isabelle\nSection 9.4 The Classical Reasoner of the Isar Reference Manual writes:\n\nThe tactics can be traced, and their components can be called\ndirectly; in this manner, any proof can be viewed interactively.\n\nI have found sections in this manual about tracing the simplifier and higher order unification.\nHow can I trace the classical reasoner?", "output": "I recall there is no end-user method to trace the classical reasoner, but there is a diagnosis configuration.\nYou should add a line setup \"Config.put_global Blast.trace true\" and further uses of blast will output the a (rather verbose) trace-like report.\nI believe that you can combine this with a judicious use of declare [[blast_depth_limit=n]] with an appropriate non-negative n to  dissect the output.", "meta": {"post_id": 1410, "input_score": 7, "output_score": 8, "post_title": "Tracing the classical reasoner in Isabelle"}}
{"input": "Coq\u2019s built-in termination checker accepts some rather intricate recursion patterns with functional values in data types, as shown by this example\nInductive Z_inf_branch_tree : Set :=\n  Z_inf_leaf : Z_inf_branch_tree\n| Z_inf_node : Z\u2192(nat\u2192Z_inf_branch_tree)\u2192Z_inf_branch_tree.\n\nFixpoint n_sum_all_values (n:nat)(t:Z_inf_branch_tree){struct t} : Z :=\n(match t with\n| Z_inf_leaf \u21d2 0\n| Z_inf_node v f \u21d2 v + sum_f n (fun x:nat \u21d2 n_sum_all_values n (f x)) end ).\n\n(taken from from Chapter 7.3.5.2 here https://www.labri.fr/perso/casteran/CoqArt/coqartF.pdf).\nI am looking for a formal description of the termination checker at work here, in order to reimplement it in a different system? Is it written down somewhere accessible?", "output": "Lennard G\u00e4her has implemented Coq's guard checker in Coq on top of MetaCoq as part of a lecture project I advised here:\nhttps://github.com/lgaeher/metacoq/blob/guarded/guarded/guardchecker.v\nIt has lots of comments and examples is the most accessible description I know of, the other two existing descriptions I am aware of being the comments in the OCaml code of Coq's implementation and this slide set by Bruno Barras, both of which served as sources for Lennard's code.\nLennard's code turns Coq's guardedness checker off to avoid making some functions structurally recursive, and thus there are no proofs about it. In principle it should however be possible to turn everything into \"proper\" Coq functions with some work and then prove the properties postulated in MetaCoq here.", "meta": {"post_id": 1428, "input_score": 6, "output_score": 8, "post_title": "Formal description of Coq\u2019s termination checker"}}
{"input": "I am a high school senior going into college and I am applying to a scholarship in which I must write an essay about a potential future technology that would dramatically impact humans. I immediately thought of proof assistants, which already exist, but I'm sure there are some limitations that can be overcome. I don't really know much about proof assistants, so I would like to learn more. I feel as though they might help to prove significant theorems that we currently have no answer to (if not already) that can benefit humanity massively.\nIf you could respond or link me to a place where I could learn more about the current state of proof assistants, I would greatly appreciate it. Here are some important questions I have:\n\nWhat types of theorems can proof assistants solve, and what types can they not at the present moment?\nHow can they be improved?\nWhat avoidable and unavoidable limitations are there?\nIs there anything else I should know about the current and potential future states of proof assistants?", "output": "These are broad questions and I doubt you will get a complete answer in a single stack exchange answer.  But since you seem ernest and interested, let me at least try to help clarify some of your questions.\n\nWhat types of theorems can proof assistants solve, and what types can they not at the present moment?\n\nFirst, to be clear, the primary purpose of proof assistants isn't to solve new theorems in mathematics automatically.  It is to help a human write proofs in an air-tight form.  Think of them as a programing language but instead of writing code, you write a proof---and instead of compiling and running the code, the computer checks that the proof is correct.\nThe three primary use cases of proof assistants are:\n\nTo prove the correctness of computer code and hardware.  Unlike mathematical proofs, these proofs are very tedious and a small missing case could mean the difference between correct code and a costly bug.  Proof assistants help to do this tedious work and not miss edge cases.\nTo build a library of formal mathematics.  By formalizing all of mathematics we can begin to make mathematics more accessible to computers and unify all of mathematics into a few curated digital libraries of mathematics.\nTo explore new types of logical systems.  New logics can be unintuitive and formal systems can help work with them.  Conversely, new foundational systems may be better suited for formalizing mathematics on a computer.\n\nA related technology called automated theorem provers are used to prove theorems automatically.  SAT solvers, SMT solvers, and resolution theorem provers are some examples.  They are often used for industrial applications, but they also have been used to solve certain combinatorial theorems.\nHaving said all this, there is a continuum between proof assistants and automated theorem provers.  More proof assistants have some levels of automation (some using full powered automated theorem provers) which fill in \"obvious proof steps\". Further a lot of current work in AI theorem proving uses data from proof assistant libraries to train AI models.\nNow, if I misunderstood your question and you meant what types of theorems can a proof assistant be used for formalize, here is the answer: In principle, all correct theorems should be formalizable since they follow from the axioms of mathematics.  In practice, some types of mathematics are easier than others.  Obviously, theorems like Fermat's Last Theorem which both depend on a lot of other kinds of mathematics and are large proofs in themselves, would be a lot of work to formalize.  It is better to build a library of mathematics slowly adding in more and more until you have the theorems and tools you need to prove modern theorems.\nAlso, some subjects like calculus and statistics are import so we teach them to college freshman, but their foundations are actually quite complicated making them difficult to formalize.\n\nHow can they be improved?\n\nUsability is a big concern.  Proof assistants are still hard to work with.  Formalizing is slow and it is a specialized skill.  Adding the state of art in program language design and interactive development environment tools would help.\nMathematics in a proof assistant is also very different looking than informal mathematics.  Bridging this gap would go a long way to speeding up formal proofs.\nBetter automation might help.  Then the user could have the computer fill in the obvious and not-so-obvious steps.  A dream would even have a computer system which could auto-formalize mathematics from say an informal LaTeX proof to a formal proof.  This is incredible challenging, but an exciting research goal.\nProof repair is an interesting topic.  Proofs break if you change the theorem you are proving.  This is especially common when proving the correctness of a piece of computer code which is subject to change.  But in many cases, it should be straight forward to repair the proof, and the computer in principle should be able to do it for you.\nThere are many proof assistants, but they are incompatible with each other.  A proof in one proof assistant isn't usable by any other proof assistants.  Better interchangeability would go a long way.\nThere is also the problem that formalization is not well rewarded in academics or industry.\n\nWhat avoidable and unavoidable limitations are there?\n\nI think I mentioned many avoidable limitations.\nAs far as unavoidable ones, baring human-level artificial intelligence, it is likely going to continue to be a somewhat tedious human process which gets progressively easier with better tools.\n\nIs there anything else I should know about the current and potential future states of proof assistants?\n\nProbably.  But at the same time, just get started playing with proof assistants if they interest you.  They are video-game level addictive!  I recommend the natural number game as a fun starting point.  After doing that, pick a proof assistant that interests you, get to know the community, and start formalizing stuff!", "meta": {"post_id": 1456, "input_score": 7, "output_score": 13, "post_title": "What is the current state of proof assistants?"}}
{"input": "What is a weak function space and what does it have to do with higher order abstract syntax?\nI mean I know what a weak function space is. It's that thing you use for HOAS in Lambda Prolog or toolkits like Beluga!\nBut what are the technical details here?\nIn category theory functions are abstracted away to exponential objects of closed Cartesian categories (not the same as a Cartesian closed category, STLC with product types is Cartesian closed, STLC without product types is closed Cartesian) and residuals/internal homs of closed monoidal categories.\nI have a hunch weak function spaces don't really work like exponentials at all.", "output": "In category theory the adjective \u201cweak\u201d is used when the uniqueness parts of a universal property is removed.\nFor example, a weak product of $A$ and $B$ is an object $P$ with morphisms $p_1 : P \\to A$ to $p_2 : P \\to B$, such that, for all $f : C \\to A$ and $g : C \\to B$ there exists (not necessarily unique!) $h : C \\to P$ satisfying $p_1 \\circ h = f$ and $p_2 \\circ h = g$.\nWe may apply the same idea to exponentials. A weak exponential of $A$ and $B$ is an object $E$ with a morphism $e : E \\times A \\to B$ such that, for all $f : C \\times A \\to B$ there exists (not necessarily unique!) $h : C \\to E$ such that $e \\circ (h \\times \\mathrm{id}_A) = h$. This is just the usual universal property of exponentials, with the uniqueness part removed.\nAn example of a category which has weak exponentials but no exponentials is the category of topological spaces and continuous maps. In more syntactic settings, such as logical frameworks, removing the uniqueness part from the definition of exponentials corresponds to removing the $\\eta$-rule. Consequently, given a term $z : C, x : A \\vdash f : B$, there may be many terms $z : C \\vdash h : A \\to B$ satisfying $$z : C, x : A \\vdash f \\equiv h\\, x : B.$$\nTo see that the $\\eta$-rule gives uniqueness, given $h_1$ and $h_2$ satisfying the above equation, observe that $$h_1 \\equiv (\\lambda x \\,. h_1 \\, x) \\equiv (\\lambda x \\,.f) \\equiv (\\lambda x \\,. h_2 \\, x) \\equiv h_2.$$\nConversely, from uniqueness we get $\\eta$-rule as follows: given any $z : C \\vdash h : A \\to B$, observe that\n$$z : C, x : A \\vdash h\\,x \\equiv h\\,x : B$$\nand by $\\beta$-reduction\n$$z : C, x : A \\vdash h\\,x \\equiv (\\lambda y \\,. h\\,y) \\, x : B$$\ntherefore by uniqueness $z : C \\vdash h \\equiv (\\lambda y \\,. h\\,y) : A \\to B$.\nAs an exercise you can verify that weakness of products correspond to absence of the $\\eta$-rule $(\\pi_1\\,t, \\pi_2\\,t) \\equiv t : A \\times B$.\nLet us also try to answer the question \u201cwhat does it have to do with HOAS?\u201c Recall that in HOAS we use the meta-level function space to model binding in the object-level terms. Does the meta-level $\\eta$-rule transfer to the object-level $\\eta$-rule? For that we would need something like (I am using $\\mathtt{typeface}$ to denote object-level syntactic constructs):\n$$\\mathtt{lambda}(\\lambda x \\,. \\mathtt{app}(h,x)) \\equiv h,$$\nwhich is not the case because \u201cobject-level application $\\mathtt{app}$ is not meta-level application\u201c. So in HOAS the object-level $\\eta$-rule is not available, hence \u201cweak\u201d exponentials.\nIncidentally, the meta-level $\\beta$-reduction fails to transfer to object-level $\\beta$-reduction also, because the object-level $\\beta$-reduction is something like\n$$\\mathtt{app}(\\mathtt{lambda}(t), u) \\equiv t \\, u,$$\nand once again the equation does not hold. Of course, one could consider HOAS with object-level equations, in which case both the $\\beta$-reduction and the $\\eta$-rule could be imposed.", "meta": {"post_id": 1491, "input_score": 5, "output_score": 8, "post_title": "What is a weak function space and what does it have to do with HOAS?"}}
{"input": "I am a little aware of various attempts to axiomize set theory within a theorem prover. Is there a standard kind of encoding of sets? An organic model to interpret set theories into? I would like to be sure my axioms have reasonable interpretations.\nSomething like this page on pure sets on nlab seems close but also seems a bit hairy.\nIt feels like these sort of encodings would be easiest with some hairy uses of functional or propositional extensionality which is mildly inconvenient.\nYou also have classical or choice principle issues. Something like CZF feels like it would be easiest to model in type theory (but hairier to axiomize.)\nFrom what I grasp of the pure set page you want something like\nInductive pregraph :=\n| sup X (f: X -> pregraph).\n\nFixpoint monic p :=\n  let 'sup X f := p in\n  f x == f y -> x = y\n  /\\ forall x, monic (f x).\n\nExisting Class monic.\n\nDefinition graph := { p: graph | monic p }.\n\nDefinition member P '(sup X ps) :=\n exists x. ps x == P.\n\nBut you need to define equivalence of pregraphs or assume some flavor of extensionality. Then you need to carve out well-founded sets. I feel like there ought to be an easier way.\nMaybe it would be nicer to first model proper classes and then model pure sets as classes which are members of other classes?", "output": "Depends on which type theory :-)\nZF set theory has been formalised in Isabelle as Isabelle/ZF by directly assuming the ZF axioms (and optionally AC). For those who prefer higher-order logic (also known as simple type theory), it's been done many times.\nDetails here and an application here. See also an impressive application of Isabelle/ZF.", "meta": {"post_id": 1525, "input_score": 4, "output_score": 10, "post_title": "Is there a \"standard\" encoding or model of material set theory in type theory?"}}
{"input": "My understanding of Coq is that Prop : Type_1, Set : Type_1, and then Type_1 : Type_2, Type_2 : Type_3, etc.\nSo, at the bottom level, the world splits into two universes: Prop (impredicative) and Set (predicative, like all the other Type_i universes). My question is: why not just have Prop : Type_0 : Type_1 : Type_2 : ..., with Prop being impredicative as it is today, and all the other levels would be predicative, also as is the case today? Wouldn't that be simpler? Would cumulatively still work?", "output": "As it often happens with Coq, the answer is historical reasons.\nIn the original version dating back from 1984, Coq was based on the Calculus of Constructions, a barebone dependent type theory. In particular, it did not feature inductive types. Instead, following the PTS tradition, it had impredicative universes.\nThe introduction of inductive types did not change this state of affair, and for a long time Coq had two impredicative universes, Prop and Set. The difference between both lied in the fact that the former was erased by extraction, but not the latter. This phase separation had been around for a long time already.\nWhen dealing with several impredicative universes, one has to be extremely careful because proofs of False lurk around the corner. In particular, Prop : Set when both are impredicative is enough be a variant of Girard's system U\u207b  and thus inconsistent. The Coq developers of yore were well aware of this issue and relied on an alternative hierarchy, so that Prop : Type and Set : Type.\nNow, at some point it was decided to make Set predicative by default, for other somewhat related reasons. Indeed, impredicative proof-relevant universes are very much inconsistent with many slightly classical principles like excluded middle in Type or some forms of choice. In particular and ironically, with an impredicative Set, Coq has no set-theoretical model (as in Polymorphism is not set-theoretic). Nowadays, Set is thus predicative except if the user opts in impredicativity with a specific flag.\nImpredicative set is essentially not used as of today, and really not tested so it has fallen to bitrot. It is extremely likely that it is not usable anymore, and there is an evergreen discussion about its removal. Yet, since the flag still exists Coq needs to at least pretend to avoid inconsistencies when the user sets it, and therefore one cannot have Prop : Set.\nIf the flag ever gets removed, maybe we can consider adding this rule, but then there might be weird backwards incompatibilities. Therefore, it is not even clear we will ever perform this change even in a situation where it is perfectly sound.", "meta": {"post_id": 1551, "input_score": 16, "output_score": 21, "post_title": "Why not have `Prop : Set` in Coq?"}}
{"input": "What are all the differences between these keywords that allow for defining top level variables?\nWhat I have noticed so far is that theorems can't be anonymous \u2014 you can write example : A := B but not theorem : A := B.", "output": "An outline of the differences in Lean 4, which you mentioned is the version you are interested in:\n\nA def is the primary way to define a named function / value in Lean. Unless its result type is Prop or it is marked noncomputable, a def will be compiled into Lean's intermediate representation (IR) in preparation for execution by Lean's interpreter and/or for further compilation into C code.\n\ndef add (x y : Nat) : Nat := x + y -- a simple computation\ndef one_plus_one_eq_two : 1 + 1 = 2 := rfl -- theorems can be defs too!\n\n\nA theorem is essentially a noncomputable def. This means that values defined by a theorem cannot be used in ordinary computation as they will not (and maybe cannot) be compiled down into executable code. Note that Lean erases types (eg., T : Type u, Prop : Type), propositions (e.g., 1 = 1 : Prop) and proofs (e.g., rfl : 1 = 1) -- all of which are technically noncomputable -- before attempting compilation, which is why they are still safe to use in computable code. Such values are termed irrelevant by the Lean compiler IR.\n\ntheorem foo : Nat := 5\ndef addFoo (x : Nat) := x + foo \n/-\nerror: \nfailed to compile definition, consider marking it as 'noncomputable' \nbecause it depends on 'foo', and it does not have executable code\n-/\n#eval foo -- same error\ntheorem addFoo' (x : Nat) : Nat := x + foo -- ok\n\n\nAn example creates an anonymous def that is type checked for correctness and then immediately discarded by Lean. Thus, an example has no lasting effect on the environment and cannot be referred to by later declarations. Note that while Lean will end up discarding the code it generates from an example, it does still, by default, generate code. Thus, you must mark an example noncomputable if it uses a noncomputable definition for a (i.e., to define a value that is not erased).\n\nexample : 1 + 1 = 2 := rfl\nexample (x y : Nat) : Nat := x + y\nexample (x : Nat) : Nat := x + foo -- same error as above\nnoncomputable example (x : Nat) : Nat := x + foo -- ok\n\nNote that there are other keywords for defining functions / values (e.g., axiom, opaque, abbrev, etc.) which I have not discussed here as you did not ask about them. Furthermore, given your choice of keywords to compare, it seemed best to focus on the named/anonymous, computable/noncomputable distinctions that separate def, theorem, and example.", "meta": {"post_id": 1575, "input_score": 8, "output_score": 8, "post_title": "What are the differences between theorem, example, def, etc?"}}
{"input": "I noticed that in Lean, the localization of rings (which is unique up to isomorphism) is defined as a predicate is_localization.\nI am not an expert in Lean, and I'm not aware of much of the discussion in the Lean community. But this has led me to wonder: Since such an approach has many benefits, why don't we use that everywhere? Is it possible to define and use is_nat, is_tensor_product, etc? If not, what are the downsides that prevent this from happening?\nOne thing that I can think of is that Lean has built in support for inductive types, so defining natural numbers or cartesian products in this way wouldn't be worth it, since all the support (pattern matching etc.) will be lost.\nAs a tangential question, how well does this approach scale with other proof assistants such as Coq and Agda?", "output": "The question is about declarative versus computational formalization.\nIn the declarative style we characterize a \u201cthing\u201d by stating conditions,\nquite often universal properties, that determine it up to a unique isomorphism. Any \u201cthing\u201d satisfying the stated conditions is acceptable.\nIn the computational style we just construct the \u201cthing\u201d concretely, and use features provided by the proof assistant to compute with it directly.\nA typical example is the structure of natural numbers. In the declarative style we define \u201cpointed successor algebra\u201d and say that $\\mathbb{N}$ is an initial pointed successor algebra. In the computational style we define $\\mathbb{N}$ as an inductive type.\nEach approach has benefits and drawbacks:\n\nThe declarative style is more flexible, but doing everything in this style complicates formalization. For example, one can never refer to \u201cthe natural numbers $\\mathbb{N}$\u201d but must always say \u201cand given an initial pointed successor algebra $N$ ...\" Now imagine you had to do this every time you wanted to mention a cartesian product, a function space, a unit type, etc.\n\nThe computational style allows us to compute more easily (although the declarative one does as well with some extra work), but it locks in specific design choices. You defined natural numbers in unary, but now you'd like to compute in binary? Tough luck, you'll have to get the proof assistant developers to hack the system and provide some magic.\n\n\nPeople tend to use the computational style, but there are situations which demand a different approach because the \u201cunique up to unique isomorphism\u201c cannot be realized as an exact equality. For example, suppose we want to define \u201cthe $n$-th power of $A$\u201d as a type? That's easily done:\nopen import Data.Nat\nopen import Data.Product\nopen import Data.Unit\n\nmodule Cow where\n\n  power : Set \u2192 \u2115 \u2192 Set\n  power A zero = \u22a4\n  power A (suc n) = A \u00d7 power A n\n\nBut what if we want $A^{n + m} = A^n \\times A^m$ to hold as an equality? This might be doable, but is at the very least going to require a lot of trickery and fragile design. And then someone is going to walk in and ask about $A^0 = 1$, and everything will crumble.\nThe Lean library defines is_localization for a similar reason. Because the \u201cequality\u201d $R[1/f][1/g] = R[1/f g]$ (or some such) cannot be achieved using any reasonable concrete construction of $R[1/f]$, we switch to declarative style as we have to deal with isomorphisms anyhow.\nIn my opinion, we are not facing just a formalization engineering problem, but rather a genuinely mathematical one. Many branches of mathematics sweep the problem under the rug by saying things like \u201cwe identify $A^{n + m}$ with $A^n \\times A^m$\u201c \u2013\u00a0but homotopy theorists and higher-category theorists thrive on it. It remains to be seen how their insights can be transformed into formalization techniques. A very good attempt is homotopy type theory and univalent mathematics, which however requires one to pay a high price \u2013\u00a0a new way of thinking \u2013 and is therefore not very popular.", "meta": {"post_id": 1629, "input_score": 6, "output_score": 8, "post_title": "Defining essentially unique objects with typeclasses"}}
{"input": "I understand that we can use W-types to encode natural numbers and a wide variety of other inductive types in intensional MLTT. Can we encode W-types using only natural numbers within type theory, potentially giving up some of the judgmental equalities, but keeping the propositional equality the same? I always thought it was the case based on this quote from nLab W-type page:\n\"In a topos with a natural numbers object (NNO), W-types for any polynomial endofunctor can be constructed as certain sets of well-founded trees; thus every topos with a NNO is a \u03a0W-pretopos. This applies in particular in the topos Set (unless one is a predicativist, in which case Set is not a topos and W-types in it must be postulated explicitly).\"\nBut I am unsure if this is a correct interpretation of the categorical concepts involved. I think the culprit here is the requirement of a topos with NNO, not just a locally cartesian closed category.\nIs this the case? Can we construct W-types from natural numbers?\nEdit: I have found a construction of W-types from natural numbers in presence of propositional resizing in Strictly Positive Types in\nHomotopy Type Theory, so I think I should specify that I want to keep the theory as predicative as possible and hopefully free of uses of K or univalence.\nEdit 2: To specify what I mean by intensional MLTT, it is the rules for $\\Pi$, $\\Sigma$, $\\mathbb{N}$, Id, 0, 1, 2, plus an infinite predicative hierarchy of universes. $U_0 : U_1$, $U_1 : U_2$ etc.", "output": "The answer seems to be no. According to Anton Setzer's PhD thesis: Proof theoretical strength of Martin-L\u00f6f Type Theory with W-type and one universe:\n\nAczel has shown in [Acz77] that Martin-L\u00f6f\u2019s type theory with one universe but no W-type has strength $| \\widehat{ID}_1 | = \\phi_{\\epsilon_0}0$.\n\nHowever:\n\nThe strength we prove, is in fact far bigger [than that shown by Palmgren], it is slightly bigger than the strength of $| KP_i|$. Until recently (work of Rathjen, [Rat90], [Rat91a], [Rat92],\nsee also [Buc93], [Sch91a], [Sch91b], [Sch92b]), $KP_i$ has been essentially the strongest theory, for which proof theoretical analysis could be carried out. For the author, $|KP_i|$ is an ordinal which seems to be an ordinal of significance similar to that of $\\Gamma_0$\n\nFurthermore:\n\nPrecisely we calculate the proof theoretical strength of intensional Russel, extensional, Tarski, and extensional Russel-version of Martin-L\u00f6f\u2019s type theory with one Universe and the W-type, $ML^i_1W_R$ , $ML^e_1W_T$ and $ML^e_1 W_R$, namely $$|ML^i_1W_R| =|ML^e_1W_T|=|ML^e_1 W_R|= |\\bigcup_{n\\in{\\mathbb N}}KPi_n^+| = \\psi_{\\Omega_1}\\Omega_{I+\\omega}.$$\n\nThis sorts out the proof theoretical strength for MLTT with a single universe.\nNow, according to Michael Rathjen's paper The strength of Martin-L\u00f6f type theory with a superuniverse (which is at least stronger than MLTT with an infinite hierarchy of universes):\n\nthe proof-theoretic ordinal of $MLS$ is $\\Phi_{\\Gamma_0}0$\n\nSo according to the wikipedia page on ordinal analysis, as pointed out by Dan Doel, which seems to suggest that\n$$\\phi_{\\epsilon_0}0 < \\Gamma_0 < \\Phi_{\\Gamma_0}0 < \\psi_{\\Omega_1}\\Omega_{I+\\omega}$$\nSo it seems that $|ML_\\omega| < |ML_1W|$ thus it cannot be possible to encode W-types in $ML_\\omega$.", "meta": {"post_id": 1674, "input_score": 7, "output_score": 12, "post_title": "Can you build W-types out of natural numbers predicatively?"}}
{"input": "all,\nI have a project in Lean which turns out to involve some features which might better be called programming. So, for that part of the project, I was thinking I would treat Lean like it is Python in a few ways. For this I was hoping to have dynamic data types. Could anyone help me by writing out the following examples, translating from pseudocode to Lean? The more explicit the better.\nThanks very much!\nINT: Dynamic natural number type\n\nINT a = 1\na = a + 1\na = 3\n\nSTRING: a dynamic string type\n\nSTRING s = \"a\"\ns = \"b\" %Now s is set to \"b\"\n\nLIST[X]: a dynamic list of type X\n\nl : LIST[STRING] = [\"a\",\"b\", \"c\"]\nl[0] = \"k\" %resets \"a\" to \"k\". The new list is [\"k\", \"b\", \"c\"]\nl.delete(1) %deletes \"b\". The new list is [\"k\", \"c\"]\nl.add(\"b\", 2) %new list is [\"k\", \"c\", \"b\"]\n\nSTRUCTURE: a dynamic class-ish type\n\nCan introduce any number of pieces of associated types like strings, lists of strings, etc.\nCan be initiated by specifying associated types\nSome associated types are default\nAccessing and modifying associated types\nDemanding proofs of various properties during initiation\n\nSTRUCTURE whose constituents have a product\n\nS : STRUCTURE\ns1 : S\ns2 : S\na : s1\nb : s2\n#check (a, b)\n\ns1 x s2\n\na : s1 x s2\n#check pi1(a)\n\ns1\n\n#check pi2(a)\n\ns2\nThanks very much for any help!", "output": "First there is no reason to put quotes around \"programming\".  Lean 4 is a fully capable programming language with all the needed features.  But it is also a (very pure) functional programming language and your question conveys some confusion both of how functional languages work and even of how Python works.\nHere is a crash course focusing on your questions.\nMutability\nIn Python, many types (e.g. int, str, tuple) are immutable, meaning you can never change an object of that type.  In Python, when you do\nx = \"Hello World!\"\nx = x.replace(\"Hello\", \"Goodbye\")\n\nyou are not changing the string \"Hello World!\", but instead creating a new string in the second line and pointing that variable x to the new string.  If the previous string is no longer referenced (as in this example), then the Python garbage collector cleans it up.  (C and Rust are different in this regard, but we don't need to worry about that.)  Lean is very similar to Python in this regard.\ndef goodbye_world : String :=\n  let x := \"Hello World!\" \n  let x := x.replace \"Hello\" \"Goodbye\" -- a new string with the same variable name as before\n  x                                    -- the value at the end of a function is what is returned\n\n#eval goodbye_world  -- Goodbye World!\n\nLists and persistent data structures\nIn Python some types are mutable (changable) like list\nx = [0, 1, 2]\ny = x        # x and y point to the same list\nx.append(3)  # change the list\nx[0] = 4     # change the list\nprint(y)     # [4, 0, 1, 2]\n\nSince in pure functional programming all types are immutable, one has to think differently.  Consider using the immutable tuple in Python instead of list:\nx = (0, 1, 2)\ny = x             # x and y point to the same tuple, but not for long\nx = x + (3,)      # x is a new tuple (0, 1, 2, 3)\nx = (0,) + x[1:]  # x is a new tuple (4, 1, 2, 3)\nprint(y)          # y never changed: (0, 1, 2)\n\nThis isn't really a problem for simple cases, but if the tuple gets too big it can lead to a lot of copying of data.\nBecause immutable structures are all there are in pure functional programming, functional programming tends to adopt persistent data structures.  These are immutable data structures where you can change a little part without having to make a copy of the whole thing.  Instead of having a list be a single block of memory, it is made up of a series of pointers (basically a linked list).  So if you change one part of the list you don't have to copy a giant block of data.  Here is a hacky version in Python, but the Lean version is cleaner:\nx = (1, (2, (3, None)))  # linked list\nhead, tail = x\ny = (4, tail)  # (4, (2, (3, None)))\n\nI've changed the first element of the linked list, but I've done so without copying the whole object.  x[1] and y[1] point to the same object in memory.  Lean's List type is built this way:\ndef example_lists : List Nat \u00d7 List Nat :=\n  let x := [0, 1, 2]\n  let y := 4 :: x\n  (x, y)   \n\n#eval example_lists  -- ([0, 1, 2], [4, 0, 1, 2])\n\nNow there is a further complexity with List in Lean.  Usually in Lean you want your functions to handle all possible cases.  So one can't just pop the head of a list since the list might be empty.  Lean4 has many ways to deal with this:\ndef l := [1, 2, 3]\ndef e := ([] : List Nat)\n\n#eval l.head (by simp)  -- 1  (Must supply a proof that l \u2260 [].)\n#eval e.head _   -- No proof for _ would make this compile.\n#eval l.head?    -- some 1  (Answer in an Option.)\n#eval e.head?    -- none    (Answer in an Option.)\n#eval l.head!    -- 1\n#eval e.head!    -- 0  (Returns default value for Nat type in #eval.  \n                 --     If run inside `main : IO unit` will give runtime error.)\n#eval l.headD 0  -- 1\n#eval e.headD 0  -- 0  (We supplied a default value.)\n\nList.head and List.head? are the most principled approaches.  List.head! is fine for simple one-off applications but your program may crash if you make a mistake.  List.headD in my opinion is more likely to lead to subtle bugs than be helpful.\nFor more list operations, see the Lean 4 source code.\nSo we can do:\ndef example_list : List Nat :=\n  let y := [0, 1, 2]\n  let y := 4 :: y.tail! -- add 4 to the front of y\n  let y := y ++ y  -- concatenate y with itself\n  y\n\n#eval example_list -- [4, 1, 2, 4, 1, 2]\n\nNote, List is meant to be used as a stack and you will get better performance operating on the left of a List than the right side.\nStructures\nLike Lists, Lean (and most other functional languages) make it easy to work with structures in a persistent way.  Often you want to change just one element of a structure.  To do that you need to make a copy of the structure and change that element.  It would be a pain to have to re-enter all the previous information again each time.  In Lean, you just have to specify which fields are changing.\nstructure Pair where\n  x : Nat\n  y : Nat\n\ndef origin : Pair := {x := 0, y := 0}\n\n#reduce origin                -- { x := 0, y := 0 }\n#reduce {origin with x := 1}  -- { x := 1, y := 0 }\n#reduce {origin with y := 1}  -- { x := 0, y := 1 }\n\nPolymorphism\nSince Lean is statically typed it has to know the type of every variable at compile time.  Nonetheless (like almost all statically typed functional programming languages) a type in Lean can depend on another type.  For example, List depends on the type of the elements in the list.  Since List (and every other type in Lean) is immutable, your can't change it, but you can make a new list with the same variable which has a different type.  This is a common use case for List.map:\ndef my_list_of_strings : List String :=\n  let x : List Nat := [0, 1, 2, 3] -- List Nat\n  let x : List Int := x.map (fun y : Nat => -y)\n  let x : List String := x.map (fun y : Int => y.repr)\n  x\n\n#eval my_list_of_strings  -- [\"0\", \"-1\", \"-2\", \"-3\"]\n\nStructures also allow polymorphism in two ways shown here:\nstructure LabeledPoints (CarrierType : Type) where\n  LabelType : Type\n  label : CarrierType -> LabelType\n\ndef my_labeled_nat : LabeledPoints Nat := { \n  LabelType := String,\n  label := fun x => x.repr\n}\n\n#check my_labeled_nat -- LabeledPoints Nat\n#eval my_labeled_nat.label 0 -- \"0\"\n\nWhether to put the type inside (e.g. LabelType) or outside (e.g. CarrierType) is largely dependent on context and use case.  (See the discussion on bundled structures.)\nType Classes\nIn a polymorphic setting everything you do must work for all possible types.  So I can make the following theorem.\ntheorem LabeledPoints.has_label\n  (T : Type) \n  (labeled_points : LabeledPoints T) \n  (t : T): \\exists l : labeled_points.LabelType, labeled_points.label = l :=\n\nand because I put it in the LabeledPoints namespace, I can use it with \"dot notation\" as follows:\n#check my_labeled_nat.has_label 0  -- Prop\n\nBut I can't construct this next function since not all types T have a string representation (like Nat did in the example above).\ndef mk_repr_labeled_points (T : Type) : LabeledPoints T := { \n  LabelType := String,\n  label := fun x => x.repr  -- this won't work since not all types T have T.repr\n}\n\nInstead, to do this, one needs type classes.  Type classes are a way that Lean (and other functional programming languages) let you define functions, structures, and other types which only depend on a certain class of types instead of all possible types.  In this next example, the type class parameter [Repr T] expresses that T must have a representative string function (which can be accessed with reprStr).\ndef mk_repr_labeled_points (T : Type) [Repr T]: LabeledPoints T := { \n  LabelType := String,\n  label := reprStr\n}\n\n#check mk_repr_labeled_points True  -- fails to typecheck\n#check mk_repr_labeled_points Int   -- LabeledPoints Int\n#eval (mk_repr_labeled_points Int).label (-1 : Int) -- \"-1\"\n\nLoops\nThe examples so far are kind of silly.  What you often want to do is keep changing a variable in a loop.  In functional programming loops are often done with recursion and you should learn it.  Here are two ways to accomplish the task of adding the squares of every number in a list:\nThis is the classic recursive approach.  (I could also make this tail recursive, but that is too advanced right now for this answer.)\ndef sum_of_squares : List Nat -> Nat\n| []     => 0\n| x :: t => x * x + sum_of_squares t\n\n#eval sum_of_squares [0, 1, 2, 3]  -- 14\n\nIterating over a List is so common, there is a function foldl just for this which makes it a one-liner.\n#eval [0, 1, 2, 3].foldl (fun x partial_sum => x * x + partial_sum) 0\n\nAdvanced Lean feature: do notation\nIf you want to do real programming, you probably want to print to STDOUT and read data from a file or from STDIN.  To do this, you have to use the IO monad.  I don't have room to go into the details, but when you are in a do block of the IO monad it looks even more like procedural programming (e.g. Python).  You can do sum_of_squares like this:\ndef sum_of_squares_io (l : List Nat) : IO Nat := do\n  let mut r := 0\n  for x in l do\n    r := r + x * x\n  return r\n\nBut note that your result will be wrapped in the IO monad.  See The Lean 4 Manual and Functional Programming in Lean for more information.\nAdvanced Lean feature: |>. notation\nInstead of let ... let ... let ... often you just want to chain your functions together into a one-liner.\n#eval \"Hello World!\".replace \"Hello\" \"Goodbye\"\n\nbut it isn't easy to add in another .replace here since .replace has arguments .  For that, Lean added the |>. notation (along with |>, and <|.)\n#eval \"Hello World!\" |>.replace \"Hello\" \"Goodbye\" |>.replace \"!\" \"?\"\n-- Goodbye World?\n\nAdvanced Lean feature: In-place changes\n(I could be a bit mistaken on what I write in this section.)  Unlike most other purely functional languages, Lean has support for non-persistent data structures without breaking the pure functional programming paradigm. For example, if you want to work with an Array or a HashMap, you can.  It works like this:  If you create an object at the same time that you dereference another object of the same Type, the Lean compiler will, instead of allocating new memory, just reuse the old memory.  For objects like Array, instead of making a copy of the array, it will just update the array in-place.  But it can be a bit tricky to get the syntax right (and unlike Rust, the Lean compiler won't help you keep track of references to be sure your only have one pointer to an array).\ndef example_array : Array Nat :=\n  let a := Array.mk [0, 1, 2, 3, 4]\n  let b := a.set! 3 10  -- makes a *copy* of a (since a is still referenced)\n                        -- and modifies element in position 3\n  let b := b.set! 4 11  -- modifies b *in-place*\n\n#eval example_array -- [0, 1, 2, 10, 11]\n\nFinal thoughts\nLearning functional programming is not a one-day activity.  It requires a new way of thinking about coding.  Lean 4 is a wonderful language with great features and syntax choices, but it is still very different from procedural languages like Python.  Expect a learning curve.  For learning more I would recommend:\n\nFunctional Programming in Lean (in progress)\nThe Lean 4 Manual (in progress)\nTheorem Proving in Lean 4 (The most comprehensive Lean 4 manual so far.  While it focuses on theorem proving, the sections on induction and recursion, and on structures would be especially applicable to programming.)\nA good tutorial or course for another functional language, like Haskell or Scala", "meta": {"post_id": 1701, "input_score": -1, "output_score": 10, "post_title": "A project in Lean which involves \"programming\""}}
{"input": "I guess my question can be reduced to implementing this function:\ndef abEq (A B : Type) (a: A) (ab : A = B): B := sorry\n\nI am new to Lean 4 and started learning from the official tutorial, currently I am at the chapter 7 (Inductive types).\nI already know that substituting things in propositions given a certain equality is done using Eq.subst, and Eq.subst ab a is among the things I tried. My problem is that the type assertion a: A is not a proposition, and I have no idea how to proceed.\n\nWhy I am trying to do this\nPerhaps I am suffering from an XY problem, so I thought including the following might help.\nIn Lean, I am trying to define what I would define in set theory like this:\nA signature is a pair $(Op, ar)$, where\n\n$Op$ is an arbitrary set of operations, and\n$ar\\colon Op \\to Nat$ a function giving their arities.\n\nLet $Sig_{Op}$ be the set of all signatures with $Op$ as the set of operations, then\n\\begin{align}\narity\\colon Sig_{Op} &\\to Op \\to Nat \\\\\n(Op, ar) &\\mapsto op \\mapsto ar(op)\n\\end{align}\nreturns the arity of the operator $op$ in the signature $(Op, ar)$.\nI came up with this:\ndef signature := (Op: Type) \u00d7 (Op \u2192 Nat)\n\ndef sigOp (Op: Type) := { s: signature // s.fst = Op }\n\ndef arity {Op: Type} (op: Op) (s: sigOp Op): Nat := ...\n\nbut I am stuck implementing the function arity. s.val.snd op does not work, because s.val.snd expects an argument of type s.val.fst, while op has type Op. The value s carries a proof s.property: s.val.first = Op, but I don't know how to apply it. Is my problem in that I just don't know how to implement arity, or are my Lean definitions themselves wrong and I need to change them? If so, how?", "output": "Indeed, you are experiencing the XY problem. You are not taking the advantage of dependent types, which you can do as shown below.\nRegarding your problem X: one just has to perform a transport across equality, but in a universe:\ndef abEq (A B : Type) (a : A) (ab : A = B): B :=\n  @eq.rec Type A (\u03bb (X : Type), X) a B ab \n\nNamely, suppose we have:\n\na type U\nelement u : U\na type family P : U \u2192 Type\nan element x : P u\nan element v : U\nequality e : u = v\n\nThen eq.rec U u P x v e will produce an element of P v. We say that we transported x : P u to P v along e. If we set U := Type, u := A, P := \u03bb X, X, x := a, v := B and e := ab then we get your example.\nNow let's get onto problem Y.\nI am including a fairly elaborate example of how to go about formalizing universal algebra because the classical treatment of universal algebra is not the right way to formalize the topic, but I see people falling into the trap repeatedly. In particular, one should not use natural numbers as arities \u2013 that creates a host of unecessary complications.\n-- A signature has a type of operation symbols, and each symbol\n-- has an arity. Improtantly, the arity is a type, not a number!\n-- That is, if we want a symbol to have arity 3 we do not use\n-- the number 3 but instead (any) type with 3 elements.\nstructure signature :=\n  (symbol : Type) -- the type of operation symbols\n  (arity : symbol \u2192 Type) -- the arities of symbols\n\nnamespace arity\n  -- we define arities for constants, unary and binary symbols,\n  -- as these are most common.\n\n  inductive const : Type\n\n  inductive unary : Type\n    | u1 : unary -- the index of the only argument\n\n  open unary\n\n  inductive binary : Type\n    | b1 : binary -- the index of the first argument\n    | b2 : binary -- the index of the second argument\n\n  open binary\nend arity\n\n-- the type of algebras for a given signature,\n-- note that each symbol op is interpreted as a map which\n-- takes a function (args : S.arity op \u2192 carrier) and returns\n-- an element of the carrier. If the type S.arity op has\n-- n elements, then args is (equivalent to) an n-tuple,\n-- as it should be.\nstructure algebra (S : signature) :=\n  (carrier : Type) -- the underlying carrier\n  (act : \u03a0 (op : S.symbol), (S.arity op \u2192 carrier) \u2192 carrier)\n\n-- Example: the free algebra generated by a signature S and a set X of generators\n\n-- the carrier of the free algebra is the type of trees whose leaves are\n-- the generators and the nodes are operation symbols\ninductive tree (Leaf : Type) (Node : Type) (arity : Node \u2192 Type) : Type\n| leaf : Leaf \u2192 tree -- the injection of generators into the tree algebra\n| node : \u03a0 (t : Node), (arity t \u2192 tree) \u2192 tree\n\n-- the free algebra on S generated by X\ndef free (S : signature) (X : Type) : algebra S :=\n  { carrier := tree X S.symbol S.arity,\n    act := tree.node }\n\n-- the type of homomorphisms between two S-algebras\nstructure hom {S : signature} (X : algebra S) (Y : algebra S) :=\n  (map : X.carrier \u2192 Y.carrier)\n  (is_hom : \u03a0 (op : S.symbol) (args : S.arity op \u2192 X.carrier),\n              map (X.act op args) = Y.act op (map \u2218 args) )\n\n-- composition of homomorphisms is a homomorphism\ndef hom_compose {S : signature} {X Y Z : algebra S}\n                (g : hom Y Z) (f : hom X Y) : hom X Z :=\n  { map := g.map \u2218 f.map,\n    is_hom := begin intros op args, simp [g.is_hom, f.is_hom] end\n  }\n\n-- example: the signature of a group\n\n-- the type of symbols of a group\ninductive group_symbol : Type\n  | uni : group_symbol -- unit\n  | inv : group_symbol -- inverse\n  | mul : group_symbol -- multiplication\n\nopen group_symbol\n\ndef group : signature :=\n  { symbol := group_symbol,\n    arity := \u03bb op, match op with\n                   | uni := arity.const\n                   | inv := arity.unary\n                   | mul := arity.binary\n                  end }\n\n-- the group Z/3Z\n\n-- the carrier of Z/3Z\ninductive Z3_carrier : Type\n  | z0 : Z3_carrier\n  | z1 : Z3_carrier\n  | z2 : Z3_carrier\n\nopen Z3_carrier\n\n-- the inverse operation in Z/3Z\ndef Z3_inv : Z3_carrier \u2192 Z3_carrier\n  | z0 := z0\n  | z1 := z2\n  | z2 := z1\n\n-- addition in Z/3Z\ndef Z3_add : Z3_carrier \u2192 Z3_carrier \u2192 Z3_carrier\n  | z0 z0 := z0\n  | z0 z1 := z1\n  | z0 z2 := z2\n  | z1 z0 := z1\n  | z1 z1 := z2\n  | z1 z3 := z0\n  | z2 z0 := z2\n  | z2 z1 := z0\n  | z2 z2 := z1\n\n-- the group Z/3Z\ndef Z3 : algebra group :=\n  { carrier := Z3_carrier,\n    act := \u03bb op,\n             match op with\n             | uni := \u03bb args, z0\n             | inv := \u03bb args, Z3_inv (args arity.unary.u1)\n             | mul := \u03bb args, Z3_add (args arity.binary.b1) (args arity.binary.b2)\n             end\n  }", "meta": {"post_id": 1740, "input_score": 11, "output_score": 9, "post_title": "Given `A = B`, how to prove `a: A` also has type B in Lean 4"}}
{"input": "I saw:\n\nsubsingleton elimination from lean-forward, which, I so far understood as \"eliminate a type in Prop to a type in whatever universe that we know has at most one constructor with arguments either in Prop or also subsingletons\".\nfunction comprehension from XTT, $\\S 8.2.1$, which corresponds to the statement that (paraphrased) if \u2200 (a : A), \u2203 (b : B), R a b \u2227 b unique, then \u2203 f : A -> B, it holds that \u2200 (x : A), R x (f x). If A lives in Type then this is trivial, but when A : Prop then this becomes more interesting: we intend to erase propositions, so we cannot have their computational content relevant, so only when we know b unique do we say \u2200 x, f x = b.\n\nI wonder are these concepts the same thing? Is there any slight difference I didn't notice? They're both about eliminating Prop into some uniquely inhabited Type.", "output": "Indeed, these two notions are about what we allow to eliminate from Prop to Type, and they both roughly follow the idea that \"using propositions to build relevant content is fine as long as there is but one way to use it\". But they are quite different.\nSubsingleton elimination (in the way this term is usually used) is a syntactic/structural criterion on inductive types, that tells you for which such inductive types pattern-matching/eliminators is allowed with a return type in Type. This criteria is designed so that there is at most one canonical form of this type, so that there is definitionally at most one inhabitant in the empty context (if canonicity holds).\nFunction comprehension, on the other hard, is a semantic criterion, which allows to construct a relevant function out of irrelevant content, provided one can show internally that there is but one possible such function. The paradigmatic example of this idea is what the HoTT Book calls principle of unique choice (section 3.9, where they also mention what you call function comprehension as corollary).\nThe important difference, as highlighted, is that the first notion is syntactic, while the second is semantic. This means that subsingleton elimination is necessarily more restrictive than unique choice/function comprehension, but it has the advantage that it can be checked mechanically, and that in practice it is nice to avoid the whole going-through-unique-choice yoga when you want to eliminate a Prop inductive into Type. Moreover, as pointed out by the people working on strict propositions (see my answer to a previous question), the subsingleton criterion might need to be refined/amended depending on the exact kind of propositions you want.", "meta": {"post_id": 1741, "input_score": 5, "output_score": 9, "post_title": "Is 'subsingleton elimination' the same concept as 'function comprehension'?"}}
{"input": "How can we define in a proof assistant (eg., Coq) a notion of a 2-player game, where each player is a program that has access to the opponent's source code?\nBackground: In Open Source Game Theory, we think about bots that reason about each other (prove theorems), knowing the other's source code. I would like implement such bots in a proof assistant, if possible. (https://arxiv.org/pdf/2208.07006.pdf, Open Problem 4)\nWhat I have in mind would looks like this:\nInductive outcome := Cooperate | Defect.\n(*  Dupoc is short for Defect Unless Proof of Cooperation   *)\nDefinition Dupoc (n:N) (bot2 : N -> bot -> outcome) : N -> bot -> outcome :=\n    'try for n steps to find a proof that bot2 n self = Cooperate, if found':\n        return Cooperate\n    else\n        return Defect.\n\nThe point is that we want to run the agents together with each other as input:\nDefinition game n1 bot1 n2 bot2 := (bot1 n1 bot2, bo2 n2 bo1).\n\nOf course, the type bot does not exist as I have written it. Is there a way to actually make this happen?\nThe other problem is that the bot's function is trying to find a proof and returning a bool that reflects the success. Is this possible in some proof assistant? The proof assistant would have to have access to its own inner workings somehow, which seems hard.\nI think I could solve the problem by having a Coq implementation in Coq.\nThen I could have, like in the paper:\nInductive CoqTokens := ...\nDefinition BotExpression := ... (* A valid expression for a Bot *)\nClass Bot := {\n    play : BotExpression -> outcome,\n    source : BotExpression (* This should be the source code for 'play' in our Coq-in-Coq implementation *)\n}.\nDefinition game bot1 bot2 := (bot1.play bot2.source, bot2.play bot1.source).\n\nBut this seems very much work if it is possible. The Coq implementation in Coq still needs to contain everything about proofing. And I am not sure if it actually solves the problem.", "output": "As you found out, the usual technique to solve this kind of difficulty is to introduce a notion of \"code\" (what you called BotExpression), which expresses the behaviour of your bot. Once you are happy with your codes, you can in a second time define a \"decoding\" function, something which takes in a code and spits out the expression, but this time as an actual Coq function. Taking the notations from your question, this means that you should be able to define a function that takes in source and outputs play. Note that to do this you do not have to reify all the syntax of Coq. Indeed, the actions available for your bots can probably be defined with a set of primitives smaller than full-fledged Coq. This lets you avoid the problem of using some form of self-description of the meta-language (which are very complicated beasts).\nSimilarly, you might also want to have such an encoding for proofs, again because your bots need to manipulate \"concrete\" data rather than meta-proofs in the ambient theory. Regarding formalization of logic, and also to give you ideas on the general code/decoding setting, you might want to give a look at eg. this library formalizing first-order logic.\nFinally, there is a second issue you might stumble upon, either in the definition of your decoding function, or that of a play, which is related to the definition of functions in Coq. Indeed, as Coq lets you only define total/terminating functions, quite a lot of things with a \"self-recursive\" flavour are not definable as functions, as they would lead to non-terminating behaviour. In such a case, a standard solution is to replace the type A -> B by the type A -> B -> Prop, ie. replace an actual function by its graph. This avoids termination issues, and also determinism: since you do not encode a game as a function, you can have multiple choices to move on. Moreover, if you define your relation as an inductive one, you can reason by induction on it, which is probably the kind of reasoning principles you will be after.", "meta": {"post_id": 1779, "input_score": 10, "output_score": 9, "post_title": "How can we formalize a game where each player is a program having access to opponent's code?"}}
{"input": "In Zermelo-Fraenkel axiomatics, one does not need to use the axiom of choice to resolve unique existential quantifiers, the axiom of replacement is enough for this.\nBut it seems that Type theory does not allow that: if you have a family of singletons, there is no way to get the family of its members.\nIf it is true, that can be proven by constructing a model of Type theory in which this does not hold.\nI suspect that \u201chomotopic\u201d models can be useful there \u2014 there are families of contractible spaces which have no section, but I have no clue about how to do it for real.", "output": "Let $\\exists! z \\in C . P(z)$ be shorthand for $\\exists z \\in C . (P(z) \\land \\forall w \\in C . P(w) \\to z = w)$.\nYour question is related to the axiom of unique choice (AUC): given a relation $R : A \\to B \\to \\mathsf{Prop}$ satisfying\n$$\\forall x \\in A . \\exists! y \\in B . R(x, y), \\tag{1}$$\nis there a map $f : A \\to B$ such that $\\forall x \\in A . R(x, f x)$? Indeed, $R$ can be thought of as a singleton in $B$ parameterized by $A$, and $f$ is the unique element of $B$ (still parameterized by $A$).\nA relation satisfying (1) is called a functional relation.\nAnother way to state AUC is this: the graph of $f : A \\to B$ is the relation $\\Gamma_f : A \\to B \\to \\mathsf{Prop}$ defined by\n$$\\Gamma_f \\, a\\, b \\mathrel{{:}{=}} (f a = b).$$\nThen AUC states that every functional relation is the graph of a function.\nWhether AUC holds depends on how precisely we set up foundations.\nIn toposes AUC holds, so in particular it holds in ZFC and other set theories.\nIn Martin-L\u00f6f type theory AUC holds if we write (1) as\n$$\\textstyle\n\\prod_{x : A}\n(\\sum_{y : B}\nR(x,y) \\times \\prod_{z : B} R(x,z) \\to \\mathsf{Id}(y,z))\n$$\nThat's because we used $\\Sigma$ in place of $\\exists$, so we can extract the witness $y$. (Of course, for the same reason MLTT validates the full axiom of choice.)\nIn homotopy type theory AUC holds. Note that\n$$\n\\textstyle\n\\exists z \\in C . (P(z) \\land \\forall w \\in C . P(w) \\to z = w)\n$$\nis defined to be\n$$\n\\textstyle\n\\|\\sum_{z : C} (P(z) \\times \\prod_{w : C} P(w) \\to z = w)\n\\|\n$$\nBecause the type inside the truncation is a proposition, we may apply Lemma 3.9.1 of HoTT book to extract $z : C$.\nIn the Calculus of Inductive Constructions, i.e., the type theory of Coq, AUC does not hold when we use $\\mathsf{Prop}$. This is so because we cannot eliminate from the proposition $\\exists! y \\in B . R(x, y)$ to $B$, unless $B$ itself is a proposition.\nSo the question really is: which formalism are you using?", "meta": {"post_id": 1783, "input_score": 5, "output_score": 8, "post_title": "Does one need a type-theoretical axiom of choice for singletons?"}}
{"input": "This is somewhat conceptual beginner's question about proof assistants.\nI've been re-reading the famous Seven Trees in One / Objects of categories as complex numbers. The gist: The type $T$ of binary trees admits an explict, combinatorial ismorphism $T \\cong T^7$.\nOne way to see this is that in every commutative semiring, the equation $x = x^2+1$ implies $x = x^7$ (straightforward equational reasoning). We can then construct what the second paper calls the Burnside rig, the commutative semiring of isomorphism classes of types. Because we have an isomorphism $T \\cong T^2+1$, the equivalence class $[T]$ in this semiring satisfies $[T] = [T]^7$, hence $T \\cong T^7$.\nThis suggests to me a nice modular way to construct the seven-trees-in-one isomorphism\n\nFormalize the Burnside rig\nuse an existing semiring tactic to solve the equational reasoning part\nextract the isomorphism\n\nFor the sake of this question, I will take it as a given that the appropriate semiring tactic exists and can solve (x = x^2 + 1) -> (x = x^7). (I know ring tactics exists, but semiring is slightly more subtle).\nMy question is thus about the remaining aspects of the formalization. How would we go about this problem in different proof assistants (Coq, Agda, Lean)?\n\nLean has quotients. Can we form the Burnside rig as is? Will we still be able to compute the isomorphism?\nWould Coq require that its ring tactic does not use vanilla = but setoids?\nAm I correct to think that under univalence, universes are commutative semirings on the nose? Does this simplify the proof, say in Cubical Agda? I've enjoyed Computing with Semirings and Weak Rig Groupoids but can it be used here in practical terms?", "output": "The main subtlety is that it doesn't seem easy to automate the semigroup equational reasoning required by Seven Trees in One, but if we put that process aside (like, accepting to do it by hand, which could still look decent if just associativity and commutativity were handled automatically), I would expect the rest to go as you would expect. I don't know about Lean's quotient, but your given approaches for Coq (generalizing up to proof-relevant equivalence relations) and Agda (using univalence) seem reasonable.\nHere's an implementation in Agda, though it does not extract a completely working bijection: one direction generates many applications of transp that Agda doesn't want to reduce for some reason. This might also affect the other direction, but I haven't found an example in my limited number of tests. If you tell it what the right normal term is, it is accepted (thanks to Dan Doel for pointing this out!); that slightly defeats the point of the exercise, hence it's \"not completely working\".\nA neat trick is that you can define the quotient \u2115[X]/(X\u2261X\u00b2+1) directly as a higher inductive type and prove the equation X^7=X in that concrete structure once and for all instead of parameterizing the proof by an abstract semiring. Initiality (aka. \"fold\") then transports that equation to other semirings with an element satisfying x = x^2+1.\n-- 1. Prove X\u2077\u2261X in the polynomial quotient semiring \u2115[X]/(X\u2261X\u00b2+1)\n-- 2. Define a map toType : \u2115[X]/(X\u2261X\u00b2+1) \u2192 Type,\n--    using the semiring structure of Type and the fact that\n--    the equation T\u2261T\u00b2+1 holds for the type of binary trees T,\n--    thanks to univalence.\n-- 3. We deduce T\u2077 \u2261 T, by T\u2077 \u2261 toType X\u2077 \u2261 toType X \u2261 T\n-- 4. ???\n-- 5. Profit\n\n{-# OPTIONS --cubical #-}\nmodule S where\n\nopen import Cubical.Core.Everything\nopen import Cubical.Foundations.Isomorphism\nopen import Cubical.Foundations.Univalence\nopen import Cubical.Foundations.Structure\nopen import Cubical.Foundations.Prelude\nopen import Cubical.Foundations.Function\nopen import Cubical.Foundations.Transport\nopen import Cubical.Foundations.HLevels\nopen import Cubical.Data.Sigma\nopen import Cubical.Data.Sum\nopen import Cubical.Data.Unit\nopen import Cubical.Data.Nat using (\u2115; zero; suc)\n\npattern 3+_ n = suc (suc (suc n))\npattern 2+_ n = suc (suc n)\npattern 1+_ n = suc n\n\ninfixl 6 _+_\n\n-- * Part 1: X \u2261 X\u00b2+1 \u2192 X\u2077 \u2261 X\n--\n-- (X\u2077\u2261X is the name of the final theorem.)\n-- The proof follows Seven Trees in One (p 6) https://arxiv.org/pdf/math/9405205.pdf\n--\n-- X^7 = X^7 + X^4 + X = X\n--     by lemma X^(1+r) = X^(1+r) + X^(k+3) + X^k\n--     used twice, with r=6,k=1 and with r=0,k=4.\n--     (here we only give ad hoc proofs for those two instances)\n\n-- The semiring \u2115[X]/(X\u2261X\u00b2+1)\n-- This actually only requires a minimal set of axioms (weaker than semirings)\n-- to prove X\u2077\u2261X.\ndata \u2115[X]/tree : Type where\n  _+_ : \u2115[X]/tree \u2192 \u2115[X]/tree \u2192 \u2115[X]/tree\n  X^ : \u2115 \u2192 \u2115[X]/tree\n  +-comm : {a b : \u2115[X]/tree} \u2192 a + b \u2261 b + a\n  +-assoc : {a b c : \u2115[X]/tree} \u2192 a + b + c \u2261 a + (b + c)\n  tree : \u2200 n \u2192 X^ (1+ n) \u2261 X^ (2+ n) + X^ n\n\ntree\u02b3 : \u2200 {a} n \u2192 a + X^ (suc n) \u2261 a + X^ (suc (suc n)) + X^ n\ntree\u02b3 {a} n =\n  a + X^ (suc n) \u2261[ i ]\u27e8 a + tree n i \u27e9\n  a + (X^ (suc (suc n)) + X^ n) \u2261\u27e8 sym +-assoc \u27e9\n  a + X^ (suc (suc n)) + X^ n \u220e\n\n-- X^ (n + i) + ... + X^ n (left associated)\nX^[_+_] : \u2115 \u2192 \u2115 \u2192 \u2115[X]/tree\nX^[ n + suc i ] = X^[ suc n + i ] + X^ n\nX^[ n + zero ] = X^ n\n\nX\u00b2\u2261X\u00b2+X\u00b3+1 : \u2200 {a} n \u2192 a + X^ (2+ n) \u2261 a + X^ (2+ n) + X^ (3+ n) + X^ n\nX\u00b2\u2261X\u00b2+X\u00b3+1 {a} n =\n  a + X^ (suc (suc n)) \u2261\u27e8 tree\u02b3 (suc n) \u27e9\n  a + X^ (suc (suc (suc n))) + X^ (suc n) \u2261\u27e8 tree\u02b3 n \u27e9\n  a + X^ (suc (suc (suc n))) + X^ (suc (suc n)) + X^ n \u2261\u27e8 cong (_+ X^ n) +-assoc \u27e9\n  a + (X^ (suc (suc (suc n))) + X^ (suc (suc n))) + X^ n \u2261\u27e8 cong (\u03bb x \u2192 a + x + X^ n) +-comm \u27e9\n  a + (X^ (suc (suc n)) + X^ (suc (suc (suc n)))) + X^ n \u2261\u27e8 cong (_+ X^ n) (sym +-assoc) \u27e9\n  a + X^ (suc (suc n)) + X^ (suc (suc (suc n))) + X^ n \u220e\n\nX\u2077\u2261X\u2077+X\u2074+X : X^ 7 \u2261 X^ 7 + X^ 4 + X^ 1\nX\u2077\u2261X\u2077+X\u2074+X =\n  X^ 7 \u2261\u27e8 expand-X\u2077 \u27e9\n  X^[ 5 + 3 ] + X^ 3 \u2261\u27e8 X\u00b2\u2261X\u00b2+X\u00b3+1 1 \u27e9\n  X^[ 5 + 3 ] + X^ 3 + X^ 4 + X^ 1 \u2261[ i ]\u27e8 expand-X\u2077 (~ i) + X^ 4 + X^ 1 \u27e9\n  X^ 7 + X^ 4 + X^ 1 \u220e\n  where\n    expand-X\u2077 : X^ 7 \u2261 X^[ 5 + 3 ] + X^ 3\n    expand-X\u2077 =\n                    X^ 7 \u2261\u27e8 tree 6 \u27e9\n      X^ 8        + X^ 6 \u2261\u27e8 tree\u02b3 5 \u27e9\n      X^[ 7 + 1 ] + X^ 5 \u2261\u27e8 tree\u02b3 4 \u27e9\n      X^[ 6 + 2 ] + X^ 4 \u2261\u27e8 tree\u02b3 3 \u27e9\n      X^[ 5 + 3 ] + X^ 3 \u220e\n\ntree\u02e1 : \u2200 {a} n \u2192 X^ (suc n) + a \u2261 X^ (suc (suc n)) + (X^ n + a)\ntree\u02e1 {a} n =\n  X^ (suc n) + a \u2261[ i ]\u27e8 tree n i + a \u27e9\n  X^ (suc (suc n)) + X^ n + a \u2261\u27e8 +-assoc \u27e9\n  X^ (suc (suc n)) + (X^ n + a) \u220e\n\nX\u00b3+1+X\u2261X : \u2200 {a} n \u2192 X^ (3+ n) + X^ n + (X^ (1+ n) + a) \u2261 X^ (1+ n) + a\nX\u00b3+1+X\u2261X {a} n =\n  X^ (3+ n) + X^ n + (X^ (1+ n) + a) \u2261\u27e8 cong (_+ (X^ (1+ n) + a)) +-comm \u27e9\n  X^ n + X^ (3+ n) + (X^ (1+ n) + a) \u2261\u27e8 sym +-assoc \u27e9\n  X^ n + X^ (3+ n) + X^ (1+ n) + a \u2261\u27e8 cong (_+ a) (sym (tree\u02b3 (1+ n))) \u27e9\n  X^ n + X^ (2+ n) + a \u2261\u27e8 cong (_+ a) +-comm \u27e9\n  X^ (2+ n) + X^ n + a \u2261\u27e8 cong (_+ a) (sym (tree n)) \u27e9\n  X^ (1+ n) + a \u220e\n\nX\u2077+X\u2074+X\u2261X : X^ 7 + X^ 4 + X^ 1 \u2261 X^ 1\nX\u2077+X\u2074+X\u2261X =\n  X^ 7 + X^ 4 +  X^ 1      \u2261[ i ]\u27e8 X^ 7 + X^ 4 + expand-X i \u27e9\n  X^ 7 + X^ 4 + (X^ 5 + _) \u2261\u27e8 X\u00b3+1+X\u2261X 4 \u27e9\n                 X^ 5 + _  \u2261\u27e8 sym expand-X \u27e9\n                 X^ 1 \u220e\n  where\n    expand-X : X^ 1 \u2261 X^ 5 + _\n    expand-X =\n      X^ 1     \u2261\u27e8 tree 0 \u27e9\n      X^ 2 + _ \u2261\u27e8 tree\u02e1 1 \u27e9\n      X^ 3 + _ \u2261\u27e8 tree\u02e1 2 \u27e9\n      X^ 4 + _ \u2261\u27e8 tree\u02e1 3 \u27e9\n      X^ 5 + (X^ 3 + (X^ 2 + (X^ 1 + X^ 0))) \u220e\n\nX\u2077\u2261X : X^ 7 \u2261 X^ 1\nX\u2077\u2261X =\n  X^ 7               \u2261\u27e8 X\u2077\u2261X\u2077+X\u2074+X \u27e9\n  X^ 7 + X^ 4 + X^ 1 \u2261\u27e8 X\u2077+X\u2074+X\u2261X \u27e9\n  X^ 1 \u220e\n\n-- * Part 2: Tree as a solution of the equation X \u2261 X\u00b2+1\n--\n-- (toType is the name of the final theorem.)\n-- (univalence is used via isoToPath)\n\ndata Tree : Type where\n  Leaf : Tree\n  Node : Tree \u2192 Tree \u2192 Tree\n\n\u228e-comm : \u2200 {A B : Type} \u2192 A \u228e B \u2261 B \u228e A\n\u228e-comm = isoToPath \u228e-swap-Iso\n\n\u228e-assoc : \u2200 {A B C : Type} \u2192 (A \u228e B) \u228e C \u2261 A \u228e (B \u228e C)\n\u228e-assoc = isoToPath \u228e-assoc-Iso\n\nopen Iso\n\nIso-Tree-tree : (A : Type) \u2192 Iso (Tree \u00d7 A) ((Tree \u00d7 Tree \u00d7 A) \u228e A)\nIso-Tree-tree A .fun (Node t\u2081 t\u2082 , a) = inl (t\u2081 , t\u2082 , a)\nIso-Tree-tree A .fun (Leaf , a) = inr a\nIso-Tree-tree A .inv (inl (t\u2081 , t\u2082 , a)) = Node t\u2081 t\u2082 , a\nIso-Tree-tree A .inv (inr a) = Leaf , a\nIso-Tree-tree A .rightInv (inl (t\u2081 , t\u2082 , a)) = refl\nIso-Tree-tree A .rightInv (inr a) = refl\nIso-Tree-tree A .leftInv (Node t\u2081 t\u2082 , a) = refl\nIso-Tree-tree A .leftInv (Leaf , a) = refl\n\nTree-tree : (A : Type) \u2192 (Tree \u00d7 A) \u2261 ((Tree \u00d7 Tree \u00d7 A) \u228e A)\nTree-tree A = isoToPath (Iso-Tree-tree A)\n\nTree^ : (n : \u2115) \u2192 Type\nTree^ 0 = Unit\nTree^ (1+ n) = Tree \u00d7 Tree^ n\n\n-- This is essentially a way to define a semiring using a HIT,\n-- although this particular formulation only covers the substructure\n-- generated by Tree.\n-- Under univalence (which is used by isoToPath), Type is a semiring.\ntoType : \u2115[X]/tree \u2192 Type\ntoType (a + b) = toType a \u228e toType b\ntoType (X^ n) = Tree^ n\ntoType (+-comm {a} {b} i) = \u228e-comm {toType a} {toType b} i\ntoType (+-assoc {a} {b} {c} i) = \u228e-assoc {toType a} {toType b} {toType c} i\ntoType (tree n i) = Tree-tree (toType (X^ n)) i\n\n-- * Part 3: X\u2077\u2261X \u21d2 toType X\u2077 \u2261 toType X \u21d2 Tree\u2077 \u2261 Tree \u21d2 Iso Tree\u2077 Tree\n\nIso-Tree\u2077-Tree : Iso (Tree^ 7) Tree\nIso-Tree\u2077-Tree =\n  Tree^ 7  Iso\u27e8 pathToIso (\u03bb i \u2192 toType (X\u2077\u2261X i)) \u27e9\n  Tree \u00d7 Unit  Iso\u27e8 rUnit\u00d7Iso \u27e9\n  Tree \u220eIso\n\n-- * Examples\n\nf = Iso-Tree\u2077-Tree\n\n_ : fun f (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt) \u2261 Leaf\n_ = refl\n\n_ : fun f (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Node Leaf Leaf , tt)\n  \u2261 Node Leaf Leaf\n_ = refl\n\n_ : fun f (Node Leaf Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n  \u2261 Node (Node  (Node (Node (Node (Node (Node Leaf Leaf) Leaf) Leaf) Leaf) Leaf)  Leaf) Leaf\n_ = refl\n\n_ : fun f (Node Leaf Leaf , Node Leaf Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n  \u2261 Node (Node (Node (Node (Node (Node (Node Leaf Leaf) (Node Leaf Leaf)) Leaf) Leaf) Leaf)  Leaf) Leaf\n_ = refl\n\n_ : inv f Leaf \u2261 (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n_ = refl -- When you ask Agda to normalize this goal, it gets stuck on transp, lots of transp. But refl still typechecks.\n\n_ : inv f (Node Leaf Leaf) \u2261 (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Node Leaf Leaf , tt)\n_ = refl\n\n\nCode on Gist: https://gist.github.com/Lysxia/24bc08e7c95aa5594d51e70b9d3842eb", "meta": {"post_id": 1814, "input_score": 11, "output_score": 8, "post_title": "Seven Trees in One, or How to formalize the Semiring of Types?"}}
{"input": "I was messing around with toy lemmas in Idris and came up with this silly proof.\n%default total\n\ndata Dumb : Nat -> Type where\n  Way : Dumb Z\n  NoWay : Dumb (S k) -> Dumb k\n\nnotDumb : Dumb (S k) -> Void\nnotDumb (NoWay d) = notDumb d\n\nAnd in Agda...\ndata Dumb : \u2115 -> Set where\n  way : Dumb 0\n  no_way : {k : \u2115} -> Dumb (suc k) -> Dumb k \n\nnot_dumb : {k : \u2115} -> Dumb (suc k) -> \u22a5\nnot_dumb (no_way d) = not_dumb d\n\nPretty straightforward. But then when I tried it in Coq, it became a bit tricky. The only way I could figure out how to do it was with dependent induction.\nRequire Import Coq.Program.Equality.\n\nInductive dumb: nat -> Prop :=\n  | way : dumb 0\n  | noway (k: nat): dumb (S k) -> dumb k.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros. intro.\n  dependent induction H.\n  apply (IHdumb (S k)).\n  auto.\nQed.\n\nAnd the proof term is pretty complicated.\nnot_dumb = \nfun k : nat =>\n(fun H : dumb (S k) =>\n (fun (gen_x : nat) (H0 : dumb gen_x) =>\n  (fun H1 : dumb gen_x =>\n   dumb_ind\n     (fun gen_x0 : nat =>\n      forall k0 : nat, gen_x0 = S k0 -> block (block False))\n     (fun (k0 : nat) (H2 : 0 = S k0) =>\n      False_ind (block (block False))\n        (let H3 : False :=\n           eq_ind 0\n             (fun e : nat =>\n              match e with\n              | 0 => True\n              | S _ => False\n              end) I (S k0) H2 in\n         False_ind False H3))\n     (fun (k0 : nat) (H2 : dumb (S k0))\n        (IHdumb : forall k1 : nat, S k0 = S k1 -> block (block False))\n        (k1 : nat) (H3 : k0 = S k1) =>\n      solution_left nat\n        (fun x : nat =>\n         dumb (S x) ->\n         (forall k2 : nat, S x = S k2 -> block (block False)) ->\n         block (block False)) (S k1)\n        (fun (_ : dumb (S (S k1)))\n           (IHdumb0 : forall k2 : nat,\n                      S (S k1) = S k2 -> block (block False)) =>\n         IHdumb0 (S k1) eq_refl : block False) k0 H3 H2 IHdumb) gen_x\n     H1) H0 k) (S k) H eq_refl)\n:\n~ dumb (S k)\n     : forall k : nat, ~ dumb (S k)\n\nSo the questions are:\n\nWhy are these proofs so different? Is there something fundamental about languages like Idris and Agda which make this sort of proof simpler? Is there something about Coq that makes this kind of proof hard? My sense is it has something to do with pattern matching, but it would be great if someone who knows more about pattern matching in Idris or Agda could verify this.\n\nWhat's going on in this Coq proof? Is it ultimately doing the same thing as what is being done in the Idris/Agda case? Is it just that Coq's totality checker is doing less work?\n\nIs there actually a very simple Coq proof of this? And the entire issue is moot?\n\n\nThanks, anything helps.\n\nFollow up. Thanks for the nice responses. They made me realize you can do this directly with the induction principle, albeit not using tactics.\nDefinition is_zero (n : nat) : Prop :=\n  match n with\n  | 0 => True\n  | (S _) => False\n  end.\n\nLemma dumb_is_zero : forall k, dumb (S k) -> is_zero (S k) -> is_zero k.\nProof.\n  intros.\n  destruct k.\n  - exact I.\n  - assumption.\nQed.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros k d.\n  exact (dumb_ind is_zero I dumb_is_zero (S k) d).\nQed.\n\nI'm still curious though, is there any way of generating the proof using tactics?\n\nI guess ultimately the point is that I really want to prove\nLemma not_dumb: forall k, dumb (S k) -> is_zero (S k).\nProof.\n  intros k d.\n  induction d.\n  - exact I.\n  - inversion IHd.\nQed.\n\nThanks again.", "output": "There is a coq proof which doesn't require dependent induction:\nLemma dumb0 : forall k, dumb k -> k = 0.\nProof.\n  intros k dumb.\n  induction dumb.\n  - reflexivity.\n  - discriminate IHdumb.\nQed.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros k dumb.\n  discriminate (dumb0 (S k) dumb).\nQed.\n\nAnd a way to write the proof as a fixpoint, which looks very similar to the Idris and Agda examples:\nLemma false_succ_eq_zero : forall {P : Prop} {k : nat}, S k = 0 -> P.\nProof.\n  discriminate.\nQed.\n\nFixpoint dumb0' {k : nat} (d : dumb k) : k = 0 :=\n  match d with\n  | way => eq_refl\n  | noway k' d' => false_succ_eq_zero (dumb0' d')\n  end.\n\nDefinition not_dumb' {k : nat} (d : dumb (S k)) : False :=\n  false_succ_eq_zero (dumb0' d).\n\nNotice how dumb0' has the same recursion scheme as notDumb and not_dumb in Idris and Agda.\n\nHowever, there's a lot more boilerplate in the Coq version. And the latter proof is hiding some of the complexity in the false_succ_eq_zero term:\nfalse_succ_eq_zero = \nfun (P : Prop) (k : nat) (H : S k = 0) =>\nlet H0 : False := eq_ind (S k) (fun e : nat => match e with\n                                               | 0 => False\n                                               | S _ => True\n                                               end) I 0 H in\nFalse_ind P H0\n\nCoq proofs are \"mechanized\": they make heavy use of generalized tactics, which produce the ugly code that you see in the Print statements, but enable proof automation (ex: auto randomly tries a predefined set of tactics, lia solves arbitrary linear relations, the Ltac language lets you define custom tactics). Idris and Agda instead rely on more traditional pattern matching, thus their definitions are more \"readable\" (and there is no separate \"proof buffer\" in the interactive theorem-proving process) and their dependent type unification is stronger (see: Agda's cubical types, which supposedly automate a lot of \"intuitive\" things you must explicitly do with Coq like rewrite equalities), but they don't have the kind of powerful automation that Coq does.", "meta": {"post_id": 1848, "input_score": 7, "output_score": 8, "post_title": "Why are these two proofs so different?"}}
{"input": "I am considering whether I should learn the ACL2 proof assistant. However, before diving into that, I wanted to know if there are significant limitations in terms of proving math. The first thing I noticed is that it is said to be first-order-logic (e.g. here).\nI am not sure if being FOL is a significant limitation in terms of proving math. From my very limited understanding, a concrete example of using higher order logic is to assert in inductions that an arbitrary property $P$ holds for all values (e.g. the ability to quantify over properties or functions). But from initial reading, it seems that ACL2 can define various kinds of induction rules:\n\nHowever, unlike Nqthm, ACL2 provides a means by which the user can\nelaborate the rules under which function applications suggest\ninduction schemes. Such rules are called :induction rules. The\ndefinitional principle automatically creates an :induction rule, named\n(:induction fn), for each admitted recursive function, fn.\n...\nIt is possible for the user to create additional :induction rules by\nusing the :induction rule class in defthm.\n\nMy questions are:\nAre there proofs using inductions that can be proved in HOL provers (such as Isabelle) but not in ACL2?\nMore generally, are there other types of mathematical proofs that can be done in HOL provers but not in ACL?", "output": "ACL2 supports well-founded induction up to, but not including $\\epsilon_0$. So you cannot prove consistency of (first order) arithmetic, whereas in higher order logic you can (type :doc ordinals in the REPL for documentation).\nMore practically, however, you cannot express infinite sets in ACL2. More precisely, you can define a predicate (ie. even x holds iff x is an even number) which holds on infinitely many objects, and think of this as an infinite set, however you cannot quantify over infinite sets. This restriction to quantification over finite sets is the real limitation here, more so than being a first-order logic (recall ZFC is a first order logic).\nThere are advantages to this approach, however, as ACL2 supports highly automated inductive proofs.", "meta": {"post_id": 1867, "input_score": 5, "output_score": 8, "post_title": "Is ACL2 first-order-logic and is this a limitation for proving math?"}}
{"input": "I am wondering whether a rule like the following is consistent with decidable conversion and type-checking for dependent type theory:\n$$ \\frac{f\\, g : (x:\\mathsf{bool}) \\to C~x\\quad f~\\mathsf{tt} \\equiv g~\\mathsf{tt} \\quad f~\\mathsf{ff} \\equiv g~\\mathsf{ff}}{f\\equiv g} $$\nThat is, if two functions with domain $\\mathsf{bool}$ agree definitionally on $\\mathsf{tt}$ and $\\mathsf{ff}$, then they are convertible.  An analogous rule for functions on general inductive types like $\\mathbb{N}$ is certainly bad, but it seems not too ridiculous to me to wonder whether the special case of $\\mathsf{bool}$ (and other non-recursive inductive types, like sum-types and enumerated types) might be possible.\nOne thing that makes me somewhat hopeful is that this rule would make $(x:\\mathsf{bool}) \\to C~x$ behave very much like $C~\\mathsf{tt} \\times C~\\mathsf{ff}$, and indeed be definitionally isomorphic to it if the latter also has an $\\eta$-rule.  Since such a rule for products is possible, I thought maybe it could also be possible for the function-type.  And implementation-wise, I could imagine a bidirectional conversion-checking algorithm noticing when the type is a function with domain $\\mathsf{bool}$ and deciding to apply this rule instead of the ordinary $\\eta$-rule for functions.", "output": "If we have $\\eta$ for functions and also your pointwise conversion rule, that implies the full $\\eta$ rule for the (finite) domain type. When checking conversion of arbitrary $t,u$, we can abstract over an arbitrary $\\mathsf{bool}$ subterm $v$ to get an equivalent conversion problem\n$$(\\lambda~(x:\\mathsf{bool}).~ t')~v\\equiv(\\lambda~(x:\\mathsf{bool}).~ u')~v$$\nFor instance\n$$\\mathsf{if}~b~\\mathsf{then}~t~\\mathsf{else}~t \\equiv (\\lambda~x.~\\mathsf{if}~x~\\mathsf{then}~t~\\mathsf{else}~t)~b \\equiv (\\lambda~x.t)~b \\equiv t$$\nThe full $\\eta$-rules for finite sums are rather complicated and expensive to check. There's a reference for STLC with finite sums and products. However, I do not know any reference for deciding finite sum $\\eta$ in richer systems.\nI expect that $\\eta$ for non-empty finite sums remains decidable even in the fully dependent type theories. $\\eta$ for the empty type is decidable only when consistency of typing contexts is decidable.\nMore specifically for our current case, let's assume that $\\bot \\to A$ is definitionally irrelevant. Then, if we have $p : \\bot$ and $t,~u : A$, then\n$$t \\equiv (\\lambda~x.~t)~p \\equiv (\\lambda~x.~u)~p \\equiv u$$\nTherefore, if $\\bot$ is provable then all terms are convertible. This is undecidable for the usual $\\bot$ in type theories, but it might be decidable  in other settings. For example, in ABCFHL cubical type theory all judgments hold when the empty cofibration is implied by the context. This is still decidable because cofibrations are decidable. This is implemented in cooltt.\nAlternatively, we can drop the function $\\eta$ rule. Then, we recover the finite product types as $(x : A) \\to C~x$ for finite $A$, and everything is decidable. I don't think this configuration is useful for anything though.", "meta": {"post_id": 1885, "input_score": 9, "output_score": 9, "post_title": "Strong eta-rules for functions on sum types"}}
{"input": "I'm implementing a version of cubical type theory where the well-definedness of pattern matching functions is implied by:\n\nthe well-typedness of the clauses (type check)\nthe coverage of the patterns (coverage check)\nthe fact that recursive calls are structurally recursive (termination check)\nthe overlapping clauses are confluent (IApplyConfluence) -- see https://amelia.how/quick/boundaries-in-cubical-agda.html\n\nMy question: in what order does it make the best sense? Obviously type check should come first, but what about the others?\n\nSince IApplyConfluence requires reduction of function bodies (for the conversion checks), we may potentially need to reduce the function being type-checked. If there is a non-termination problem, then we are in trouble (the type checker may loop). However, it makes me worried because how am I about to reduce a function that we do not yet know if it's well-defined?\nTermination check is, in my impression, the last phase of the type checking, because there are techniques developed to do some clever reduction to make more definitions type-checked. It seems that we should definitely termination-check those who have no problem on reduction. However, if the clauses are not even confluent, we may reduce the wrong thing!\nIn case of mutually recursive functions, it's worse: the termination check of the two functions happens after both of their type checking, which means\n\nif termination comes last, then both of their confluence check will happen when the other is neither known to be structurally recursive nor confluent.\nif confluence comes last, then both of their termination check will happen when the other is neither known to be structurally recursive nor confluent.", "output": "As you have noticed there is no \"natural\" way to prove confluence and termination: to prove confluence from local confluence you need termination, but proving termination often depends on typing, which becomes a lot more tricky without confluence.\nIn our paper on rewrite rules (The Taming of the Rew: a Type Theory with Computational Assumptions) we took one possible approach, which is to prove (global) confluence directly, i.e. without relying on termination. The basic idea is to rely on the triangle property of parallel reduction, which is a well-known trick that was first used to prove confluence of untyped lambda calculus. The main challenges here are (1) to define a proper notion of parallel reduction for your language and (2) to prove that the triangle property holds.\nAlternatively, you can look into first proving termination without assuming well-typedness of your program, or at least without assuming that subject reduction holds. There are many possible directions to explore here. One example that bears mentioning is SizeChangeTool: A Termination Checker for Rewriting Dependent Types, which requires local confluence but not global confluence.", "meta": {"post_id": 1890, "input_score": 8, "output_score": 10, "post_title": "Termination and confluence -- which goes first?"}}
{"input": "A question was recently asked on the Coq-club mailing list on Coq rejecting a nastily nested inductive type. We encountered a similar difficulty while trying to port code from Agda to Coq: Agda accepts the following\nrecord Foo (x : Set -> Set) :  Set where\n\ndata Bar : Set -> Set where\n  bar : Bar (Foo Bar)\n\nwhile Coq rejects\nRecord Foo (x : Type -> Type) : Type := {}.\n\nFail Inductive Bar : Type -> Type :=\n    | bar : Bar (Foo Bar).\n(*Non strictly positive occurrence of \"Bar\" in \"Bar (Foo Bar)\".*)\n\nIt seems like Coq does not accept nested inductive types in indices, while Agda does.\nSo I tried to look up what kind of complex induction pattern (induction-induction, induction-recursion, nestedness\u2026) Agda exactly allows.\nI found the manual page on mutual recursion, which gives some examples, but is rather vague\u2026 Is there any more precise reference on what Agda allows and forbids? And, ideally, some hint as to why the allowed things are ok/have a semantic?", "output": "I try to write about what's supported in Agda and the semantics that we have (or don't have) for it. Disclaimers:\n\nI'm not an expert on the relevant parts of the Agda source code, I mostly write here as an Agda user, so I might make mistakes. Corrections are welcome.\nI don't write about coinduction. It adds many new weird and interesting feature interactions, but I'm not as nearly as experienced with coinduction as with induction in Agda.\nI don't write about erasure, irrelevance or the flat modality. I've never used these.\nI don't write much about cubical features. They don't yet work well together with many inductive features, e.g. indexed inductive types break canonicity in the 2.6.3 version of cubical Agda. The most general rigorous specification of cubical (higher) inductive types so far is in Evan Cavallo's thesis, and that only covers plain indexed HITs (no IR, no induction-induction, no nesting).\n\nI also don't write much about dependent pattern matching. It is a powerful and complicated feature, but, perhaps surprisingly, I do not think that it's a source of weirdness and exotic features. The --without-K option precisely captures the UIP and non-UIP behaviors, and everything works generally as expected.\n1. What's in Agda\n\nIt's a fun exercise to try to summarize this in one phrase. I'd say that Agda supports\n\nSet/Prop-sorted non-interleaved structural inductive-inductive-recursive types, nested with simple first-order polarities\n\n(Disregarding the cubical mode).\nI'll expand on what the above means, but first let's do an overview of inductive signatures. In a nutshell, an Agda inductive signature is a list where each entry is one of four possible things:\n\nA declaration of a type constructor (sort). Each declaration has a telescope of parameters, a telescope of indices and a return sort which is either Set i or Prop i. In --without-K mode, the return universe size must be large enough to accommodate the index telescope.\nA declaration of a definition.\nThe specification of all constructors of a previously declared sort.\nThe body of a previously declared definition.\n\nAll signature entries are well-typed modulo all previously declared symbols and the definitional equalities stemming from definition bodies. Moreover, a signature is valid only if\n\nAll inductive types are strictly positive. This depends on polarities and positivity checking.\nAll definitions are total. This depends on termination checking.\nAll inductive types return in a universe which is large enough to accommodate all fields of all constructors of the type.\n\nSet/Prop-sorted\nWe can freely mix Set and Prop sorts, where Prop contains definitionally irrelevant types. This has not been described in any publication. I think that this is a fairly tame feature compared to some of\nthe others, in the sense that I don't see any issue with computational adequacy (e.g. canonicity), and it shouldn't be difficult to handle in semantics.\nNon-interleaved\nAgda mandates that all constructors of a sort are specified together. The following can't be expressed:\nA  : Set\nB  : A \u2192 Set\nC  : (a : A) \u2192 B a \u2192 Set\na1 : A\nb  : B a1\na2 : C a1 b \u2192 A\n\nThe interleaved mutual keyword also doesn't allow this; it's merely syntactic sugar which desugars to forward-declared signatures. The problem here is that each constructor depends on the previously declared ones, so they can't be reordered. However, Szumi Xie discovered that we can convert this (and every analogous example) to a non-interleaved version by pushing all sorts inside a Tarski-style \"universe\":\ndata U  : Set\ndata El : U \u2192 Set\n\ndata U where\n  A : U\n  B : El A \u2192 U\n  C : (a : El A) \u2192 El (B a) \u2192 U\n\ndata El where\n  a1 : El A\n  b  : El (B a1)\n  a2 : El (C a1 b) \u2192 El A\n\nI note though that while initial algebras can be derived this way, de-interleaving changes the presented theory itself (notions of algebras are different).\nAlso, this technique does not (fully) work with mixed Set/Prop induction, because we have to choose a single universe for El.\nStructural\nAgda functions are defined using structural recursion, instead of eliminators. In simple cases, the two approaches coincide, but in Agda's generality, they don't. We will see examples later.\nAgda uses lexicographic recursion, which implies that multiple function arguments are considered simultaneously in termination checking. Contrast this to Coq where there's at most one structurally decreasing argument. When we specify constructors of a sort, we extend the structural ordering\nrelation on types by positing that every constructor is larger than its fields.\n(Side comment: predicativity affects structural ordering, because an impredicative function is not necessarily larger than its result values. This is relevant in Coq but not in Agda).\nInductive-inductive-recursive\nClearly, Agda includes the inductive-inductive and inductive-recursive types from the literature, but it includes more than that; see examples later.\nNested with simple first-order polarities\nAgda supports nested induction. The classic example:\ndata Tree (A : Set) : Set where\n  node : A \u2192 List (Tree A) \u2192 Tree a\n\nHere we refer to a pre-existing List type former. But we can use any type former in scope in a nested way:\ndata Bush (A : Set) : Set where\n  nil  : Bush A\n  cons : A \u2192 Bush (Bush A) \u2192 Bush A\n\nThe most general specification of nested induction that I know about is by Johan & Polonsky. But this only covers parameterized finitary algebraic data types, a tiny subset of Agda. The positivity complications are sidestepped by only having finitary types (no function types are allowed in signatures). In ibid. the following is accepted:\ndata Fix f = Fix (f (Fix f))\n\nThe reason is that the type language only allows strictly positive instantiations for f. In Agda, this is not accepted. Instead there's positivity checking and a system of polarities. Like termination checking, positivity checking can look into all definitions and types in a signature.\nAny function-typed symbol can have a polarity for each argument. Polarities are constant, positive, strictly positive, negative and invariant. Function arguments with arbitrary types can have polarity. Polarities are simply typed and first-order, in the sense that they are concrete and and non-dependent for each argument. For example, it is not expressible that the polarity of a parameterized type depends on parameter values. The following fails to check:\ndata A (b : Bool)(X : Set) : Set where\n  a : if b then X else (X \u2192 X) \u2192 A b X\n\ndata B : Set where\n  b : A true B \u2192 B\n\nIt works though when A is a plain definition, because while Agda only considers parameter polarities for data, it does unfold definitions:\nA : Bool \u2192 Set \u2192 Set\nA b X = if b then X else (X \u2192 X)\n\ndata B : Set where\n  b : A true B \u2192 B\n\nWhat about the strength of nested induction compared to non-nested induction? We don't know much about this in the generality of Agda, and from what I've heard, not even in the generality of Coq. We do know that nested induction allows construction of some types which are otherwise not constructible. For example, infinitary quotient inductive types are not constructible from inductive types and quotients, but if we can use nested quotients, they actually are.\n2. Exotic features\n\nLet's look at some notable concrete features which fall out from the Agda implementation.\nRecursion-recursion\nThis feature has no semantics in the literature. The name \"recursive-recursive\" was coined by Fredrik Forsberg, as a characterization of inductive-inductive eliminators, but the weird part here is that recursion-recursion works in Agda wholly separately of induction-induction. Take this ordinary mutual inductive definition:\ndata Con : Set\ndata Ty  : Set\n\ndata Con where\n  \u2219 : Con\n  _,_ : Con \u2192 Ty \u2192 Con\n\ndata Ty where\n  Pi  : Con \u2192 Ty \u2192 Ty \u2192 Ty\n  U   : Con \u2192 Ty\n  El  : Con \u2192 Ty\n\nThis is intended to be \"raw syntax\" for some toy type theory. Now, we want to specify well-formedness for contexts and types. We want to specify that U \u0393 is a type in \u0393, and El \u0393 is a type in (\u0393 , U). We define well-formedness predicates by recursion:\nopen import Data.Product\nopen import Relation.Binary.PropositionalEquality\nopen import Data.Unit\n\nWfCon : Con \u2192 Set\nWfTy  : \u2200 \u0393 \u2192 WfCon \u0393 \u2192 Ty \u2192 Set\n\nWfCon \u2219       = \u22a4\nWfCon (\u0393 , A) = \u03a3 (WfCon \u0393) \u03bb \u0393w \u2192 WfTy \u0393 \u0393w A\n\nWfTy \u0393 \u0393w (Pi \u0393' A B) = \u0393 \u2261 \u0393' \u00d7 \u03a3 (WfTy \u0393 \u0393w A) \u03bb Aw \u2192 WfTy (\u0393 , A) (\u0393w , Aw) B\nWfTy \u0393 \u0393w (U \u0393')      = \u0393 \u2261 \u0393'\nWfTy \u0393 \u0393w (El \u0393')     = \u0393 \u2261 (\u0393' , U \u0393')\n\nAgda accepts this, but it cannot be defined directly using the elimination principle of Con and Ty! The problem is that the type of WfTy refers to WfCon, but no such dependency is possible in the eliminator. Notions of \"elimination\" and \"structural recursion\" diverge here.\nNotably, much of the infamously tedious boilerplate in the initiality constructions is for recovering recursive-recursive definitions from the elimination principles of untyped syntax.\nThis is related to the construction of inductive-inductive types: we represent inductive-inductive types using mutually inductive \"raw terms\" together with well-formedness predicates. The bulk of the work is to show\nthat recursion-recursion is derivable for the ordinary mutual inductive terms.\n\nWe know that finitary inductive-inductive types are constructible from UIP, funext and plain induction.\nWe reasonably conjecture that infinitary inductive-inductive types are constructible from UIP, funext, SProp, propext, quotients and WISC.\n\nBut we know pretty much nothing --without-K. Except, if we throw recursion-recursion in the mix, it looks like all inductive-indutive types are constructible! This is unpublished and rather fuzzy, because we don't even have a specification of recursion-recursion, but I'm fairly confident in it. I conjecture that neither recursion-recursion nor induction-induction are derivable --without-K, in the absence of some other unknown features.\nDouble induction-recursion\nThe definition of universes using induction-recursion is well-known. But what about universes of setoids, where every represented type is equipped with an equivalence relation? This is required when we want to model an observational type theory. There are several ways to do it in Agda; here's a minimal incomplete example.\nopen import Data.Nat\nopen import Data.Unit\nopen import Data.Empty\nopen import Relation.Binary.PropositionalEquality\nopen import Data.Product\n\ndata U : Set\nU~     : U \u2192 U \u2192 Set\nreflU  : \u2200 {A} \u2192 U~ A A\nEl     : U \u2192 Set\nEl~    : \u2200 {A A'} \u2192 U~ A A' \u2192 El A \u2192 El A' \u2192 Set\n\nresp~ : \u2200 {A} \u2192 (El A \u2192 U) \u2192 Set\nresp~ B = \u2200 {a a'} \u2192 El~ reflU a a' \u2192 U~ (B a) (B a')\n\ndata U where\n  \u2115' : U\n  \u03a0' : (A : U)(B : El A \u2192 U) \u2192 resp~ B \u2192 U\n\nU~ \u2115'         \u2115'           = \u22a4\nU~ \u2115'         (\u03a0' _ _ _)   = \u22a5\nU~ (\u03a0' _ _ _) \u2115'           = \u22a5\nU~ (\u03a0' A B _) (\u03a0' A' B' _) = \u03a3 (U~ A A') \u03bb A~ \u2192 \u2200 {a a'} \u2192 El~ A~ a a' \u2192 U~ (B a) (B' a')\n\nreflU {\u2115'}        = tt\nreflU {\u03a0' A B Br} = reflU , \u03bb a~ \u2192 Br a~\n\nEl \u2115'         = \u2115\nEl (\u03a0' A B _) = (a : El A) \u2192 El (B a)\n\nEl~ {\u2115'}       {\u2115'}         A~        x y = x \u2261 y\nEl~ {\u03a0' A B _} {\u03a0' A' B' _} (A~ , B~) f g = \u2200 {a a'} a~ \u2192 El~ (B~ a~) (f a) (g a')\n\nHere U itself is equipped with a relation and each El type is also equipped with a heterogeneous equivalence. The resp~ field in the type of \u03a0' requires that the codomain type respects the chosen equivalences.\nThis definition, however, is not covered by any specification of induction-recursion in the literature. In classic IR, an inductive sort can be fibered over an external type. Here, U~ maps from two copies of U to Set; hence the name \"double induction-recursion\". More generally, Agda supports recursive functions mapping out from any telescope of inductive sorts.\nIn this specific example, the double IR can be avoided by defining U~ inductively and El~ homogeneously, but a) that doesn't compute as strictly as we'd like to have in observational TT b) in the complete formalization of the setoid universe we'll need double IR anyway in some other places.\nIn the recent OTT paper, Pujet & Tabareau use similar IR for the semantics of the universe. They work in constructive set theory and don't give a particular justification for this IR. I don't know enough about set theory to tell if this IR is indeed trivially valid there. However, I think that double IR is very likely to make sense and it is the most convenient way to formalize setoid universes. The alternative is rather gnarly: Altenkirch et al. defines the same setoid universe without IR, using complicated induction-induction.\nEndo-induction-recursion\nIn classic IR, a recursive function has to map into an external type, i. e.\nsome type which is not declared as a sort in the signature that is being defined. In Agda, we can map wherever we want. Example:\ndata A : Set\ndata B : A \u2192 Set\nf      : (a : A) \u2192 B a\ndata A where ...\ndata B where ...\n\nThis is again not described in the literature. But the semantics looks a bit easier in this case, as far as I see. I expect that endo-IR can be derived from quotient induction and UIP. The following is due to Ambrus Kaposi.\nFirst, we define a quotient inductive type which includes f as an ordinary constructor and the definition of f as a bunch of quotient equations. Then, we use elimination to define f again as a function instead, which is always possible if the original f specification is properly structurally recursive. The thus strictified initial algebra can be then shown to be equivalent to the one with the quotienting.\nThe point is that we can often redefine quotients as functions, which is more convenient in formalization. This is useful even if we can't easily get rid of all quotienting. Endo-IR can be viewed as a shortcut to such strictification which relies on Agda's termination checking.\n3. Summary of Agda features in the literature\n\nI describe sets of features which are covered in the literature. These are the largest such sets that I was able to find.\n\nChapter 5 of my thesis specifies infinitary inductive-inductive types. It's a special case of quotient induction-induction, but quotients can be trivially dropped from the syntax & the semantics. This chapter allows us to compute eliminators and their beta rules in pretty much the same way as they would appear in Agda. Prop is not  supported though, nor sorts at different universe levels; Section 5.6 only supports having the same universe level for all sorts. I skipped this feature because universe levels are rather tedious to wrestle all the time.\nThis note of mine extends this to infinitary induction-induction-recursion. This follows the style of Dybjer & Setzer, in that recursive functions eliminate from exactly one inductive sort to an external type. Semantics is not developed in the note but I don't expect any complication for the set-truncated case.\nAs I mentioned, Johan & Polonsky cover most ground from nested induction.\nEvan Cavallo covers most ground from higher inductive types in the cubical mode. It's a different cubical type theory, but everything should be straightforward to port over to Agda's flavor.\n\nI shall leave coinduction to someone else as I'm not that familiar with the literature.", "meta": {"post_id": 1981, "input_score": 15, "output_score": 21, "post_title": "What are the complex induction patterns supported by Agda?"}}
{"input": "I heard that many proof assistant programs are made based on the type theory.\nFor me, as a mathematician, when I met Coq at first, it is difficult to accustomed with it.\nSo I have a question.\nIs it possible to make a proof assistant program based on ZFC?\nOr Can we make a setting on Coq so that we can do set theoretical approach on Coq?\nIf it is possible, then I think that many undergraduate mathematicians can use it easily and get many benefits from it.", "output": "There are three major foundations to theorem provers.  First order logic with set theory (usually something close to ZFC but maybe with inaccessible cardinals and/or better support for proper classes).  Higher order logic (a form of type theory).  Dependent type theory.  Mizar, Metamath (set.mm), Isabelle/ZFC all use the first.  HOL-light, HOL4, and Isabelle/HOL use HOL.  Coq, Lean, and Agda use DTT.\nSo this answers your first question.  Yes, it is possible to base a theorem prover on something like ZFC.  Also see Open source proof assistants for first order logic with equality and set theory.\nAs for taking a set theoretical approach in one of the later theorem provers, I am not sure I see the point.  But nonetheless, let me explain how I see the connections.  Pure HOL is the theory of a topos, so instead of the theory having set based models, it has topos based models.  But once one adds in the axiom of choice (which all HOL based provers do), then you can think of all the types as sets.  The rules of HOL plus the axiom of choice and the axiom of infinity are basically just another fairly natural way to describe set theory.  It may be a bit more like the category Set in category theory more than the models of ZFC in set theory, but still a form of set theory nonetheless.  (If I remember correctly, I think one also needs replacement to get equivalence with ZFC.)  In HOL (plus the various axioms), it is easy to write many theorems of set theory.\nAs for dependent type theory, again the base logic is more categorical (maybe describing various types of infinity categories or infinity topoi if I remember correctly).  It also works well as a syntax for a programming language and it has a nice computational interpretation.  Again if you add in the axiom of choice (as is common in Lean), your types behave like sets and you can think of it as a different form of set theory. Mario Carniero\u2019s master\u2019s thesis for example shows that Lean\u2019s logic is equi-consistent with ZFC plus countably many inaccessible cardinals.  It is certainly possible to do (meta) set theory in DTT like proving the independence the continuum hypothesis in ZFC.  (Also see https://proofassistants.stackexchange.com/a/1533/122.)\nAlso, if you are interested in the meta theoretic aspects of these three logics, see Jeremy Avigad\u2019s article on the Foundations of proof assistants.\nEdit: as for making it more accessible to mathematicians, I would argue that mathematicians rarely think about foundations (with the exceptions of some with more of a logical bent).  The biggest practical aspects are (1) usability of the systems and (2) being able to describe the mathematical objects and reasoning that one wants.  As for (1), that doesn\u2019t have much to do with foundations.  As for (2), I think a lot of mathematicians who got into proof assistants actually find type theory, especially dependent type theory, to be a very natural way to encode the mathematical objects they work with.  The biggest sticking point with foundations is the need for the axiom of choice.  In Coq it is common to avoid it if possible, but many mathematicians, in say Lean, use it freely.  That way one doesn\u2019t have to worry about constructive math, which while interesting and important, is very foreign to the working mathematician.", "meta": {"post_id": 2167, "input_score": 7, "output_score": 10, "post_title": "Is it possible to make a proof assistant program based on ZFC?"}}
{"input": "I'm unclear on the terms \"deep embedding\" and \"shallow embedding\".  I understand they are concerned with using one logic inside another, e.g. using Metamath/set.mm proofs in Lean or building Coq inside Coq.  I also get the impression that deep embeddings involve some sort of re-implementation, whereas shallow embeddings do not.\nI'm looking for the following:\n\nA good explanation of the differences between deep and shallow embeddings.\nAn example of each type.  (My impression is that most projects are deep embeddings, so in particular, I don't have a good example of a shallow embedding.)\n(Bonus) Whether each of the following projects are shallow or deep embeddings:\n\nCoq in Coq (Coq -> Coq)\nTranslating Dirichlet's theorem from Metamath to Lean (paper )(Metamath/set.mm -> Lean)\nCoq is a Lean Typechecker (Lean -> Coq)\nTowards self-verification of HOL Light\n (HOL Light -> HOL Light)\nSelf-verifying Metamath Zero (MM0 -> MM0/Peano)\nFlypitch (ZFC -> Lean)\nDedukti (? -> ?)\nLean4Lean (Lean -> Lean)\nMathPort (Lean 3 -> Lean 4)\nEncoding the HOL Light logic in Coq (HOL Light -> Coq)", "output": "The terms \"deep\" and \"shallow\" embedding are somewhat informal and have many variations. I will explain the essentials only.\nAn embedding from a source language $S$ to a target language $T$ is shallow if the constituent parts of $S$ are translated (mostly) directly to the corresponding parts of $T$. This is useful when we want to use $S$ but all we have is $T$. For example, if $S$ is a domain-specific logic and $T$ is a full-fledged proof assistant, we encode $S$ using the logic of $T$ and then use $T$'s theorem proving abilities to prove $S$-theorems. Even more specifically, suppose $S$ is the classical propositional calculus and $T$ is a proof assistant based on constructive type theory, equipped with a tactic for proving intuitionistic propositions. We can shallowly translate $S$ into $T$ using the double-negation translation and use $T$'s tactic to prove classical $S$-statements.\nAn embedding is deep if in $T$ we define (inductive) types that describe the syntax and inference rules of $S$. This is useful when we want to use $T$ to study $S$ as a formal system. For example, if $S$ is a programming language and $T$ is a full-fledged proof assistant, we can define in $T$ types that represent the syntax, the typing rules, and the operational semantics of $S$. This allows us to prove theorems about $S$, such as type safety. (In fact, this was one of successful early applications of proof assistant technology.)\nExample: simply-typed $\\lambda$-calculus\nSuppose $S$ is the simply typed $\\lambda$-calculus with the following parts:\n\nthe types of natural numbers nat\nfunction types, written a \u21d2 b\nconstructors zero and succ\n$\\lambda$-abstraction $\\lambda (x{:}A),\\, e$ and application $e_1 \\, e_2$.\n\nThere are also typing contexts and typing rules which I am not going to write here, as they are quite standard (and also formalized below as a deep embedding in Agda).\nShallow embedding\nThe following is a shallow embedding of the calculus in Agda. It is mostly just a collection of abbreviations, so that we can write nat instead of Agda's \u2115, and A \u21d2 B instead of Agda's A \u2192 B, etc.\n(Note: the embedding shown below is not as shallow as it could be, because we map the typing contexts of \u03bb-calulus to types in Agda. An even shallower embedding would map contexts to contexts, but that would be a bit confusing to beginners because most of the embedding would become invisible.)\nmodule Shallow where\n\n  open import Data.Nat\n  open import Data.Product\n  open import Data.Unit\n\n  -- The types of the \u03bb-calculus are Agda's types.\n  type = Set\n\n  -- The typing contexts are sets.\n  context = Set\n\n  -- The empty context is the unit type\n  \u2219 : context\n  \u2219 = \u22a4\n\n  -- Extending a context with type is Agda's cartesian product\n  infixl 5 _\u2a1f_\n  _\u2a1f_ : context \u2192 type \u2192 context\n  \u0393 \u2a1f A = \u0393 \u00d7 A\n\n  -- The \u03bb-calculus type nat is Agda's type \u2115\n  nat : type\n  nat = \u2115\n\n  -- The \u03bb-calculus function type is Agda's function type\n  infixr 7 _\u21d2_\n  _\u21d2_ : type \u2192 type \u2192 type\n  A \u21d2 B = A \u2192 B\n\n  -- The \u03bb-calculus typing judgement x : nat, f : nat \u21d2 nat, y : nat \u22a2 succ (succ x) : nat\n  example-1 : (\u2219 \u2a1f nat \u2a1f nat \u21d2 nat \u2a1f nat) \u2192 nat\n  example-1 (((_ , x) , f) , y) = suc (suc x)\n\n  -- The \u03bb-calculus typing judgement \u2219 \u22a2 (\u03bb (x y : nat) succ x) (succ (succ 0))\n  example-2 : \u2219 \u2192 nat \u21d2 nat\n  example-2 tt = (\u03bb (x y : nat) \u2192 suc x) (suc (suc 0))\n\n  -- The \u03bb-calculus typing judgement f : A \u21d2 A \u22a2 \u03bb (x : A) f (f x) : A \u21d2 A\n  example-3 : {A : type} \u2192 (\u2219 \u2a1f A \u21d2 A) \u2192 (A \u21d2 A)\n  example-3 {A} (_ , f) = \u03bb (x : A) \u2192 f (f x)\n\nWith the shallow embedding it is very easy to compute in \u03bb-calculus, just use Agda's normalization abilities to compute their normal forms. This is possible because our embedding makes sure that the \u03bb-calculus normalization and Agda's normalization coincide.\nDeep embedding\nHere is the corresponding deep embedding.\n\nmodule Deep where\n\n  -- The \u03bb-calculus simple types\n  infixr 7 _\u21d2_\n  data type : Set where\n    nat : type\n    _\u21d2_ : type \u2192 type \u2192 type\n\n  -- The \u03bb-calculus typing context, we use de Bruijn indices\n  infixl 5 _\u2a1f_\n  data context : Set where\n    \u2219 : context\n    _\u2a1f_ : context \u2192 type \u2192 context\n\n  -- A \u2208 \u0393 is the type of those de Bruijn indices whose type in \u0393 is A\n  infix 3 _\u2208_\n  data _\u2208_ : type \u2192 context \u2192 Set where\n    z : \u2200 {A : type} {\u0393 : context} \u2192 A \u2208 \u0393 \u2a1f A\n    s : \u2200 {A B : type} {\u0393 : context} \u2192 B \u2208 \u0393 \u2192 B \u2208 \u0393 \u2a1f A\n\n  -- term \u0393 A is the type of \u03bb-calculus terms of type A in context \u0393\n  data term : context \u2192 type \u2192 Set where\n    zero : \u2200 {\u0393} \u2192 term \u0393 nat\n    succ : \u2200 {\u0393} \u2192 term \u0393 nat \u2192 term \u0393 nat\n    var : \u2200 {\u0393 A} \u2192 A \u2208 \u0393 \u2192 term \u0393 A\n    lam : \u2200 {\u0393} A {B} \u2192 term (\u0393 \u2a1f A) B \u2192 term \u0393 (A \u21d2 B)\n    app : \u2200 {\u0393 A B} \u2192 term \u0393 (A \u21d2 B) \u2192 term \u0393 A \u2192 term \u0393 B\n\n  -- The \u03bb-calculus typing judgement x : nat \u2a1f f : nat \u21d2 nat\u2a1f y : nat \u22a2 succ (succ x) : nat\n  example-1 : term (\u2219 \u2a1f nat \u2a1f nat \u21d2 nat \u2a1f nat) nat\n  example-1 = succ (succ (var (s (s z))))\n\n  -- The \u03bb-calculus typing judgement \u2219 \u22a2 (\u03bb (x y : nat) succ x) (succ (succ 0)) : nat \u21d2 nat\n  example-2 : term \u2219 (nat \u21d2 nat)\n  example-2 = app (lam nat (lam nat (succ (var (s z))))) (succ (succ zero) )\n\n  -- The \u03bb-calculus typing judgement f : A \u21d2 A \u22a2 \u03bb (x : A) f (f x) : A \u21d2 A\n  example-3 : {A : type} \u2192 term (\u2219 \u2a1f A \u21d2 A) (A \u21d2 A)\n  example-3 {A} = lam A (app (var (s z)) (app (var (s z)) (var z)))\n\nIn the deep embedding the terms of the \u03bb-calculus are elements of the inductive type term. We cannot use Agda's normalization algorithm to compute with these, they're all in normal form already.\nHowever, with the deep embedding we can reason about \u03bb-calculus. For example, we can define a model of the \u03bb-calculus:\nmodule Semantics where\n\n  -- The set-theoretic model of \u03bb-calculus\n\n  open import Data.Nat\n  open import Data.Unit\n  open import Data.Product\n\n  open Deep\n\n  -- Types are interpreted as sets\n  \u27e6_\u27e7\u1d57 : type \u2192 Set\n  \u27e6 nat \u27e7\u1d57 = \u2115\n  \u27e6 A \u21d2 B \u27e7\u1d57 = \u27e6 A \u27e7\u1d57 \u2192 \u27e6 B \u27e7\u1d57\n\n  -- Contexts are interpreted as sets\n  \u27e6_\u27e7\u1d9c : context \u2192 Set\n  \u27e6 \u2219 \u27e7\u1d9c = \u22a4\n  \u27e6 \u0393 \u2a1f A \u27e7\u1d9c =  \u27e6 \u0393 \u27e7\u1d9c \u00d7 \u27e6 A \u27e7\u1d57\n\n  -- Auxiliary map, for looking up values of variables\n  lookup : \u2200 {\u0393 A} \u2192 A \u2208 \u0393 \u2192 \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  lookup z (_ , a) = a\n  lookup (s x) (\u03b7 , _) = lookup x \u03b7\n\n  -- A typing judgement \u0393 \u22a2 e : A is interpreted as a map \u27e6 e \u27e7 : \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  \u27e6_\u27e7 : \u2200 {\u0393 A} \u2192 term \u0393 A \u2192 \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  \u27e6 zero \u27e7 \u03b7 = 0\n  \u27e6 succ e \u27e7 \u03b7 = suc (\u27e6 e \u27e7 \u03b7)\n  \u27e6 var x \u27e7 \u03b7 = lookup x \u03b7\n  \u27e6 lam A e \u27e7 \u03b7 = \u03bb a \u2192 \u27e6 e \u27e7 (\u03b7 , a)\n  \u27e6 app e\u2081 e\u2082 \u27e7 \u03b7 = \u27e6 e\u2081 \u27e7 \u03b7 (\u27e6 e\u2082 \u27e7 \u03b7)\n\n  -- The interpretation of \u2219 \u22a2 (\u03bb (x : nat) . succ (succ x)) (succ zero) : nat\n  cow : \u27e6 \u2219 \u27e7\u1d9c \u2192 \u27e6 nat \u27e7\u1d57\n  cow tt = \u27e6 app (lam nat (succ (succ (var z)))) (succ zero) \u27e7 tt\n  -- Agda normalizes cow tt to the numeral 3\n\nYes, the model is also an interpreter for \u03bb-calculus. Welcome to constructive mathematics.\nThe laundry list\nI am not familiar with all the embeddings you listed in the question. Hopefully other will provide answers for the missing ones:\n\n\u201cCoq in Coq\u201d is deep. They want to prove things about Coq. A shallow embedding would be the identity map.\nDedukti is a logical framework, which makes it a bit special. Its purpose is formalization of formal systems. One defines the syntax and typing rules of the embedded system (deep embedding), but in such a way that Dedukti's contexts, variables, type-checking, normalization and rewriting abilities can be reused by the embedded system (shallow embedding).\n\u201cCoq is a Lean Typechecker\u201c is shallow. The web page even says: \u201cThe basic idea is to translate Lean Prop to Coq SProp, inductives to inductives, etc.\u201c\n\u201cTowards self-verification of HOL Light\u201d has a section called \u201cFormalized syntax\u201d so I will guess it is deep without reading the entire paper.\n\u201cLean4Lean\u201c is about implementing Lean in Lean, a self-interpreter. This is a deep embedding because it will involve parsing syntax and interpreting it into machine code. A shallow embedding of Lean in Lean is just the identity transformation.\n\u201cMathPort\u201d is a shallow embedding, because they tried as hard as possible to make it the identity transformation.\n\u201cEncoding the HOL Light logic in Coq\u201d is shallow.\n\nSupplemental: What about set theories?\nAn interesting question is whether emeddings of set theory into type theory are shallow or deep. Let us just focus on material set theory such as ZFC (although the story is much the same for structural set thoeries).\nA material set theory is typically a single-sorted first-order theory. In type theory we commonly find something like this:\nopen import Level\nopen import Data.Empty\nopen import Data.Product\nopen \u03a3\n\nmodule SetTheory (\u2113 : Level) where\n\n  -- The type of well-founded trees with branching type from universe level \u2113\n  data V : Set (suc \u2113) where\n    set : \u2200 {I : Set \u2113} \u2192 (I \u2192 V) \u2192 V\n\n  mutual\n    -- Membership relation\n    infix 5 _\u2208_\n    data _\u2208_ : V \u2192 V \u2192 Set \u2113 where\n      \u2208-set : \u2200 {I} {t} {u : I \u2192 V} \u2192 (\u03a3[ i \u2208 I ] t \u2248 u i) \u2192 t \u2208 set u\n\n    -- Subset relation\n    infix 5 _\u2286_\n    _\u2286_ : V \u2192 V \u2192 Set \u2113\n    set x \u2286 t = \u2200 i \u2192 x i \u2208 t\n\n    -- Equality of well-founded trees as sets\n    infix 5 _\u2248_\n    data _\u2248_ : V \u2192 V \u2192 Set \u2113 where\n      \u2248-set : \u2200 {t u : V} \u2192 t \u2286 u \u2192 u \u2286 t  \u2192 u \u2248 t\n\n  -- We check that (V, \u2208, \u2248) satisfies the set-theoretic axioms.\n  -- For illustration we only provide extensionality and empty set.\n  -- (Note that we cannot validate the powerset axiom, Agda is too predicative.)\n\n  data empty-type : Set \u2113 where\n\n  \u2205 : V\n  \u2205 = set {I = empty-type} \u03bb {()}\n\n  axiom-empty-set : \u03a3[ x \u2208 V ] \u2200 (y : V) \u2192 y \u2208 x \u2192 \u22a5\n  axiom-empty-set = \u2205 , \u03bb {y (\u2208-set ())}\n\n  axiom-extensionality : \u2200 {x y : V} \u2192 x \u2286 y \u2192 y \u2286 x \u2192 x \u2248 y\n  axiom-extensionality x\u2286y y\u2286x = \u2248-set y\u2286x x\u2286y\n\n  -- and so on\n\nI would call this a model of set theory in type theory. It is not an \"embedding\" in the sense of having a syntactic translation from one formal system to another.\nThis situation is completely analogous to other situations involving first-order theories, which is what a material set theory usually is. Take the theory of a group, for example. If we define a single group, say $\\mathbb{Z}_7$, then that is like defining a single model V of set theory, as above. We might define the structure of all groups\nstructure Group : Type where\n  carrier : Type\n  mul : carrier \u2192 carrier \u2192 carrier\n  inv : carrier \u2192 carrier\n  assoc : ...\n  unit-left : ...\n  unit-right : ...\n  inv-left : ...\n  inv-right : ...\n\nbut that would still just be semantics, as we're now just defining (the object part of) the category of all groups. We could to the same for set theories and define the category of all models of set theory, so not an embedding.\nIf you insist, you may call the above a shallow something-or-other of set theory into type theory (because we are using type-theoretic logic to validate set-theoretic axioms), but I hesitate to call it an \"embedding\" because it does not faithfully reflect provability of set-theoretic statements. That is, type theory might prove some set-theoretic statements about V which set theory does not prove (this is not an easily answered question, and it depends on the exact set-theoretic axioms and the exact type-theoretic rules).\nA deep embedding of set theory into type theory would begin with inductive types representing the first-order formulas and terms. But that's a different kind of animal that we would use if we wanted to prove meta-theorems about the first-order theory known as \"set theory\".", "meta": {"post_id": 2499, "input_score": 5, "output_score": 10, "post_title": "What is a deep embedding vs a shallow embedding? With examples?"}}
{"input": "I mainly work in the field of high-performance computing (HPC). For a 2020 paper I did a Monte Carlo calculation using 24800 CPUs, and one part of the paper was about showing that even up to 24800 CPUs, the speed of the calculation still roughly doubles when doubling the number of CPUs. To reach 24800 CPUs I had to use hundreds of compute nodes, with each node having 40 CPUs.\nI was therefore curious about the possible existence of \"high-performance\" proof assistants.\nWhen I did a literature search, I did find some papers about parallel implementations that run on several cores (CPUs) but nothing at a scale that I'd call large. Before this site went live, I asked about this in the Proof Assistants Chatroom on MathOverflow, and Hu\u1ef3nh Tr\u1ea7n Khanh told me that the proof assistant Lean does spawn multiple threads to do the calculations required to check proof correctness, but only uses shared memory parallelism meaning that it can run on multiple CPUs but not multiple nodes. .\nI wonder if there's any proof assistant software that can be run on not only mutliple cores of a single node, but also on multiple cores spread across multiple nodes?", "output": "Rolling with Cl\u00e9ment's distinction of concurrency, there is a third one:\n\nConcurrency when doing proof search and/or finding counter examples.\n\nIsabelle supports all three kinds of concurrency. However, and this is why this answer might not be satisfying for you, all these are almost embarrassing parallelism only:\n\n(1) is solved by realization that proofs are basically irrelevant, we only care that the certification succeeds at some point (which can be later in time). [1]\n(2) is solved by exploiting the DAG structure of a theory, checking independent theories in parallel. [1]\n(3) The tools sledgehammer (and try0) and nitpick, described in [2] and [3] respectively, leverage some forms of concurrency. The former will check different provers (CVC4, Vampire, z3 etc.) in parallel and reports which have succeeded. It can also make use of remote solvers SystemOnTPTP web service, you might find interesting techniques for some of those servers (I don't know enough to comment on that).\n\n\n1: Parallel Proof Checking in Isabelle/Isar by Makarius Wenzel\n2: Hammering Away: A User\u2019s Guide to Sledgehammer for Isabelle/HOL by Jasmin Blanchette et al.\n3: Picking Nits: A User\u2019s Guide to Nitpick for Isabelle/HOL by Jasmin Blanchette", "meta": {"post_id": 1, "input_score": 25, "output_score": 9, "post_title": "High-performance proof assistants"}}
{"input": "I want to get started with Lean 4, but I need to install it. The download page links to binaries to download, but I don't know what to do with them. Is there a \"best practice\" way to install Lean, or should I just download them and put them in my $PATH?\nI'm using Arch Linux (x86-64).", "output": "Install the latest Lean 4 nightly through elan: in any bash-compatible shell, run:\ncurl https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh -sSf | sh -s -- --default-toolchain leanprover/lean4:nightly\n\nOn Windows, instead run in cmd\ncurl -O --location  https://raw.githubusercontent.com/leanprover/elan/master/elan-init.ps1  powershell -f elan-init.ps1 --default-toolchain leanprover/lean4:nightly  del elan-init.ps1\n\n\nInstall VS Code.\n\nOpen VS Code and install the lean4 extension.\n\nCreate a new file with the extension .lean and add the following code:\n#eval Lean.versionString\n\nYou should get a syntax-highlighted file with a \"Lean Infoview\" on the right that tells you the installed Lean version when placing your cursor on the last line.\n\nYou are set up! You can now also run lake init foo from the command line to create a package, followed by lake build to get an executable version of your Lean program.\n\n\n\nNote: Packages have to be opened using \"File > Open Folder...\" for imports to work. Saved changes are visible in other files after running \"Lean 4: Refresh File Dependencies\" (Ctrl+Shift+X).\n\nNote: This information may be outdated. Find out more here.\n\nSource: Quickstart - Lean Manual", "meta": {"post_id": 2, "input_score": 8, "output_score": 10, "post_title": "How do I install Lean 4?"}}
{"input": "I keep bumping into references to \"Lean\", but then find there are two GitHub repositories for Lean:\n\nhttps://github.com/leanprover\nand\n\nhttps://github.com/leanprover-community\n\n\nCan someone explain the difference? Is one better than the other, or do they have different purposes?", "output": "https://github.com/leanprover is the official Lean organization, and contains the original works being built by Leo de Moura and his team. It also is in particular where development of Lean 4 is happening.\nhttps://github.com/leanprover-community is run by the broader Lean community, albeit in close contact with the Lean developers (some of whom contribute to repositories in both organizations). It hosts the community website and resources, but also Mathlib, which is in some sense Lean's \"killer app\" -- over a million lines of mathematics built by many mathematicians and contributors, and with extremely active continued work. As of July 2023, it has been fully migrated to Lean 4.\nThe leanprover-community repository also hosts a community website, including instructions for getting started with Lean 4 and Mathlib.", "meta": {"post_id": 4, "input_score": 21, "output_score": 33, "post_title": "What is the difference between `leanprover` and `leanprover-community` GitHub repositories?"}}
{"input": "I know that Proof Assistants can be used in fact-checking, but what else can they be used for? How would they be used for specific roles, including industry jobs and everyday life?", "output": "The seL4 microkernel (approx ~9000 lines of C) was verified in Isabelle/HOL. See this page for a discussion of just what this proof means (e.g., in terms of what assumptions underpin it). This code has been deployed by the likes of Collins Aerospace and Boeing as part of DARPA research projects.\nThe CakeML system is a verified compiler for a functional programming language that has proofs of correctness for every transformation from input strings to machine code, and bootstraps itself.", "meta": {"post_id": 6, "input_score": 16, "output_score": 10, "post_title": "What are some examples of proof assistants doing something useful?"}}
{"input": "Formal verification is\n\nthe act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.\n\nProof assistants can help with that; it's only a small jump from algorithms to computer source code. Let's say I have some code written in [insert the language of your choice here]: can I use proof assistants to verify whether it's free from bugs? You may assume it's a low-level language if that matters, but I expect it doesn't because almost all programming languages are Turing complete.", "output": "An important point to clarify is the meaning of \"free of bugs\". Correctness only makes sense relative to a specification. Only once you have decided what property you care about (your definition of \"correctness\"), can you start to think about how to prove it.\nFor safety properties (memory safety (buffer overflows, use after free, etc.), undefined arithmetic (overflow, divide by zero, etc.), unhandled exceptions, etc.), there are popular approaches to detect or prevent them automatically, via type systems (\"well-typed programs do not go wrong\") or program analyses (e.g., Infer).\nFunctional correctness is a class of properties that relate your program to a \"high-level\" specification. For example you may expect that a function int sum(int* x) does compute a sum $\\sum_i x_i$, and that allows you to think about that function as the mathematical sum without worrying about how the numbers are laid out in memory. Since there is no hope for full automation for the most expressive specification languages, this is an area where proof assistants have a lot of potential (and it has been active for decades).\nA most popular example is sel4, a microkernel written in C and verified in Isabelle/HOL. In the following excerpt, note again the precision of \"against its specification\":\n\nseL4's implementation is formally (mathematically) proven correct (bug-free) against its specification, has been proved to enforce strong security properties, and if configured correctly its operations have proven safe upper bounds on their worst-case execution times. It was the world's first operating system with such a proof, and is still the only proven operating system featuring fine-grained capability-based security and high performance. It also has the most advanced support for mixed criticality real-time systems.\n--- https://sel4.systems/About/\n\nFor another example, VST provides a program logic for C embedded in the Coq proof assistant. VST is particularly notable because it is formally related to a verified compiler, CompCert: there is a proof that VST's program logic is sound with respect to the same operational semantics that CompCert was proved against. Software Foundations, Volume 2 contains an introduction in Coq to Hoare logic, a core idea in the development of program logics for imperative languages, such as VST.\nFor a functional language such as Haskell, it is a lambda calculus at its core, similar to proof assistants based on type theory (Coq, Lean, Agda). hs-to-coq banks on that similarity, by translating functions into the proof assistant's language, so you can prove any properties expressible there.", "meta": {"post_id": 9, "input_score": 9, "output_score": 18, "post_title": "Could proof assistants be used to prove that some piece of code is free of bugs?"}}
{"input": "What was the first proof assistant? What was it used for? When and by who was it created? Is it still used today? And what was its purpose?", "output": "I regard \"proof assistant\" as synonymous with \"interactive theorem prover\", which rules out many of the systems mentioned in other answers. For example, the input to AUTOMATH was a deck of punched cards. AUTOMATH was a milestone but it was not a proof assistant.\nI believe that the Boyer/Moore theorem prover was interactive from the start, though it may have supported a batch mode as well. Commands consisted of axioms, requests to prove particular formulas, and hints concerning rewriting. Boyer and Moore published this work originally in 1975. And it continues to this day in the form of ACL2.\nAnother very early system was Weyhrauch's FOL. It was an interactive proof checker for first-order logic. I never saw the point even back in the 1970s, since FOL is both inexpressive and easily automated.\nThe first recognisably modern proof assistant is undoubtedly Edinburgh LCF, which introduced the architecture on which most of today's proof assistants are closely based, especially those of the HOL family. Mike Gordon traces the evolution here.", "meta": {"post_id": 17, "input_score": 25, "output_score": 13, "post_title": "What was the 1st Proof Assistant?"}}
{"input": "How does it compare to dependent records in Agda/Lean/etc.?\nI heard that locales in Isabelle are used to model algebraic structures in a talk, bu. Isn't this a kind of dependent type in some sense? Is Isabelle really simply typed?", "output": "Basically, a locale is a name for a set of fixed parameters, that come with assumptions. Inside the locale, these assumptions are added implicitly to everything you define/prove. When you have concrete parameters that satisfy the assumptions, you can interpret the locale, exposing the stuff inside (instantiated for your concrete parameters).\nIn Isabelle, locales are quite powerful. You can base your locales on other locales, partially instantiate them, rewrite stuff, etc. (for details https://isabelle.in.tum.de/dist/Isabelle2021-1/doc/locales.pdf)", "meta": {"post_id": 23, "input_score": 12, "output_score": 12, "post_title": "What are \"locales\" in Isabelle?"}}
{"input": "I found a site where I can use Coq in my browser: jsCoq Interactive Online System. Are there any major differences between the experience I'm getting on this site and the downloadable version(s)?", "output": "jsCoq is based on the Coq source code (regularly updated to use newer Coq versions), compiled to Javascript (or WebAssembly) to run in the browser using js_of_ocaml. The main differences with running Coq on the desktop is that jsCoq is slower and that the fast reduction machines (vm_compute and native_compute) are disabled. It also comes with a set of bundled libraries, which is similar in spirit to the Coq Platform, but different in terms of library choice and version numbers. Obviously, the GUI is also (very slightly) different to the various editors with Coq support that are out there.\njsCoq is developed at: https://github.com/jscoq/jscoq", "meta": {"post_id": 29, "input_score": 11, "output_score": 14, "post_title": "What are the differences between jsCoq and the versions of Coq that can be downloaded?"}}
{"input": "How would you explain to a beginner in simple terms, what a Proof Assistant is?", "output": "Let us clarify some jargon. Usually we call proof assistants systems which can really help us finding proofs, i.e. they should have a tactic language. Systems which lack this feature (tactics) are usually called type checkers or just type systems. On the other hand, in modular systems, kernels in proof assistants could also be called just type checkers. All of them could be called theorem provers as long as they have at least the hole inference and hole finding features.", "meta": {"post_id": 35, "input_score": 18, "output_score": 19, "post_title": "What is a Proof Assistant?"}}
{"input": "From the Wikipedia article on Proof Assistant it shows some Proof Assistants are based on Higher Order Logic, (HOL Light) and some are based on Dependent Types, (Coq).\nAre there any other means upon which to build a Proof Assistant? (Thinking MetaMath and ACL2).", "output": "There are a lot of bases, theories and techniques in proof assistants. Let me show you how deep the rabbit hole is (in suggested order of implementation):\n(Fibrational) Dependent Type Theories\n\nCoC (Calculus of Constructions, PTS, Coquand)\nCiC (Calculus of Inductive Constructions, Paulin-Mohring)\nMLTT (Martin-L\u00f6f)\nHoTT-I (Isaev)\nCubical (Cubical Type System, CCHM)\nHTS (Homotopy Type System with Two Equalities, 2LTT, Voevodsky, ACKS)\nde Rham (Infinitesimal Shape Modality, Cherubini, Groupoid)\nCartesian Cubical (ABCFHL)\nGuarded Cubical (BBCGSV)\nRezk (Synthetic Theory of $\\infty$-categories, Riehl, Shulman, Kudasov)\nEquivariant Super HoTT (Schreiber)\n\nThese are what we basically call Type Theories in Martin-L\u00f6f sense with 5 inference rules, some of them are monadic modalities. But on the market there are present proof assistants based on different theories, like custom model checkers or direct term rewriting systems. They internalize upper MLTT type systems into itself, like Dedukti. So the full answer to this question could be a good survey type PhD! Systems 1-7 are implemented as examples and included as parts of cubical.systems.\nEmbeddable DSL in host language\nBefore fibrational provers, the logic of computable functions was built upon relying on STLC flavours. In that sense you can write a needed type-checking rules in ML language as in meta theory and embed programs as strings instead of direct internalizations in the style of synthetic dependent type theories. This technique is not unique to HOL series, but is known as embeddable DSL in host-language technique. In that sense ACL2 is embeddable DSL in ANSI Common Lisp.\nAd-hoc dedicated model checkers and other LF\nEverything that is big enough or impractical to run in the main Proof Assistant environment usually is being rewritten into separate ad-hoc model checker for the sake of speed, simplicity, etc. This category could include all known models checking cores: Reference SMT solver Z3 by Leonardo de Moura, TLA+ by Leslie Lamport, even Metamath-like systems or set-theoretical systems like Mizar. I think if you built the basis only for Set theory but not all mathematics it is called ad-hoc. Logical Frameworks like Twelf or system with separate language for automation of inference rules like Andromeda are also could be treated as separate ad-hoc logical systems. Modern powerful ad-hoc term rewriting systems like Dedukti could eat 2LTT with less effort than HoTT in HOL by Josh Chen.", "meta": {"post_id": 36, "input_score": 32, "output_score": 29, "post_title": "What are the bases for different Proof Assistants?"}}
{"input": "How would you explain so a beginner in simple terms why Proof Assistants are important, and how / why they are used?", "output": "(A lightly edited copy and paste of my answer to a similar question on the Lean Zulip chat: What is the point of Lean's maths library.)\nIn my talk about Lean-GPT-f at Harvard, I gave five different motivations for formalized mathematics:\n\nMechanically check mathematical proofs.  While it doesn\u2019t yet make sense to have mathematicians formalize all of their work, it does make sense to formalize particularly foundational, interesting, or troublesome proofs, such as undergraduate mathematics, the independence of CH, or Scholze\u2019s lemma in the Liquid Tensor project.\nDigitize mathematics.  Everything nowadays is digitized, and systematically organized into databases.  However, mathematics is remarkably resistant to this trend.  Sure we have pdfs and LaTeX files of most articles, but none of these formats capture the mathematical content of the articles.  Formalization of a large body of mathematics, whether the full proofs or just the theorem statements, would open up many possibilities for automatic retrieval and search of mathematics.\nUnify and archive mathematical knowledge.  Right now the world of mathematics consists of a mostly disjoint collection of articles (most of them mostly true).  There have been past heroic efforts from Euclid to Bourbaki to systematically organize the current mathematical knowledge of the time.  More modern incarnations include Wikipedia, nLab, and the Stacks Project.  Formalization provides yet another approach to this, whereby we systematically build up a library of mathematics starting at the foundations, up to the most interesting new developments.\nProve the correctness of software and hardware. There have been some great success stories so far in software and hardware verification and I see a lot of potential for verified security software.  It is also the most likely way to monetize ones skills in formal theorem proving.  As the mathematics in formal libraries get more sophisticated, so do the types of algorithms that we can formalize.\nMaking mathematics accessible to computers in a new way.  I\u2019ve heard it said that the one thing mathematics is good for is more mathematics.  This \u201cfalse truism\u201d also holds for formalization.  We don\u2019t know the full applications of formal mathematics right now except that it will improve our ability to formalize more mathematics in the future.  However, we do have hints already at the ways that one can use a large library of formal mathematics.  AI projects like TacticToe, HOList, Tactician, and GPT-f are trained off of many person-years of formalization effort, and hopefully will eventually become a virtuous loop, helping users better formalize more mathematics.  It\u2019s hard to predict what we can do with a large library of formalized, computer-readable, interconnected mathematics, but it is very reasonable to believe it could have important positive future impacts on mathematics, technology, science, and education.", "meta": {"post_id": 57, "input_score": 19, "output_score": 19, "post_title": "Usage and Importance of Proof Assistants"}}
{"input": "Related to What is the easiest proof assistant to start with? \u2013 I suppose familiarity with programming in general would be beneficial to learning how to work with proof assistants. But which kind of programming languages (used in 'regular' programming, think the top 100 tags on Stack Overflow) would be most helpful? Do functional programming languages offer benefits over object-oriented or procedural ones?", "output": "An old (vintage / classic) functional language that has been specifically designed for proof assistants is the Standard ML language. Quoting the book ML for the Working Programmer\n\nEvery successful language was designed for some specific purpose: Lisp\nfor artificial intelligence, Fortran for numerical computation, ... ML was designed for theorem proving.\n\nThe same book even contains a chapter devoted to writing a FOL tactical theorem prover from scratch.", "meta": {"post_id": 60, "input_score": 14, "output_score": 8, "post_title": "Which programming languages are most similar to proof assistants?"}}
{"input": "The Natural Number Game suggests the possibility of rewriting mathematics textbooks with embedded interactive Lean code. I'm envisioning something like Google Colab or Mathematica, where you can insert either code blocks or text/LaTeX blocks.\n\nQuestion: Is there already a good package for doing this?\n\nIf there were one, and it were available readily, this would accelerate the interface between mainstream mathematics and proof assistants. (Imagine classic mathematics texts rewritten with embedded code exercises.)\nThe best so far, to my knowledge, Patrick Massot's formatter discussed here, and maybe this is good enough? I'd be interested in knowing what is being done, and collecting the results here.\n(Maybe this is CW?)", "output": "There is also Patrick Massot's https://github.com/PatrickMassot/leanblueprint which is powering\n\nhttps://leanprover-community.github.io/sphere-eversion/blueprint/\nhttps://leanprover-community.github.io/liquid/\nhttps://b-mehta.github.io/unit-fractions/blueprint/index.html\n\nThis is not so much a package that formats Lean code into LaTeX, but it is certainly helping to\n\naccelerate the interface between mainstream mathematics and proof assistants.", "meta": {"post_id": 62, "input_score": 26, "output_score": 8, "post_title": "Is there software for interfacing Lean code with LaTeX?"}}
{"input": "In Lean, explicit function arguments are enclosed in round brackets and implicit ones in curly brackets, as in this example:\nlemma surjective.comp {g : \u03b2 \u2192 \u03c6} {f : \u03b1 \u2192 \u03b2} (hg : surjective g) (hf : surjective f) :\n  surjective (g \u2218 f) := ...\n\nHowever, occasionally we see some strange double curly brackets like this:\ndef injective (f : \u03b1 \u2192 \u03b2) : Prop := \u2200 \u2983a\u2081 a\u2082\u2984, f a\u2081 = f a\u2082 \u2192 a\u2081 = a\u2082\n\nI'm sure I once read an explanation of this, but my attempts to search for it now have failed.  What do the double curly brackets mean?", "output": "Single braces{\u22ef} indicate a maximally inserted implicit argument and {{\u22ef}} a weakly inserted implicit argument, as explained in the manual.\nConsider the definition of the identity map in which the first argument is implicit:\ndef id {\u03b1 : Type u} (x : \u03b1) : \u03b1 := x\n\nIn an expression such as id 3 Lean will figure out the implicit argument \u03b1 to be \u2115. But what will it do when faced just with id, not applied to anything? There are two options:\n\nIt could create a new meta-variable ?M (also known as an existential variable) and convert the lone id to id ?M. The value of ?M will be determined later as more information becomes available. This is known as a maximally inserted implicit argument, because it is inserted whenever possible. The above definition does so.\n\nIndicate that the argument should not be maximally inserted by using {{\u22ef}} instead:\ndef id {{\u03b1 : Type u}} (x : \u03b1) : \u03b1 := x\n\nNow, when faced with id, Lean will read it precisely as id without inserting anything. This strategy is known as weakly inserted implicit arguments.\n\n\nLet's try it out:\nuniverse u\n\ndef id\u2081 {\u03b1 : Type u} (x : \u03b1) : \u03b1 := x\n\n#check id\u2081\n-- output: id\u2081 : ?M_1 \u2192 ?M_1\n\ndef id\u2082 {{\u03b1 : Type u}} (x : \u03b1) : \u03b1 := x\n\n#check id\u2082\n-- output: id\u2082 : \u03a0 \u2983\u03b1 : Type u_1\u2984, \u03b1 \u2192 \u03b1\n\nIn most cases the difference between the maximally and weakly inserted arguments does not matter. But when it does matter it becomes very annoying if only the wrong one is available. For example, the OP shows the definition of injective map using {{\u22ef}}. They are there, because with {\u22ef} the expression injective f would mean injective f ?a\u2081 ?a\u2082 which reads \u201cfor some fixed but yet to be determined a\u2081 and a\u2082, if f a\u2081 = f a\u2082 then a\u2081 = a\u2082\". That's not what \u201cf is injective\u201c means.", "meta": {"post_id": 66, "input_score": 30, "output_score": 31, "post_title": "In Lean, what do double curly brackets mean?"}}
{"input": "I am trying to update a large body of Lean code to work with the current version of mathlib.  This is difficult because there have been a large number of backwards-incompatible changes even since I last did this in mid-November.  When I come across something that no longer works, I guess some keywords and try to search for commits or issues or current code in github that explains what has changed and why.  Is there any recommended method that is more systematic than this?", "output": "The mathlib commit log serves as a de facto changelog. Since the project updates far too frequently to keep a traditional changelog, a good method if you know approximately when breaking changes occured is to look at commits around that date. Titles are hopefully descriptive, and link to the pull request which explains the change in more detail.\nThe leanprover-community organization hosts CI for third-party projects, including a GitHub Actions script that will try to update your project periodically. If changes in mathlib break your code, it will open an issue on your repository linking to a diff of the mathlib changes that may be to blame.\nNote that this action is most helpful if you fix the breakage relatively quickly. Breaking changes can compound, and it can only alert you at the first breaking change.", "meta": {"post_id": 71, "input_score": 22, "output_score": 17, "post_title": "How to track backwards-incompatible changes in mathlib?"}}
{"input": "The most well-known tutorials/learning materials, including Book 1, Chapter 2 of Software Foundations for Coq and the Natural Number Game for Lean, use natural numbers to introduce induction. But there are multiple complaints by new learners that having to prove trivial facts of natural numbers via induction feels unintuitive and intimidating (one example).\nHas there been any attempt at a tutorial that doesn't start with natural numbers? I can imagine using monomorphic lists instead, say a list of booleans. For example, proving associativity of list concatenation (a ++ b) ++ c = a ++ (b ++ c) should feel less intimidating (because it is no longer a \"trivial\" property of natural numbers the students would have taken as granted) and more interesting than proving associativity of addition on natural numbers. And then the properties of natural numbers could be introduced by pointing to similarities with list operations.", "output": "Joseph Hua, Ken Lee, and Bendit Chan have recently released the HoTT Game, designed as an introduction to Cubical Agda for mathematicians with no proof verification experience. The two parts of this tutorial are Trinitarianism (nicely summarized here) which introduces all the basic notions of (homotopy) type theory, and Fundamental Group of the Circle, whose goal is showing that $\\pi_1(S^1)\\cong\\mathbb Z$.\nGranted, at some points working with $\\mathbb Z$ reduces to working with (two copies of) $\\mathbb N$, but unlike the tutorials mentioned in the original post, the theorems of this tutorial are not about arithmetic of $\\mathbb N$.", "meta": {"post_id": 103, "input_score": 18, "output_score": 10, "post_title": "Has there been any attempt at a tutorial that doesn't start with natural numbers?"}}
{"input": "Most proof assistants (with some exceptions like Isabelle/ZF or the B method) rely on type theory.\nSee also the MathOverflow question What makes dependent type theory more suitable than set theory for proof assistants?\nNext, formalizing mathematics is one of the main use cases of an interactive proof assistant.\nBut many pen-and-paper mathematical proofs are written in a set-theoretic fashion (e.g., in ZFC).\nFinally, there are many different type theories (first-order, higher-order, dependently-typed or not, intuitionistic or not, etc.)\nSo, formalizing existing set-theoretic results typically involves some formal hindrances.\nKeeping the constructiveness issue apart, and focusing on the definability of mathematical objects:\n\nwhat mathematical concepts can't typically be \"formalized as is\", within the main proof-assistants / type-theories?\n(e.g., we can think about matrices in a non-dependently-typed theory\u2026)\nis a \"workaround\" possible, to still be able to formally deal with this mathematical concept in the considered theory? (if yes, feel free to cite a formalization example)", "output": "Andrej's answer that \"Almost no pen-and-paper mathematics is written in ZFC\" is correct.  But it's perhaps also worth noting that some pen-and-paper mathematics is written in ZFC (or, at least, something closer to ZFC than to type theory), such as much of the mathematics done by set theorists (in which phrase, of course, \"set\" refers traditionally to a ZFC-set).\nA good deal of set theory can be rephrased more structurally to make sense in type theory, but this would be a rephrasing, and the amount of work that would be required is, I think, variable.  For instance, set theorists work with ordinals defined in the von Neumann way, with $\\in$ as the well-founded relation.  This in particular implies that ordinals are literally unique representatives of isomorphism classes of well-ordered sets.  In type theory it is more natural to work \"structurally\" with arbitrary well-ordered sets rather than with von Neumann ordinals.  This makes some things more cumbersome because of isomorphisms that get carried around (although under univalence, these isomorphisms become equalities), but probably doesn't change things a whole lot.\nOn the other hand, as far as I know, no one has ever come up with a convincing structural presentation of Godel's constructible universe $L$.  The only structural approaches to $L$ that I know of essentially involve building a model of ZF(C) in some way and then constructing $L$ inside that model.  Note that $L$ is, as far as I know, still the only way to prove the relative consistency of the Axiom of Choice, so it is of interest even to mathematicians who don't care about ZF for its own sake.  (In contrast, the symmetric/permutation methods used to prove relative consistency of not-AC, and the forcing methods that can be used to prove consistency of other properties such as GCH and not-GCH, seem much more amenable to structuralization.)  [Note: this paragraph was edited to remove a false claim that GCH is not forceable; see comments.]\nAs another example, Scott's trick is a method used in pen-and-paper mathematics that is of interest beyond set theory, but relies on the axiom of foundation from ZFC, and so is not obviously formalizable in type theory.", "meta": {"post_id": 107, "input_score": 26, "output_score": 10, "post_title": "What set-theoretic definitions can't easily be formalized in a type theory?"}}
{"input": "Conor proposed a seemingly promising way towards universe polymorphism: https://mazzo.li/epilogue/index.html%3Fp=857&cpage=1.html IMHO, the idea behind this proposal is:\n\nYou build things starting at level 0 and going up to whatever constant level you need, but then you can lift the whole thing up any number of other levels.\n\nHowever, it seems to be not implemented in any system except redtt. Is it flawed somehow, or it's just people are unaware of its existence?\nIs it sufficiently powerful? Is it formalized?", "output": "I don\u2019t know for sure, but I don\u2019t think this has been explored, no. At least I can say for sure it\u2019s not around in Coq\u2019s world (yes, time has passed and Coq now has universe polymorphism), although I think the way Coq does things subsumes this.\nBasically, how polymorphism works in Coq is that when you have a (polymorphic) definition, the system collects all constraints the universe levels in the definition need to satisfy for it to be well-typed. When you use it later on, the definition gets instantiated with new levels, and the system checks at that point that the constraints on those are satisfiable.\nThis is quite similar to choosing the right \"universe lifting\" as McBride proposes, but there\u2019s one lack of flexibility in the proposal: the relation between the universes remains fixed, while in the constraint-based approach you also can make the \"distance\" between the universes in the definition vary. I'm really not sure how annoying that might get in practice, but it might bite, requiring to duplicate definitions with different distances (rather than at different levels). On the other hand, universe constraints in Coq are becoming a performance bottleneck in quite a few places, so this might be a way to mitigate those\u2026", "meta": {"post_id": 113, "input_score": 15, "output_score": 9, "post_title": "What happened to Conor McBride's universe polymorphism proposal?"}}
{"input": "I recall many years ago seeing a very large and well-interlinked (by computer) list of verified results starting from base assumptions and leading to all sorts of things that naive me did not expect computers to be able to handle e.g. measure theory. I don't recall what software was used, and certainly don't see any sort of link on e.g. https://leanprover.github.io/.\nWhere can I find these types of lists?", "output": "Most of the main theorem provers have central libraries of formal mathematics.\nHere I've including their official statistics (accurate as of 2022-02-10) and more importantly a link to where you can find up-to-date statistics.\n\nIsabelle - Archive of Formal Proofs (AFP) (statistics)\n\n193,100 lemmas\n661 articles\n423 authors\n3,324,900 lines of code\n\n\nLean - mathlib (statistics)\n\n77,316 theorems\n32,040 definitions\n2,436 files\n221 contributors\n783,799 lines of code\n\n\nMizar - Mizar Mathematical Library (MML) (statistics)\n\n62,163 theorems\n12,840 definitions\n1,357 articles\n263 authors\n\n\nMetamath - set.mm (enumerated list)\n\n29,300 axioms, definitions, and theorems\n\n\nHOL Light - HOL Light repo (no statistics)\nCoq - Coq Standard Library\n\nUnlike the others, Coq doesn't have a central mathematics library that I'm aware of, although it has a lot of mathematics.  There are also no official statistics that I can find.", "meta": {"post_id": 141, "input_score": 27, "output_score": 11, "post_title": "Where can I find lists of theorems that have been verified?"}}
{"input": "The basics of model theory contain, as far as I understand, some theorems that are difficult to prove in their full generality.\nFor example, the compactness theorem in FOL for languages with arbitrarily many symbols was proven by Maltsev in 1936. G\u00f6del originally proved the countable case in 1930.\nTrying to formalize model theory and make a usable library in a proof assistant seems like it would be pretty tough, since you have to make some awkward foundational choices. What exactly is $\\models$? It's probably too big to be a set.\nI'm wondering if there are any proof assistants out there that have a model theory library or ones that were specifically designed to tackle this area (or do other similar things, like study set-based semantics of non-classical logics with a classical set theory in the background).", "output": "First, recall that model theory (of FOL) is just mathematics about certain types of mathematical objects, \"structures\" as they are defined in model theory.  So it isn't really any more difficult than working with say groups or topological spaces.  (I'm probably brushing too much under the rug, but at least Lean and Coq, since they have universes, they should avoid the \"set\" issue you mention.  Also, I think the technical term for how model theory is usually handled is \"deep embedding\", but I'm not sure I could give a good definition off hand.)\nHOL-Light\nJohn Harrison formalized FOL model theorem in HOL-Light. The paper is Formalizing basic first order model theory. Some code I've found is some theorems on logic up the the compactness theorem, and this folder about the logic of arithmetic which I think also has other logic stuff including some completeness theorem results (and maybe also the incompleteness theorem).  This is another completeness theorem possibly for a different logic.\nOther theorem provers\nAlso see the Lean project A formalization of forcing and the unprovability of\nthe continuum hypothesis.  Besides being the type of work I think you are interested in, it also has a good background section which I'll quote here:\n\nFirst-order logic, soundness, and completeness\nThere are many existing formalizations of first-order logic. Shankar [39] used a deep embedding of first-order logic to formalize incompleteness theorems. Harrison gives a deeply embedded implementation of first-order logic in HOL Light [18] and a proof-search style\naccount of the completeness theorem in [19]. Margetson [33] and Schlichtkrull [34] use\nthe same argument for the completeness theorem in Isabelle/HOL, while Berghofer [6] (in\nIsabelle) and Ilik [22] (in Coq) use canonical term models.", "meta": {"post_id": 144, "input_score": 11, "output_score": 11, "post_title": "Proof assistants or libraries in proof assistants for working with model theory"}}
{"input": "Coq and Lean are two of the most common proof assistants out there (but the question of course applies to other proof assistants too).\nWhat are the main differences between Coq and Lean? Ideally it would be nice to know the differences for a mathematician not interested in foundations and for someone more involved in CS and type theory.", "output": "It it hard to write an answer here which is not just a technical list of differences in specification, but also avoids the flame war of \"my theorem prover is better than yours\".  (I'd be happy to delete or change this answer if it comes across too flame-war-ish or incorrect.)\nLean is essentially classical, meaning it usually uses the law of excluded middle and the axiom of choice for proofs when it is convenient.  This is mostly a social norm, in that this is the standard of the Lean community and main math library, mathlib.  But it is also technical in a sense in that it is difficult to avoid classical reasoning with the common tactics (and basically impossible to do homotopy type theory even within term proofs).  For many just interested in formalizing pure math (except of course, say, homotopy type theory or the internal logic of some topos) this is fine and maybe preferable.  If you want more flexibility on axioms or want to be able to extract algorithms from your proofs it is a problem.  (However, some projects in Coq also use classical reasoning, so it isn\u2019t like it is forbidden.  It just might mean that projects which use classical reasoning won't be used by other projects which are trying to stay constructive.)\nThere is also a difference in community goals.  I can't speak to the goals of the Coq community, but it seems to me that the main goal of the Lean3 community is to add as much mathematics as they can to the mathlib library, the main mathematical library of Lean.  There is a strong emphasis on unity and cohesion in this library.  It is constantly refactored to make it work well together.  This (according to Lean users) makes it possible to easily combine separate areas of mathematics like rings and topological spaces (to get topological rings), but it also means that any project which is not in mathlib or trying to stay up-to-date with mathlib will quickly bit-rot.  (Even Lean3 itself---at least the community edition most commonly used---changes rapidly as well, sometimes in backward incompatible ways.)\nWe will see in Lean4 if the focus is the same or changes.  Already, many are using Lean4 as a very capable programming language, so there may be more diversity of projects and less focus on cohesion, but nonetheless, I think the contributors of mathlib would like the Lean4 version to still be highly cohesive.\nThere is also a lot written on the internet about say Lean vs Coq.  Of course note that since Lean and (to a lesser degree) Coq are constantly changing, some of that information may be out of date.  (For example, there was a time when Lean 2 supported both classical and constructive logic, even homotopy type theory.)", "meta": {"post_id": 153, "input_score": 58, "output_score": 33, "post_title": "What are the main differences between Coq and Lean?"}}
{"input": "Isabelle/HOL\u2019s logic is called a logic of \u201ctotal functions\u201d, yet there are functions that are obviously not total, such as division div.\nHow does this make sense?", "output": "If you have a Isabelle/HOL function f :: a \u21d2 b between two types a and b (NB: the function arrow in Isabelle is \u21d2, not \u2192), then \u2013 by definition of what it means to be a function in HOL \u2013 whenever you have a value x :: a, then the expression f x (i.e. f applied to x) is a value of type b. Therefore, and without exception, every Isabelle function is total.\nThis has a few consequences when modeling functions that are commonly considered partial.\n\nIn some cases, like subtraction on naturals, the function is made artificially total by returning a particular value: 0 - 1 = 0.\nThis may look fishy at first, but it is ok (theorems about subtraction that don\u2019t hold for the extended version will have necessary preconditions) and useful (a fair number of theorems do hold, and can be used without having to discharge such preconditions).\n\nIn other cases the partial function takes on an arbitrary, but unspecified and unknown value.\nConsider the Eps :: \"('a \u21d2 bool) \u21d2 'a\" operator, which models choice. When the predicate P is true for some x, then Eps P is one of these, and P (Eps P) holds (\u2203x. P x \u27f9 P (Eps P) is a theorem). But nothing stops you from using Eps with a never-true predicate, e.g. Eps (\u03bb n, n = n + 1) :: nat. This is a value of type nat, but you don\u2019t know which one, and no useful theorems are provided for it.\nSimilarly, the undefined :: 'a term is a value of any type, and again one that you don\u2019t know anything about.\n\nRelatedly, in HOL all types are inhabited \u2013 for example by the undefined above. This is crucial for the tricks shown above, and otherwise not a big deal either \u2013 in contrast to Logics based on Type Theory, which conflate types with propositions, and inhabited types with theorem.\n\n\nI wrote a longer text on partiality and definedness of Isabelle/HOL function.", "meta": {"post_id": 154, "input_score": 18, "output_score": 17, "post_title": "How can all functions be total in Isabelle?"}}
{"input": "I think in NBE, you get rid of certain substitutions, and it solves the problem of binding representation (so you don't have to use indices or capture-avoiding substitution or something). But I believe there are other differences I'm not aware of.\nIn particular, I'm interested in elaboration of dependently typed lambda calculus, where nbe can be used to calculate the normal forms of core terms.", "output": "When I asked this question of some people with experience implementing proof assistants, their answer was \"eta-laws\".\nIf you test equality using only some kind of reduction algorithm, then generally speaking your equality will only incorporate beta-reductions such as $(\\lambda x. M)(N) \\equiv M[N/x]$.  Eta-equivalences such as $M \\equiv (\\lambda x. M x)$ are quite difficult to implement with a reduction-only algorithm.  There are various reasons for this, such as the fact that if you want to reduce $\\lambda x. M x$ to $M$ you need to check that $x$ doesn't occur in $M$, and if you want to expand $M$ to $\\lambda x. M x$ you need to have type information present to know that $M$ has a function-type.  I won't say it's impossible, but normalization-by-evaluation is a clean, generalizable, and easy-to-reason-about family of algorithms that perform both $\\beta$-reduction and $\\eta$-expansion in a type-directed way.\nTo be sure, I believe this answer depends on a somewhat broader meaning of \"normalization by evaluation\" than is sometimes used.  For instance, this property doesn't depend on the representation of values of function-type as actual metalanguage functions; it's sufficient to defunctionalize them and we can still call that \"normalization by evaluation\" for this purpose.\nIn addition, one doesn't have to check $\\beta\\eta$-equality by first normalizing to a $\\beta\\eta$-normal form and then doing an $\\alpha$-equivalence check: one can incorporate the two $\\beta$ and $\\eta$ stages directly into a \"bidirectional\" equality-checking algorithm.  But for purposes of this answer (which is, again, not one that I invented, but was given by those in the know) we also consider that to be a form of \"normalization by evaluation\".", "meta": {"post_id": 196, "input_score": 18, "output_score": 10, "post_title": "What are the advantages of normalization by evaluation over traditional reduction-based normalization?"}}
{"input": "I started programming a Haskell-clone recently (source code).\nAfter basic hindley-milner, problems start to arise. Implementation of data/codata statements seem to ensure my language is breaking up on every front.\nAt this point I seem to lack foundations for the language. System F introduces kinds, but would it be better to approach theorem provers and pick a different calculus as foundation?\nAlso looking for overall suggestions for this kind of a project.", "output": "If you are asking \"can we use proof assistants to develop foundations of programming languages\" then the answer is positive. Two well-known such developments are:\n\nSoftware foundations by Benjamin Pierce, implemented in Coq.\nProgramming language foundations in Agda by Phil Wadler, implemented in Agda.\n\nThere are also specialized proof assistants that make it easier to develop the meta-theory of a programming language, such as Twelf, Abella and Beluga.", "meta": {"post_id": 205, "input_score": 15, "output_score": 12, "post_title": "Can theorem provers be used to form foundations for programming languages?"}}
{"input": "One could define streams in the following manner\nSection stream.\n  Variable A : Type.\n\n  CoInductive stream : Type :=\n  | Cons : A -> stream -> stream.\nEnd stream.\n\nBut as far as I can tell, such a coinductive stream is isomorphic to a function nat -> A. Arguably, working with the coinductive definition is harder.\nWhat is an example of a situation where working with a coinductive type is a more intuitive option to express a certain object? When is it more convenient to use a coinductive proof? What are some instances when the coinductive proof/value makes the most sense?", "output": "First a note: you are using the syntax for so-called \"positive\" coinductive types, which makes them look like inductive types, defined by \"constructors\".  This has various problems: it's not well-justified semantically, and syntactically it breaks \"subject reduction\".  Modern versions of Coq also support \"negative\" coinductive types, which look like record types rather than inductive types, defined by \"fields\", and are much better-behaved, so it is suggested to use those instead.  See the reference manual.\nAs for the actual question, I believe one place that coinduction is valuable is when computational behavior matters.  An infinite list represented by a function nat -> A is \"random access\": you can ask it for f n whenever you wish, and it doesn't in principle need to know anything about f k for k < n to compute f n.  Whereas the elements of a coinductive stream must be computed in order.  This certainly makes the function-representation easier to reason about.\nHowever, if your \"infinite list\" has the property that in practice we have to compute all the f k for k < n in order to find f n, then working with the coinductive version respects this, and encourages the user to take account of it.  Given a stream we can turn it into a function where f n is computed by pulling off n elements of the stream and returning the last one, but this performs a lot of computation if n is large, and if we call f a lot of times it could be very inefficient.  A coinductive data representation encourages the writing of corecursive functions, which force us to deal with this kind of stepwise-computed list more efficiently.\nStreams are arguably a bit too simple for this behavior to be clearly visible.  I like to think of a general coinductive type as a \"server\", in line with Conor McBride's comment in Let's see how things unfold:\n\nAs a total programmer, I am often asked 'how do I implement a server as a program in your terminating language?', and I reply that I do not: a server is a coprogram in a language guaranteeing liveness.\n\nAn element of a coinductive \"server type\" represents the server in a particular state.  When we call its destructor, we pass it some request from a client, and it returns a response to the client and also the new state of the server.  By contrast, using the corresponding \"functional\" representation, in order to compute the response of the server to any query we would have to replay the entire history of interaction of all clients with the server up to that point.  Clearly this is inefficient, and also a bad representation of how a real-world server actually works.\nCoinductive types can also be valuable in situations where type dependency makes it tricky to even come up with a functional representation, and if you do come up with one it's more awkward.  I'm most familiar with this in the case of homotopy theory and higher category theory.  For instance, a \"globular set\" can be defined coinductively as a set $A$ together with a family of globular sets indexed by $A\\times A$:\nCoInductive GSet : Type := {\n  carr : Type;\n  hom : carr -> carr -> GSet;\n}.\n\nThe corresponding functional representation is much less clear.  This can be enhanced to deal with infinite-dimensional categories in a surprisingly clean way.", "meta": {"post_id": 208, "input_score": 18, "output_score": 18, "post_title": "When to use coinductive types?"}}
{"input": "In the Lean mathlib, I see some places where a typeclass argument is included in a class definition, such as locally_finite_order. In other places, I see the \"extends\" keyword used, such as ring. What are the differences and tradeoffs between these two ways of doing things?", "output": "Section 2.2 of Anne Baanen's recent paper Use and abuse of instance parameters in the Lean mathematical library gives a very nice explanation of this, referring to them as \"unbundled subclasses\" (like locally_finite_order) and \"bundled subclasses\" (like ring). This paper discusses, in various places, situations where we might prefer one over the other.\nOne example is in section 5.1. Defining a class\nclass module (R M : Type) extends add_comm_monoid M := ...\n\nwould automatically create an instance that says, \"in order to show add_comm_monoid T, it suffices to show module ?x T.\" Instances like this can lead to huge performance issues, so the preferred pattern here is the unbundled definition:\nclass module (R M : Type) [add_comm_monoid M] := ...\n\nIn the other direction, overuse of unbundled subclasses can lead to performance problems of a different kind (Baanen section 10, or a blog post by Ralf Jung). Full unbundling leads to exponential blowup in the number of type class arguments (and thus searches) needed as you climb a hierarchy of classes. Baanen gives the example:\ninstance prod.comm_monoid\n  [has_one M] [has_one N] [has_mul M] [has_mul N]\n  [semigroup M] [semigroup N] [mul_one_class M] [mul_one_class N]\n  [monoid M] [monoid N] [comm_semigroup M] [comm_semigroup N]\n  [comm_monoid M] [comm_monoid N] :\ncomm_monoid (M \u00d7 N) := ...", "meta": {"post_id": 209, "input_score": 16, "output_score": 13, "post_title": "Extends vs including a typeclass argument"}}
{"input": "Here is a stripped down version of something that is causing me trouble.\nvariables {\u03b1 : Type*} [decidable_eq \u03b1]\n\ndef S (a : \u03b1) : set \u03b1 := \u03bb b, b = a\n\ndef P (a : \u03b1) : decidable_pred (\u03bb (c : \u03b1), c = a) := \n by { apply_instance, }\n\ndef Q (a : \u03b1) : decidable_pred (\u03bb (c : \u03b1), c \u2208 (S a)) := \n by { simp[S], apply_instance, }\n\n\nLean is happy with the definition of P, but rejects the definition of Q as follows:\ntactic.mk_instance failed to generate instance for\n  decidable_pred (\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a)\nstate:\n\u03b1 : Type u_1,\n_inst_1 : decidable_eq \u03b1,\na : \u03b1\n\nIn both P and Q, we ask tactic.mk_instance to generate a decision procedure for a predicate.  The predicate in Q is equivalent under \u03b2-reduction to the one in P, but tactic.mk_instance succeeds with P but fails with Q.  Is this just a weakness in the tactic or is there some more subtle issue?  Is there some straightforward way to work around this kind of problem?", "output": "The predicate in Q is equivalent under \u03b2-reduction to the one in P\n\nNot according to Lean in this case. set.mem is a def, which are not unfolded during typeclass inference by default. Thus inference fails at unifying it with =:\nset_option trace.type_context.is_def_eq_detail true\n\n[class_instances]  class-instance resolution trace\n[class_instances] (0) ?x_0 a_1 : decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) := _inst_1 (?x_1 a_1) (?x_2 a_1)\n[type_context.is_def_eq_detail] [1]: decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) =?= decidable (?x_1 a_1 = ?x_2 a_1)\n[type_context.is_def_eq_detail] [2]: (\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1 =?= ?x_1 a_1 = ?x_2 a_1\n[type_context.is_def_eq_detail] after whnf_core: a \u2208 \u03bb (b : \u03b1), b = a_1 =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] [3]: @set.mem \u03b1 a (\u03bb (b : \u03b1), b = a_1) =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] [4]: set.mem =?= eq\n[type_context.is_def_eq_detail] on failure: set.mem =?= eq\n[type_context.is_def_eq_detail] on failure: @set.mem \u03b1 a (\u03bb (b : \u03b1), b = a_1) =?= ?x_1 a = ?x_2 a\n[type_context.is_def_eq_detail] on failure: decidable ((\u03bb (c : \u03b1), c \u2208 \u03bb (b : \u03b1), b = a) a_1) =?= decidable (?x_1 a_1 = ?x_2 a_1)\nfailed is_def_eq\n\nWe can fix this with either local attribute [reducible] set.mem or simp[S, set.mem].\nThis still begs the question, why is set.mem (and others, including set itself) not marked as reducible? AFAIR this was a conscious decision to prevent it from \"unexpectedly\" unfolding and thus mixing the \"world of sets\" with the \"world of predicates\". People usually think of them as separate concepts, so it can be confusing when your goals are suddenly switched to the other world.", "meta": {"post_id": 222, "input_score": 15, "output_score": 10, "post_title": "Why does apply_instance fail to generate decidable_pred here?"}}
{"input": "As mentioned in another question, the way that VSCode and other editors implement unicode characters that cannot be typed directly from most keyboards is an editor feature.\nAs such, different editors could do it differently, so that for instance, $\\varphi$ and $\\phi$ could be entered as \\varphi, \\phi, the other way around, or something else.\nBut perhaps there is still an official recommendation on how editors should let you type unicode characters; or was a choice made to not make an official recommendation?", "output": "Because maintaining such a big list of Unicode abbreviations is annoying and none of the editor mode maintainers are interested in encouraging untransferrable abbreviations, as far as I know, all of the Lean editor modes just copy the abbreviations.json file (the VS Code plugin is the most popular, so new abbreviations usually land there first). In practice I haven't heard of divergence between editor abbreviations files ever being an issue.\nSo to the extent that there is an \"official recommendation\" to use the VS Code plugin for Lean, that comes with the bundled list of abbreviations, but users can also add their own abbreviations if they want.", "meta": {"post_id": 237, "input_score": 11, "output_score": 9, "post_title": "Is there an official recommendation for editors to input Lean unicode characters?"}}
{"input": "In Ulf Norell's PhD thesis, which is considered the standard reference of the Agda 2 language, the universes are cumulative, say, Set i is not just an instance of Set (suc i), but also a subtype of it.\nHowever, in the implementation of Agda, this is not true. We only have the \"instance of\" relation, not the subtype relation, until recently (--cumulativity).\nIMO, it is good to have more flexibility, but why would Agda developers delete this in the beginning? What inconveniences or problems did cumulative universes bring to us?", "output": "One reason (not particularly specific to Agda) might be that eta reduction is not compatible with cumulativity.\nWith cumulativity (that is $Set_0 \\leq Set_1$), and given\n\\begin{align*}\nf &:= \\lambda X.(\\lambda Y.Y)X &: Set_0\\to Set_1\\\\\nf_\\eta &:= \\lambda Y.Y &: Set_1\\to Set_1\\\\\nf_\\beta &:= \\lambda X.X &: Set_0\\to Set_0\n\\end{align*}\nwe would have (and we can prove, say, in Coq)\n$$ f \\equiv_\\eta f_\\eta\\qquad\\text{ and }\\qquad f\\equiv_\\beta f_\\beta$$\nwhich are well-typed because\n$$(Set_1\\to Set_1) \\leq (Set_0\\to Set_1)\\quad\\text{ and }\\quad(Set_0 \\to Set_0) \\leq (Set_0\\to Set_1) ,$$\nbut we have the awkward situation that $f_\\eta$ is not comparable to $f_\\beta$.\n\nSo while Coq has cumulativity but does not eta-reduce (it does eta-convert however), Agda supports eta-reduction (please correct me if this is wrong) but does not support cumulativity.", "meta": {"post_id": 239, "input_score": 13, "output_score": 14, "post_title": "Why did Agda give up cumulative universes?"}}
{"input": "Trying to answer another question, I can't help second guessing myself about whether eta equality is provable in Coq. In particular, can one prove\nTheorem eta : forall (A : Type) (f : A -> A), (fun x => f x) = f?\n\nDoes this require functional extensionality?\n\nEdit: I already accepted an answer, but a better (followup) question would be, if Coq supports eta-conversion, why does Eval compute in (fun x => f x). not reduce to f?", "output": "As for your follow-up question: Coq implements \u03b7-conversion, but not \u03b7-reduction/expansion, because \u03b7 really is treated at the level of the conversion test and not at that of reduction (which is a sub-routine thereof). Concretely, the conversion test works roughly as follows when comparing t and u:\n\nreduce t and u to weak head normal forms (using a bunch of reduction rules including \u03b2 but not \u03b7)\nif one of the reducts is a neutral n and the other is an abstraction fun x : A => t, recursively compare n x and t (this corresponds to \u03b7-expanding the neutral followed by a congruence rule for abstraction)\n\nThe reasons to do things like this are a bit tricky, but the bottom line is that implementing \u03b7 inside reduction (be it as expansion or reduction) would either break good properties or simply be unfeasable in current Coq.\nI did a talk about this at WITS a few weeks ago, the video does not seem to be up on Youtube yet (it should be there at some point) but the slides and abstract are on my page.", "meta": {"post_id": 250, "input_score": 13, "output_score": 10, "post_title": "Is eta-equality provable in Coq?"}}
{"input": "To my knowledge, the well-known ones like Coq, Agda, Lean are all purely functional languages, and therefore require any computational function to be written in lambda calculus style. For example, you cannot directly write down an imperative algorithm for isprime: nat -> bool equivalent to, say, a C-like function\nbool isprime(nat n) {\n    if n < 2 {\n        return false;\n    }\n    for i in 2 to n-1 {\n        if n % i == 0 {\n            return false;\n        }\n    }\n    return true;\n}\n\nInstead, Coq/Agda/Lean requires the computation to be written in functional style, or you need to encode the computational model itself and reason about the process of computation manually (e.g. via modeling state transition as a binary relation and reasoning about its reflexive transitive closure).\nIs there any proof assistant implementation that allows imperative computation? Is it at least theoretically possible? What are the main challenges to allowing it (if any)?", "output": "Although Lean 4 is a pure functional programming language, it is capable of expressing functions almost exactly like your example:\ndef isprime (n : Nat) : Bool := Id.run do\n  if n < 2 then\n    return false\n  for i in [2:n-1] do\n    if n % i == 0 then\n      return false\n  return true\n\nBut you can argue that this is a cheat, since really lean is a functional language at heart, and this is true. What about \"truly imperative\" languages? Well, one issue with non-pure-functional languages is that functions are not pure, which means that you have to be careful about using them in specifications. What does the proof language even look like? Dependent type theory is one way to unify the concept of \"writing programs\" and \"writing proofs\"; in an imperative language these necessarily diverge.\nIt is, of course, possible to prove properties of imperative programs, but these tend to take the form of annotations around code, either using refinement types or design by contract style requires and ensures clauses around functions, loop invariants and other critical points in the code, with some automation to fill in the gaps. Why3 and Dafny are two strong examples of languages designed for reasoning about imperative code.\nAnother interpretation of your header question is \"a proof assistant that does imperative computation in the course of proving theorems\", and I think there are plenty of examples of this, because the metalanguage of a scriptable proof assistant can be any language at all. Isabelle and the HOL family use ML or OCaml as their metalanguage, which is not a pure functional language (although it has lots of functional leanings). Metamath is a proof language primarily used for proving theorems in ZFC, and the mmj2 proof assistant for it (written in Java) allows running user tactics written in Javascript, which is not really a functional language. Generally, I would say that there is no need for a functional language in this space, but rather the people working in the space have a predisposition toward functional languages, either because they are easily embeddable or because functional features are good for writing operations on syntax.", "meta": {"post_id": 251, "input_score": 10, "output_score": 15, "post_title": "Any proof assistant that allows imperative computation within the language itself?"}}
{"input": "In the theory and design of proof assistants based upon dependent types, I feel like there\u2019s a somewhat cultural divide between the \"MLTT\" world (with Agda as the main representative proof assistant) and the \"CIC\" world (with Coq). For instance, there are two presentations in annex A of the HoTT book, the first one presenting conversion as the equivalence relation generated by computation rules (this is I believe the traditional \"CIC\" presentation), while the second uses a typed conversion judgment (I would relate this to the \"MLTT\" culture).\nWhat would you list as the features most representatives of each side, be it in the theoretical presentation or in the implementations? Or do you think this view of two different cultures is not accurate, and if so why?", "output": "The technical answers are correct, but they completely overlook the philosophical differences between the two formalisms. Martin-L\u00f6f type theory closely reflects Arendt Heyting's explanations of the logical signs and is intuitionistic by construction. CIC is a formal calculus based on string substitution with no underlying philosophy that I can detect.\nI have personally heard Martin-L\u00f6f criticise CIC, in particular because it is impredicative. The habit of using CIC to do constructive mathematics has never made sense to me.", "meta": {"post_id": 267, "input_score": 25, "output_score": 12, "post_title": "What are the differences between MLTT and CIC?"}}
{"input": "Today, there's an exciting large-scale effort to digitize mathematics in Lean's mathematics library mathlib.\nI understand that a transition to Lean 4 is looming. I've heard that Lean has had issues with backwards compatibility in the past, but I don't know the details.\nWhen is Lean 4 expected to become the most widely used version of Lean? Will mathlib suffer from this transition? If so, how?", "output": "BIG UPDATE\nThe port to Lean4 is officially complete. There is a couple changes that we are still hoping to officially put into mathlib3 before moving it into mathlib4, but the content in mathlib3 is now a strict subset of the content of mathlib4, apart from tactics. In the very near future, the mathlib3 repository will be frozen, and all future development should now be done in Lean 4. This does not mean that Lean 4 is stable yet - there is a lot of hard work right now into this.\n\nThe community hopes to eventually move fully to Lean4. There's some details about the history of earlier Lean versions here, but they're not too relevant as the first two versions were very experimental. Now, some members of the community and Microsoft are diligently working on mathport, which will hopefully provide a full syntax-ful translation of Lean3 code into idiomatic Lean4 code. The main test-case for this, at the moment, is mathlib itself, and we are pretty determined to make it work.\nThis blogpost offers some further details on what to expect from the transition - there's still not been a consensus on how gradual the transition is, but I suspect that eventually there will be a day declared where there is an announcement \"no more PRs to mathlib3 will be accepted, please move your work over to Lean 4\".\nUPDATE (Jan '23): Currently, there's a \"progressive locking\" model going on, where we are porting files and, as soon as a file is ported to mathlib4, any PR to it in mathlib3 must be accompanied with an equivalent PR to mathlib4 making the same changes. About 20% of all files have been ported, and also many tactics (definitely over 20% of them!), but I do not think this means 20% of the work has been done - later files in the import hierarchy are often much harder to port (and also often longer).\nUPDATE (Apr/May '23): We are now at 60% progress; this can be tracked here.", "meta": {"post_id": 274, "input_score": 31, "output_score": 35, "post_title": "What will happen to mathlib when we transition to Lean 4?"}}
{"input": "Many proof assistants start with the notion of $Type : Type$ which is inconsistent. This choice makes type checking and some natural recursions arguably easier. Of course, universe levels can be added on afterward, but I'm curious if any proof assistants have taken a different path by adding a notion of unsafety.\nIn the Rust programming language, there is an unsafe fragment that relaxes some of the restrictions of the full language. Some of those \"superpowers\" include:\n\nDereference of a raw pointer\nCall other unsafe functions\n\nCritically, safe interfaces can be constructed from unsafe components, with the implicit assumption that invariants required by safe code are enforced by the programmer (instead of the type/borrow checker).\nThe trusted computing base (TCB) of a proof assistant is generally considered a desirable thing to keep small (Coq and Isabelle for instance have reasonably small TCB, in my opinion). With unsafety, any unsafe portions of library or application code (or proof code!) would also be part of the TCB, at least in terms of making sure the invariants/properties that are used in the unsafe code are true. However, this is not an obviously bad situation because the core type checker could become much simpler if unsafety is designed as a feature from the beginning. Moreover, many different type systems could be shallowly embedded by directly using unsafety to build safe interfaces. As an example, many different variants of universe hierarchies could be implemented directly from unsafe recursive types and other standard safe features. Likewise, many flavors of safe recursive types can also be derived from unsafe recursive types by requiring monotonicity proofs as guards.\nThis is how I imagine such a proof assistant might look: Suppose we have only the combinators $Y, S, K, I$ with $S, K, I$ typed in any consistent way (such that $Y$ is not derivable from them). Then, we mark $Y$ as unsafe and require that any safe combinator, $C$, constructed using $Y$ must satisfy the invariant that using $C$ to construct any other safe combinator is still safe (Ideally, we want a local invariant to make this situation more reasonable to prove). Now, the definition of $C$ is part of the TCB, but any usage of $C$ is not.\nHave any proof assistants taken an approach like this or similar?", "output": "I will summarize the answers from the comments and add my own about Coq.\nMany proof assistants have a way of adding \"unsafe\" declarations and tracking which definitions rely on them.\n\nCoq has Unset Universe Checking (a local version of the -type-in-type command-line option), which I believe is used, for example, by UniMath to introduce a computational resizing axiom.  I believe Print Assumptions tracks whether or not a definition depends on use of this flag\nAgda has the infective --type-in-type and the coinfective --safe flags, on a per-file granularity\nLean has the meta keyword for unrestricted recursion\nIdris has per-definition totality flags", "meta": {"post_id": 277, "input_score": 19, "output_score": 8, "post_title": "Have any proof assistants incorporated a notion of unsafety with Type : Type?"}}
{"input": "The paper in question: An Implementation of a Dependently Typed Lambda Calculus by Andres L\u00f6h, Conor McBride and Wouter Swierstra.\nI'm wondering whether it's correct in what's it is doing. Precisely on page 7. In type checker. It's got these evaluations with empty environments. Eg. let v = eval\u2193 t []. Precisely when these are evaluated, could it crash to the d !! i in the Figure 4?\nDependently typed languages are related to implementation of proof assistants.", "output": "All is well.\nThe implementation distinguishes between bound variables, represented as Bound i where i :: Int and free variables, represented as Par n where n :: Name. The evaluation function eval\u2191 in Figure 4 converts names to values using vpar and it replaces bound variables by looking them up in the environment (this is the line with d !! i).\nIn Figure 12 there are several calls of the form eval\u2193 \u03c4 []. These are valid because in all cases \u03c4 does not contain any \"exposed\" bound variables, therefore it is safe to evaluate it in the empty environment (\u03c4 may contain free variables, but those are not problematic).\nIndeed, the clause for typing an abstraction Lam e in the definition of type\u2193 typechecks e by first replacing the bound variable 0 with a free variable of the form Par (Bound i), where i counts the abstraction depth. This way we make sure that an \"exposed\" bound variable will never be  encountered.", "meta": {"post_id": 281, "input_score": 11, "output_score": 8, "post_title": "In the \"Simply Easy!\" paper, why is it safe to evaluate types in the empty environment?"}}
{"input": "This answer to this question about \u03b7-equivalence in Coq draws a distinction between judgmental equality and definitional equality.\nIn the simply typed lambda calculus (henceforth STLC), the following is an example of a typing rule.\n$$ \\frac{\\Gamma \\vdash t_1 : \\alpha \\to \\beta \\;\\; \\text{and} \\;\\; \\Gamma \\vdash t_2 : \\alpha}{\\Gamma \\vdash t_1(t_2) : \\beta } \\;\\; \\text{is modus ponens} $$\nIn my understanding, $t_1 : \\alpha \\to \\beta$ is a typing judgment.\nSTLC can be formalized using typing judgments exclusively.\nJudgmental equality is slightly more confusing.\nDoes it refer to a new type of judgment such as $t_1 = t_2$ for terms and $\\alpha_1 = \\alpha_2$ for types, as described here?\nI think an equality type is just a type $t_3 : \\alpha \\equiv \\beta$ that can be inhabited or not like any other type.\nAssuming the above is true, then, I'm not sure what definitional equality would be.", "output": "This answer to this question about \u03b7-equivalence in Coq draws a distinction between judgmental equality and definitional equality.\n\nThis is an incorrect attribute to the answer you linked to. Sarah didn't say that judgmental equality is distinct from definitional equality. I suggest reading that answer again.\nJudgmental equality usually means the fact that two terms are semantically exactly the same (in other words, they share the same \"semantical object\"). Definitional equality is a synonym of judgmental equality.\nEquality types are types that encode certain equality, or \"witness\" certain equality. Usually, an equality type is parameterized by two terms (and maybe something else), where every instance of such type is a proof that these two terms satisfy an equality. Equality relations encoded in an equality type are usually called \"propositional equality\". There can be different equality relations encoded by different equality types, depending on the exact definition of the equality types. In my impression, they're mostly stronger than the mathematical sense of equivalence relation (binary relation that is refl, sym, and trans).\nObviously, judgmental equality implies propositional equality (check!), but the reverse direction is not true. In case the reverse is true, we usually refer to the type theory as an extensional type theory, with the reverse known as 'equality reflection'.", "meta": {"post_id": 284, "input_score": 18, "output_score": 13, "post_title": "What is the difference between judgmental equality, definitional equality, and equality types?"}}
{"input": "In Lean, when proving basic theorems, one runs into the following kind of thing:\nimport tactic\nvariables (P : Prop)\n\nexample : P \u2192 P :=\nbegin\n  intro p,\n  exact p,\nend\n\nAfter the intro p, step, the tactic state is\nP : Prop\np : P\n\u22a2 P\n\nAt this point, we need to use the exact tactic to close the goal. All exact seems to do is to tell Lean that the hypothesis p is the same (syntactically? definitionally?) as the target.  It seems as if it would be a reasonable design goal (in the interest of efficiency) to have Lean automatically close the goal once the target is at least syntactically identical to one of the hypotheses.\nIs this a design choice, or is there some deeper reason why this doesn't happen, and what is it?", "output": "My take on that is that, in their first approximation, tactics are just ways to build terms (typically proof terms), so it is expected to have basic tactics that closely correspond to the various term formers. In particular, in your example\n\nintro x for \u03bb x, \u2026\nexact e for e.\n\nThis hopefully justifies the existence of a basic tactic like exact.\nAnd it may also justify why, unless you ask for it, no automation happens. Consider using tactics to build terms, and imagine end would just close goals using any tactic:\ndef fst {a : Type} : (a,a) \u2192 a :=\nbegin intro x, cases x with x1 x2, end\n\nNow there are two assumptions that can be used, but the choice certainly matters.\nBut when using more advanced tactics, some of these do have such automation built in, e.g. simpa.", "meta": {"post_id": 289, "input_score": 12, "output_score": 15, "post_title": "In Lean, why is the exact tactic necessary when the goal is the same as a hypothesis?"}}
{"input": "It seems Lean can produce .olean files from Lean files, and the mathlib project provides an infrastructure to download olean files, which seems to allow the developer to edit a module without having to load and process its dependencies.\nBut what are they exactly? What is their format, and what do they contain?", "output": "(My answer is for Lean 3.  I don't know how .oleans changed in Lean 4.)\nAlso, see this discussion on the Lean Zulip.\nTL;DR\nAn .olean is a \"compiled\" .lean file.  They make everything faster.\nFor users of Lean 3\nA .lean file is computationally expensive to check and process each time.  Checking all of Lean's mathlib library for example would take hours.  By running lean --make one can \"compile\" a lean file and all its dependencies to .olean files.  (Here \"compile\" is maybe best thought of an analogy.)\nIn particular there is a hash in the .olean to make sure it is in sync with the .lean files.  If the files are in sync, then when you import a dependency, Lean will skip the .lean file and only look at the .olean file.  This is orders of magnitude faster.  (Actually, it is even possible to just have use .olean files without the corresponding .lean files.)\nBecause of the huge speedup, when one uses a project based on mathlib, one should get the .olean files as well.  (This is naturally done with leanproject.)  Also, if Lean all of a sudden gets slow or takes a lot of memory, it is possible you accidentally opened or touched a dependency file in a way which is is making Lean reprocess that file and everything that depends on that file, skipping the .olean files.\nFor advanced use (such as working on mathlib), see the advice here.\nMore technical details\nAn .olean file\n\n...is basically a dump of everything in the Lean environment for that file.\n...doesn't contain any information about the tactic proofs.  Instead it only contains the term proofs which are much faster to check.  (I don't think Lean even checks the proofs in an .olean file when it is loaded, but I'm not positive.)\n...contains all the information needed by the Lean VS Code extension (which is run using the Lean language server).  For example if you hover over a definition, VS Code will give you information about the location of that definition, its type,  and its value.  You can also jump to that definition.  This is because that information is stored in the .olean file.\n...provides all the information needed to parse a Lean file.  Notation, definition names and types, and tactic parsing code is stored in the .olean files in some form or another.\n...provides all the information about the environment that tactics need.  Some tactics like library_search need to search over all theorems in the environment.  Others tactics like simp look at a list of simp lemmas to apply.  That data is all stored in the .olean files.\n...provides all the information needed by #print.  This includes the term proof of a theorem.  (Yet another reason why the term proof is in the .olean file.)\n...can be used by other checkers to double check that the proofs are correct.  There are a few external checkers available for .olean files.  They implement the same logic as in Lean's kernel.  This provides an extra guarantee that your proof is correct.  (I could be mistaken, but I think the continuous integration of mathlib runs at least one external checker on all of mathlib every time there is a PR.)\n...is not in a human readable format.  I think it is stored as a sort of linked graph similar to how it is stored internally in Lean.\n...is not well documented.  The best documentation is likely Mario's olean-parser mentioned in Joachim's answer.\n...is probably not the best way to access Lean information.  One of the best ways to get and work with the information in the .olean files is through Lean meta-programming.   Basically you can write a Lean program to traverse the environment.  This isn't well documented either, but there are a lot of knowledgeable folks and examples on the Lean Zulip chat.", "meta": {"post_id": 292, "input_score": 14, "output_score": 13, "post_title": "What is in an olean file?"}}
{"input": "Attempts to apply machine-learning (as opposed to hand-crafted automation) to formal theorem proving are still in their infancy.\nStill, there have been some successes. GPT-f found some proofs that were shorter than existing proofs in the Metamath library, and were accordingly accepted there.\nAre there other applications of machine-learning to constructing proofs in any proof assistant that attained a success of at least this magnitude - i.e. finding somewhat better versions of existing proofs, if not entirely new proofs?\n\"Better\" can be measured by whatever standards that proof assistant's community usually uses to evaluate proofs, i.e. a shorter proof might not count if it is less readable or less maintainable.", "output": "The group at OpenAI has recently published a new paper, among other problems they  have been able to prove two questions from the International Mathematical Olympiad. Isabelle's Sledgehammer has been able to find solutions to some of these examples as well, and they of course have been proved by many people already.\nI guess it's open for debate whether the proofs found by GPT-f are better than the ones found by ATP or constructed by humans, but at least Kevin Buzzard finds some of the proofs quite surprising.", "meta": {"post_id": 309, "input_score": 23, "output_score": 8, "post_title": "Successes of machine learning in formal theorem proving"}}
{"input": "tl;dr: Are there any good introductions/tutorials on how to formalize definitions and theorem statements in Lean? (in contrast to introductions on formalizing proofs)\n\nSome background: I am a phd student in combinatorial optimization and a few month ago I decided that I wanted to learn more about proof assistants and in particular Lean. So, I set myself the goal to try to get to a point where I can start to formalize mathematics I am actually currently working on (e.g. some exercises or even some lemma from my thesis) - just to have a goal and see whether I like it.\nSo, I read/worked through some of the \"obvious\" introductions (like the natural number game and Theorem proving in Lean) and I think I am now at a point where I can formalize some non-trivial proofs.\nHowever, whenever I am now trying to work on something new, I notice that I get mostly stuck on writing the definitions/theorem statements and I get the feeling that I don't really know how to do them \"the right way\" for Lean (e.g. when should I use def/class/structure/instance/...? Should my objects be types or sets? What arguments should should be implicit/explicit? ...?).\nNow, I understand that this is probably just another non-trivial thing one has to learn but I don't really know of any resources I can learn this from. Some of the tools get explained in the later chapters of \"Theorem proving in Lean\", but at least for me there is not enough relating these explanations to their applications in mathematics. One can of course also look into mathlib for examples, but most of the time this just shows me what a good formalization looks like but not really why it is the way it is and how one gets there.\nSo, in conclusion: Are there any good introductory texts or video tutorials on how write good definitions/theorems in Lean? (or any other ways to learn how to do that)", "output": "Lean for the Curious Mathematician 2020\nYou seem to be the exact right audience for this workshop. We have video recordings on youtube: https://www.youtube.com/watch?v=8mVOIGW5US4&list=PLlF-CfQhukNlxexiNJErGJd2dte_J1t1N\nTutorials\nSome other tutorials and resources are listed here: https://leanprover-community.github.io/learn.html#hands-on-approaches", "meta": {"post_id": 314, "input_score": 23, "output_score": 18, "post_title": "Tutorials for formalizing mathematical definitions/statements in Lean"}}
{"input": "Type systems, and the proof assistants based on them, are frequently divided into predicative and impredicative.\nWhat exactly does this mean? I've heard the slogan \"impredicativity means you can't quantify over things you haven't defined yet\", but I don't know how to apply this definition to a type system.\n\nPredicativity is mentioned here in this answer to this question. I don't really understand predicativity and don't understand how the concept in a classical setting lines up with or doesn't line up with the concept in a proof assistant setting.\nThis question form the Mathematics Stack Exchange and its answer describe what predicativity is when applied to the axiom schema of separation in ZFC.\nI'll do something a little nonstandard in the notation and always split the variable intepretations fed to $\\varphi(\\vec{p}; \\vec{x})$ into parameters $\\vec{p}$ and ordinary variables $\\vec{x}$.\nThe impredicative version is the ordinary one, given below.\n$$ \\forall x \\exists y \\forall z \\mathop. (z \\in y) \\leftrightarrow \n((z \\in x) \\land \\varphi(\\vec{p}; z)) \\;\\; \\text{is impredicative separation} $$\nThe predicative version is similar, but $\\varphi(\\vec{p}; z)$ is constrained to contain exclusively quantifiers bound by a parameter, i.e. $\\forall x \\in q \\mathop. \\square$ and $\\exists x \\in q \\mathop. \\square$ where $q$ is in $\\vec{p}$ or is bound by an earlier quantifier.\nBased on my understanding on the text quoted in the linked answer, this does not constitute a complete ban on impredicative quantification since the axiom schema of predicative separation itself contains unbound quantifiers $\\forall x \\exists y \\forall z \\mathop. \\square$ in its prenex that cannot be paraphrased away.\nSo in this case, \"predicativity\" applies only to the value of the metavariable $\\varphi(\\cdots)$ and a set theory with this axiom would still be an \"impredicative theory\". (Maybe?)\nChanging gears a little bit, if I look at the inference rules of the calculus of constructions, on which the proof assistant Coq is based, I'm not sure how to assess whether the formalism is predicative or not.\nHere are the rules for convenience.\n$K$ and $L$ range over $\\{P, T\\}$.\n$$ \\frac{}{\\Gamma \\vdash P : T} \\;\\; \\text{$P$ is a large type} $$\n$$ \\frac{}{\\Gamma, x: A, \\Gamma' \\vdash x : A} $$\n$$ \\frac{\\Gamma \\vdash A : K \\;\\;\\text{and}\\;\\; \\Gamma, x : A \\vdash B:L}{\\Gamma \\vdash (\\forall x : A \\mathop. B) : L} \\;\\; \\text{is universal introduction}$$\n$$ \\frac{\\Gamma \\vdash A : K \\;\\;\\text{and}\\;\\; \\Gamma, x: A \\vdash N :B}{\\Gamma \\vdash (\\lambda x: A \\mathop. N):(\\forall x : A \\mathop. B)} \\;\\; \\text{is function introdction} $$\n$$ \\frac{\\Gamma \\vdash M : (\\forall x : A \\mathop. B) \\;\\; \\text{and}\\;\\; \\Gamma \\vdash N:A }{\\Gamma \\vdash M(N) : B[x:=N]} \\;\\; \\text{is function elimination (but dependent!)} $$\n$$ \\frac{\\Gamma \\vdash M:A \\;\\;\\text{and} \\;\\; A=_\\beta B \\;\\;\\text{and}\\;\\; \\Gamma \\vdash B:K}{\\Gamma \\vdash M : B } $$\nIn CoC, the \"quantifiers\" are always bound, but the text of the article implies that the calculus of inductive constructions is impredicative, perhaps suggesting that CoC itself is too.\n\nSome of its variants include the calculus of inductive\nconstructions (which adds inductive types), the calculus of\n(co)inductive constructions (which adds coinduction), and the\npredicative calculus of inductive constructions (which removes some\nimpredicativity).", "output": "Impredicativity is one of those soft concepts that appears in many related forms, but it is difficult to explain what precisely they share. Let me try anyhow.\nImpredicativity allows us to single out, construct, or characterize a particular entity $e$ of some totality (set, type, universe) $T$ by quantification over all of $T$. Such constructions are sometimes considered problematic, especially when $T$ is a \"non-completed\", \"large\" or otherwise mysterious collection, say the class of all sets, or the subsets of an infinite set.\nIf we ignore the fears that haunt predicative mathematicians, there still remains the technical observation that impredicativity often greatly increases the logical strength of a formal system. In fact, unbridled impredicativity was the culprit for the early paradoxes of set theory.\nHere are some examples.\nThe axiom schema of separation\nIn set theory we may define an element of the class $\\mathsf{Set}$, i.e. a set, by quantifying over the entire class $\\mathsf{Set}$ using unbounded quantifiers. For example,\n$$\\{x \\in P(\\emptyset) \\mid \\forall y . y \\times x \\cong y\\}$$\nis a silly way of constructing the set $\\{\\{\\emptyset\\}\\}$ (sorry, I cannot think of a real example right now, but I am sure set theorists can come up with essential uses of unbounded separation). This is considered impredicative because $y$ ranges over all of $\\mathsf{Set}$, including the set that is being defined.\nIt should be clear that bounded separation $\\{x \\in A \\mid \\phi\\}$ in which all quantifiers appearing in $\\phi$ are bounded (of the form $\\forall y \\in S$ and $\\exists y \\in S$) may be considered predicative since the sets over which the quantifiers range have been constructed already (or else we could not mention them).\nThe related principle of unbounded comprehension, stating that for any predicate $\\phi$ there is a set $\\{x \\mid \\phi\\}$ is of course a form of impredicativity. In fact, this form of impredicativity is too strong, as it leads to Russell's paradox. It shows that impredicativity is not to be trifled with.\nComplete lattices\nA poset $(P, {\\leq})$ is a complete lattice when it is closed under arbitrary suprema (in which case it is also closed under arbitrary infima).\nThis is impredicative because it allows us to obtain an element $x \\in P$ by quantifying over a subset $S \\subseteq P$ of which $x$ may be an element (\u201c$x$ is supremum of $S$\u201d quantifies over $S$). The prime example is Tarski's fixed-point theorem: given a monotone map $f : P \\to P$, the least fixed-point $x$ of $f$ is defined as\n$$x \\mathbin{{:}{=}} \\sup \\{ y \\in P \\mid y \\leq f(y) \\}.$$\nThis definition of $x$ is impredicative because $x = f(x)$ and so $x$ is the element of the set that was used to specify $x$.\nPowersets are complete lattices and thus may be considered a source of impredicativity. It should be noted that as soon as the powerset of the singleton $\\{\\star\\}$ exists, all powersets exist because $P(A) \\cong P(\\{\\star\\})^A$. So the haunting is already done by the smallest non-trivial powerset. This is why topos theory is impredicative at its core: the subobject classifier $\\Omega$ is precisely $P(\\{\\star\\})$.\nImpredicativity in type theory\nIn type theory quantification is carried out by products. An impredicative universe $U$ is thus one that is closed under quantification over $U$:\n$$\n\\frac{\\vdash F : U \\to U}{\\vdash (\\Pi (A {:} U) \\,.\\, F A) : U}\n$$\nAn example is the universe of propositions $\\mathsf{Prop}$ which is closed under arbitrary products:\n$$\n\\frac{\\vdash A \\, \\mathsf{type}\n\\quad\nx {:} A \\vdash P(x) : \\mathsf{Prop}\n}{\\vdash \\forall x {:} A . P(x) : \\mathsf{Prop}}\n$$\nThe universe $\\mathsf{Prop}$ is impredicative because as a special case of the above we may quantify over $\\mathsf{Prop}$ itself. This is very useful, of course, as it allows us to define logical connectives by Church encodings, e.g.,\n$$p \\lor q = \\forall r {:} \\mathsf{Prop} \\,.\\, (p \\to r) \\to (q \\to r) \\to r.$$\nThe above definition is impredicative because the meaning of $p \\lor q$ is given by quantification over all propositions, one of which is $p \\lor q$ itself.\nSystem F is impredicative because it allows a type to be defined by quantification over all types, in much the same way as the impredicative encodings in $\\mathsf{Prop}$. For related reasons second-order Peano arithmetic is considered impredicative (a predicate may be defined by quantification over all predicates).\nA universe $U$ that contains itself and is closed under products is impredicative \u2013 and this is the case of impredicativity having gone too far because it leads to an inconsistency.\nPredicativity\nThose who feel uneasy about impredicativity, and those who want to carefully callibrate the logical strength of their formal systems, look for ways of avoiding impredicativity. In general this works by replacing self-referential constructions with iterative and inductive constructions which \"build the object from below\".\nFor example, Tarski's theorem (see above) may be proved as follows: give a monotone map $f : L \\to L$, its least fixed point is the limit of the ordinal-indexed chain\n\\begin{align*}\nx_{\\beta}  &\\mathrel{{:}{=}} \\sup \\{ f(\\alpha) \\mid \\alpha < \\beta \\} \n\\end{align*}\nWe have replaced impredicativity with transfinite recursion on ordinals \u2013 and in any specific case only a set-sized amount of such recursion is required.\nIn type theory impredicativity is typically avoided by introduction of universe levels, so that quantification over a universe $U_k$ yields a type in the next universe $U_{k+1}$.\nOne might still wonder whether some impredicativity is involved even in the most innocuous looking constructions. For example, when we replace the Church encoding of $p \\lor q$ with direct-style inference rules, we might write the elimination rule as\n$$\n\\frac{p \\vdash r \\qquad q \\vdash r}{p \\lor q \\vdash r}\n$$\nIs this not quantification over all propositions $r$, just as before?\nNo, there is an important difference: in the case of the Church encoding of $p \\lor q$ the quantification was internal to the formal system (we used a quantifier that is one of the constructors of type theory), whereas the above rules use external or meta-level or schematic quantification. The latter has much less of a punch.", "meta": {"post_id": 326, "input_score": 37, "output_score": 35, "post_title": "What is predicativity?"}}
{"input": "I know of a handful of automated theorem provers for classical first-order logic such as Vampire (source code).\nInternally, I think most of these provers work by translating premises and the negated goal into disjunctions of literals and then applying resolution. This also requires Skolemization as a preprocessing step, which removes existential quantifiers in the prenex of a wff and adds new function symbols to the language. Here's a link to the the Skolemization implementation in Vampire.\nThis technique will not work in a constructive setting. Negating the goal, converting to negative normal form and then prenex normal form is not constructively valid.\nWhat automated theorem provers for intuitionistic first-order logic are there (whether they're capable of emitting proof terms or not)? If there are any, broadly speaking, what strategy do they use?", "output": "The firstorder tactic in Coq is an \"experimental extension of tauto to first-order reasoning.\"  The  tauto tactic \"implements a decision procedure for intuitionistic propositional calculus based on the contraction-free sequent calculi LJT* of Roy Dyckhoff [Dyc92]\".  Perhaps one of the Coq developers can speak to the theory behind firstorder.\nThese tactics do not rely on classical axioms, and do generate proof terms.", "meta": {"post_id": 347, "input_score": 15, "output_score": 10, "post_title": "Are there automated theorem provers for constructive logics? What strategy do they use?"}}
{"input": "I already know that refl is called a tactic, and that rfl is a term; can you explain with examples how they technically differ? I've read that refl is \"more powerful\", but I never knew beforehand if one would definitely work where the other may not. Bonus points for by exact rfl explanations. I also wonder if there are significant changes to these between different versions of Lean 3...", "output": "Separately from the rfl (term) vs refl (tactic) distinction, there is also the distinction between rfl and refl in lemma names:\n#check @eq.refl  -- \u2200 {\u03b1 : Sort u_2} (a : \u03b1), a = a\n#check @rfl      -- \u2200 {\u03b1 : Sort u_2} {a : \u03b1}, a = a\n\n#check @iff.refl -- \u2200 (a : Prop), a \u2194 a\n#check @iff.rfl  -- \u2200 {a : Prop}, a \u2194 a\n\n#check @le_refl  -- \u2200 {\u03b1 : Type u_2} [_inst_1 : preorder \u03b1] (a : \u03b1), a \u2264 a\n#check @le_rfl   -- \u2200 {\u03b1 : Type u_2} [_inst_1 : preorder \u03b1] {a : \u03b1}, a \u2264 a\n\nThe difference here is that that rfl uses an implicit {} binder for a, while refl uses a explicit () binder. So iff.rfl is shorthand for iff.refl _, le_rfl is shorthand for le_refl _, etc.", "meta": {"post_id": 409, "input_score": 19, "output_score": 9, "post_title": "What is the difference between refl and rfl in Lean 3?"}}
{"input": "I need to work with the set of all lines in the Cartesian plane. For my context, the natural way to think of this is that a line can be described by an equation $Ax + By + C = 0$, where $A$ and $B$ cannot both be zero. So a natural representation is as triples $(A,B,C)$ where $A$ and $B$ are not both zero. But the lines characterized by $(A,B,C)$ and by $(7A, 7B, 7C)$ are the same, so I need a quotient. To be explicit, I want to look at the set $Y$ defined by\n\\begin{align}\nX &= \\Bbb R^2 - \\{(0,0)\\}\\\\\nY &= X \\times \\Bbb R\n\\end{align}\nand the equivalence relation on $Y$ where $((A,B), C) \\sim ((P, Q), R)$ if and only if there's a real number $k$ such that $P = kA, Q = kB, R = kC$. Equally good would be to say that\n$$\nY = \\Bbb R^3 - \\{(0,0,C) \\mid C \\in \\Bbb R \\},\n$$\nbut that seemed as if it might be more difficult to express in Lean.\nThose who like geometry will recognize that my set $Y$ is just $\\Bbb RP^2$ minus one point, i.e., it's topologically a Mobius band (with \"open\" edges).\nOnce I have a description of $Y$, preferably as a type, I can write down the equivalence relation and prove it's an equivalence and even construct the quotient.\nCan someone point me in the right direction to defining $Y$ (or even $X$) as a type? (As you may have guessed, I'm a beginner at Lean.)", "output": "Let me answer your immediate question first with the following code snippet (which relies on mathlib):\nimport data.real.basic\nimport data.matrix.notation\n\ndef X := { v : fin 2 \u2192 \u211d // v \u2260 0 }\n\n-- The first def for `Y`.\ndef Y := X \u00d7 \u211d\n\n-- The second def for `Y`.\ndef Y' := { v : fin 3 \u2192 \u211d // v 0 \u2260 0 \u2228 v 1 \u2260 0 }\n\n-- Alternative for `Y'`, using matrix notation:\ndef Y'' := { v : fin 3 \u2192 \u211d // \u2200 c : \u211d, v \u2260 ![0,0,c] }\n\nSince projective spaces are interesting objects as well, I think it would be worthwhile to first define $\\mathbb{RP}^2$, or more generally, $\\mathbb{P}^n(k)$, where $k$ is any field, or even better, $\\mathbb{P}_k(V)$, the projectivization of the $k$-vector space $V$. Once projective spaces are defined as a type, the next steps would be to build up the API to make them usable! Finally, remove a point from $\\mathbb{P}_{\\mathbb{R}}(\\mathbb{R}^3)$ to obtain your desired object.\nFor the initial definition of $\\mathbb{P}_k(V)$, I see essentially two ways to do this:\n\nConsider the type of all one-dimensional subspaces of $V$.\nTake a quotient of $V \\smallsetminus \\{0\\}$ by the equivalence relation given by multiplication by elements of $k^\\times$.\n\nThe first approach could be defined as\nimport linear_algebra.finite_dimensional\n\nvariables (k V : Type*) [field k] [add_comm_group V] [module k V]\n\n-- Note that an infinite dimensional space has `finrank = 0`, by convention.\ndef projectivization := { M : submodule k V // finite_dimensional.finrank k M = 1 }\n\nwhile the second can be defined as follows (modulo the missing proof):\nimport linear_algebra.finite_dimensional\n\nvariables (k V : Type*) [field k] [add_comm_group V] [module k V]\n\n-- Since `y` is nonzero by assumption, `c \u2022 (x : V) = y` will force `c \u2260 0` as well.\ndef projectivization.setoid : setoid { v : V // v \u2260 0 } := \n{ r := \u03bb x y, \u2203 c : k, c \u2022 (x : V) = y,\n  iseqv := sorry }\n\ndef projectivization := quotient (projectivization.setoid k V)\n\nNote that both of these code snippets rely on mathlib, and, of course, one could easily change the implementations (e.g. replacing the finrank condition by an equivalent Prop.\nI hope these are useful to get started, and additional help is always available on the Lean zulip.", "meta": {"post_id": 418, "input_score": 13, "output_score": 11, "post_title": "Representing $\\Bbb RP^2$ in Lean: building a type representing a particular set"}}
{"input": "TL;DR: How / where to formalize results concerning the logical strength of systems? Are proof assistants having a weak base theory but also enough infrastructure to make it feasible?\n\nI'll start by giving some context. In studying Logic and foundations, a clear interest is the consistency of the set of axioms that you use; especially when showing that some statement is \u201cindependent\u201d (or simply doesn't follow) from others. The cardinal example (pun slightly intended) of this is the Continuum Hypothesis (CH) that since Cohen we know that it can't be proved nor disproved with current mathematical methods.\nThe catch, obviously is that \u201ccurrent mathematical methods\u201d above corresponds to \u201cZFC set theory\u201c (for which it was proved), and even by adding large cardinal assumptions (which include, by translation, CIC with universes). But if those foundations are self-contradictory, the previous paragraph is wrong. The correct reading is: If ZFC is consistent, then it doesn't prove CH; to the effects of the present question, the interesting version is: If ZFC is consistent, so is ZFC + \u00acCH.\nThere are other equiconsistency results as that. For example, the following theories are equiconsistent:\n\nZF + Choice for countable families + \u201cEvery subset of $\\mathbb{R}$ is Lebesgue measurable\u201d;\nZFC + \u201cthere exists an inaccessible cardinal\u201d (i.e., there exists a set universe).\n\nIn order to be able to formalize this theorem in a reasonable context, the underlying logic of the proof assistant should be at least weaker that ZFC plus one universe. Same goes for the independence of CH. On the other hand, proof assistants like Lean (and Coq?) assume a sequence of universes from the start; also, Isabelle/HOL has a session devoted to set theory, but its consistency strength is that of statement 2 above.\nThe question arises even for Isabelle/ZF if you want to work with theories weaker than ZFC. In this state of affairs, I would like to know if there is any proof assistant that has a very weak foundation, that allows to work smoothly with theories of varying strength.\nBy searching the site, I found a related question asking for the formalization of model theory, and another relevant one\nmentioning also other systems of different strength (Peano / Primitive Recursive Arithmetic / Bounded Arithmetic, systems of reverse mathematics). But one point to be considered is that if one starts with plain PA / PRA then you have to code all the reasoning inside it, and usually tactics work only at the surface.", "output": "You could address your goals by using a generic proof assistant, one that supports user-definable theories, such as Isabelle and Metamath, and I am sure there are others. These allow you to define your own weak formal system and work with it.\nHowever, once you define your favorite formal system, you will have a bare-bones proof assistant that will only become usable once you implement a substantial library of basic facts and constructions \u2013 and that is a rather daunting task. We need to learn how to lower the bar to having domain-specific proof assistants.\nAnother possibility is to use a popular proof assistant that supports a very strong formalism, but to carefully avoid its power. While not entirely satisfactory, doing so will allow you to actually get things done.\nSpecifically, suppose you want to show that if $\\mathrm{ZFC} + X$ is consistent then so is $\\mathrm{ZFC} + Y$, and you'd like to do this by demonstrating that a model of $\\mathrm{ZFC} + X$ yields a model of $\\mathrm{ZFC} + Y$. In whatever proof assistant you like, formalize the basics of Tarskian semantics: the syntax and logic of the first-order theory $\\mathrm{ZFC}$, $\\in$-structures, models of $\\mathrm{ZFC}$ etc. Then you prove a theorem\nZFCX\u21d2ZFCY : \u03a0 (M : \u2208-structure) . (M \u22a8 ZFC+X) \u2192 \u03a3 (N : \u2208-structure) . (N \u22a8 ZFC+Y)\nZFCX\u21d2ZFCY = \u2329proof-goes-here\u232a\n\nSince your proof assistant is too powerful, it allows you to just construct a model N of ZFC+Y from scratch, but you shouldn't do that. Instead you resist the Dark Side and use forcing to construct N from M in the weak fragment that you wish you were working in.\nThis is not ideal but is still well-worth doing. Think of it as partial formalization. You have formalized the desired construction, but you still have to manually check the proof to verify that it remains within the confines of a weak fragment (this may not be so simple if you use the standard library and lose track of what the library is doing). People actually do such things for lack of a better method. For example, the UniMath library formalizes mathematics in Univalent foundations on top of the assumption Type : Type \u2013 which is inconsistent! The authors review the code carefully and avoid dangerous moves. (I have it on good authority that the inconsistent assumption keeps them awake at nights.)", "meta": {"post_id": 425, "input_score": 13, "output_score": 9, "post_title": "Attempts to accommodate theories of different consistency strength in single assistant"}}
{"input": "In the early 2000s (or maybe even earlier) Freek Wiedijk published a list of 100 theorems which were a sort of litmus test of the state of the art in formalized mathematics.  As the completion rate nears a stable point, I want to ask the community's reflection on the list and its future.\n\nHas Freek's list been a positive impetus to the community?  Especially for the (much?) smaller half of the community focusing on developing libraries of formal mathematics?\nAs this list is nearly complete (save for FLT), what next?  Do we need a new list, or something similar, to bring visibility to formal theorem proving, encourage growth, compare tools, and measure progress in the field?\nIf so, what would that next thing look like?  Would it be just mathematics?  Mathematics and software?  Is there any hope that one could get buy-in from lots of proof assistant communities, or are their respective aims just too different?  Who would maintain it?  Or should each community just maintain their own lists according to their own standards?\n\nEdit: I am mostly just asking if we need a new list or something similar to replace it, not asking what the next big thing in all of formal theorem proving needs to be.  The immediate motivation for this question was a discussion about if there is any meaningful way to compare the various libraries of formal mathematics since they have different methods of counting theorems and some focus more on pure mathematics than others.  While any individual or any proof assistant community can make their own list, I wanted to use this forum involving many different proof assistants to discuss if there is any agreement on this matter.  I wrote the question a bit more broadly since I thought this list was a good way to measure progress in the field and show what can be done.", "output": "Freek's list served a very helpful purpose. It's nice to have a list of standard challenges which are recognisable to those outside the community, and its near-completion makes it very clear that it is possible to formally prove interesting theorems.\nThat said, we do not need another list. The point -- that it is possible to do serious mathematics in proof assistants -- has been made, and there's no sense in making the same point over and over again.\nNow that we know it is possible, the next problem is showing that mechanised mathematics is convenient. Plenty of working mathematicians use tools like SageMath and Mathematica, and they do so because they makes computations, plotting, and algebraic manipulations much faster and more reliable. But very few people use Coq because it makes proofs easier[1]. For sensible mathematics, it is only a mild exaggeration to say proof assistants are actually proof obstructants!\nIn my view, there are two major intertwined problems.\n\nWe simply do not know how to build large and usable libraries of mechanised mathematics. A working mathematician has a large body of knowledge and standard facts at her fingertips, and can use them straightforwardly in her work. This is partly a matter of lacking experience, and partly a matter of not knowing which features are missing from our proof assistants.\nFor example, the work in Coq on HierarchyBuilder is a good example of adding features to a proof assistant to support the needs of large-scale verification. Conversely,the Lean mathlib project is a very important attempt to work out how to design a large corpus (\"the undergrad curriculum\") by actually doing it.\n\nNo proof assistant has anything approaching a predictable envelope of operation.\nComplex tools need to have a clear and teachable set of principles of operation. Without a good mental model, it's very hard for a human operator to figure out which things they try will work, how things can go wrong, and how to repair errors when they do occur.\nAt the moment, there are basically no proof assistants which satisfy this criterion. You basically have to understand the exact internals of the implementation to handle complex problems.\nAnd while one might be tempted to think this is impossible for decidability/complexity reasons, I\nthink this is wrong.\nFor example, SMT solvers actually do satisfy this criterion. If you stick to using quantifier-free combinations of decidable theories, they basically just work, even though the problems are NP-complete in general.\n\n\n[1] It really does make your life easier when doing formal verification. If you need to revise the induction hypothesis on a seven-bajillion case induction over all the instructions, addressing modes, and protection levels of the ARM instruction set or something, then a proof assistant will save your life.", "meta": {"post_id": 436, "input_score": 21, "output_score": 16, "post_title": "Impact of \"Formalizing 100 Theorems\" and what is next"}}
{"input": "In Agda with --guardedness, we can define productive definitions using copatterns like the following\nrepeat : \u2200 {A} -> (a : A) -> Stream A\nrepeat a .head = a\nrepeat a .tail = repeat a\n\nGiven the coinductive definition of Stream. Similarly we can define zipWith:\nzipWith : \u2200 {A B C} -> (A -> B -> C) -> Stream A -> Stream B -> Stream C\nzipWith f a b .head = f (a .head) (b .head)\nzipWith f a b .tail = zipWith f (a .tail) (b .tail)\n\nHowever, if we want to go further and define the Fibonacci stream:\nfib : Stream Nat\nfib .head = 0\nfib .tail .head = 1\nfib .tail .tail = zipWith _+_ fib (fib .tail)\n\nAgda productivity checker will complain that the last clause is not (structurally) productive. In my impression, the recommended way to make fib work is to introduce a size parameter to the definition of Stream. However, since sized types are incompatible with guardedness, we have to completely get rid of guardedness if we want to use sized types (or give up --safe, which is undesirable). So, I wonder if there's a way to make fib work with guardedness (which means, no sized types)?", "output": "I recommend the state machine definition that we'd use for looping over Fibonacci numbers:\nfib' : \u2115 \u2192 \u2115 \u2192 Stream \u2115\nhead (fib' x y) = x\ntail (fib' x y) = fib' y (x + y)\n\nfib : Stream \u2115\nfib = fib' 0 1\n\nThe classic Haskell zipWith definition is more intended to show off several language features at once, and it's both less efficient and harder to reason about than the above version.\nIt's a good general approach to try to figure out the state that has to remembered and updated on each unfolding, when we try to convert complicated coinduction to versions that are more palatable to Agda.", "meta": {"post_id": 446, "input_score": 8, "output_score": 8, "post_title": "Is it possible to define `fib` coinductive stream w/o sized types?"}}
{"input": "How close can you get to introducing and discharging hypotheses in metamath?\nI'm trying to define a symbol formal system for the $\\land$-only fragment of classical propositional logic.\nIt has the following rules.\n$$ \\frac{A \\;\\;\\text{and}\\;\\; B}{A \\land B} \\;\\;\\text{and}\\;\\; \\frac{A\\land B}{A} \\;\\; \\text{and} \\;\\; \\frac{A \\land B}{B} \\;\\; \\text{are the inference rules} $$\nI'm interested in proving a simple statement $P \\land Q \\vdash Q \\land P$.\nHere's my attempt to prove it so far in metamath. It is semi-syntactically correct, failing at the verify proof step in the command line interface.\nI'm working using demo0.mm as an example.\nHere's what I have so far.\n$( define 1 (top) 0 (bottom) and * (conjunction)\n   Also define |- entailment $)\n\n$c 1 0 * wff |- ( ) $.\n\n$( define metavariables P,Q,R $)\n\n$v P Q R $.\n\nwff-P $f wff P $.\nwff-Q $f wff Q $.\nwff-R $f wff R $.\n\nwff-conj-P-Q $a wff ( * P Q ) $.\n\n$( inference rule 1: conjunction intro:\n\n   A and B\n   -------\n    * A B\n$)\n\n${\n   conj-intro-min  $e |- P $.\n   conj-intro-maj  $e |- Q $.\n   conj-intro      $a |- ( * P Q ) $.\n$}\n\n$( inference rules 2 and 3 : conjunction elim $)\n\n${\n   pi1-prem   $e |- ( * P Q ) $.\n   pi1        $a |- P $.\n$}\n${\n   pi2-prem   $e |- ( * P Q ) $.\n   pi2        $a |- Q $.\n$}\n\n$( prove that ( ( * P Q ) ) |- ( * Q P ) $)\ncommutative-conj $p ( ( * P Q ) ) |- ( * Q P ) $=\nwff-P\nwff-Q\nwff-conj-P-Q\n$.\n\nThe proof of commutative-conj is very, very incomplete and does not work.\nI know that metamath is vaguely Hilbert-style and ... I'm pretty sure I can't just introduce a hypothesis.\nSo, I tried to do the next best thing and write a rule that contains a free variable in its conclusion that wouldn't unify with anything.\n$( An extremely weak form of monotonicity of entailment\n\nIf  |- A  holds, then  B |- A  holds.\n\n$)\n${\n   weak-mono-entailment-prem $e       |- P $.\n   weak-mono-entailment      $a ( Q ) |- P $.\n$}\n\nHowever, this approach seems to require me to assert more and more ad hoc facts about $\\vdash$ in order to make any progress at all ...\nIs there a better, more idiomatic way to mimic introduction and discharge of hypotheses?", "output": "Concerning your actual proof attempt:\nHere is the completed proof:\n${\n   commutative-conj-prem $e |- ( * P Q ) $.\n   $( prove that ( ( * P Q ) ) |- ( * Q P ) $)\n   commutative-conj $p |- ( * Q P ) $=\n     wff-Q wff-P wff-P wff-Q commutative-conj-prem pi2 wff-P wff-Q\n     commutative-conj-prem pi1 conj-intro $.\n$}\n\nJason has already explained the changes needed to the theorem statement itself.\nHere is how the Metamath program displays the proof:\nMM-PA> SHOW PROOF commutative-conj\n 5     pi2-prem=commutative-conj-prem $e |- ( * P Q )\n 6   conj-intro-min=pi2             $a |- Q\n 9     pi1-prem=commutative-conj-prem $e |- ( * P Q )\n10   conj-intro-maj=pi1             $a |- P\n11 commutative-conj=conj-intro    $a |- ( * Q P )\n\n\nStep 5, it invokes commutative-conj-prem,\nStep 6, it invokes pi2, with step 5 as a premise,\nStep 9, it invokes again commutative-conj-prem,\nStep 10, it invokes pi1, with step 9 as a premise,\nStep 11, if finally invokes conj-intro, with steps 6 and 10 as premises.\n\nYou'll notice some steps are not shown. These are used to build the expressions which are used as substitutions. In your case, those are invocations of wff-p and wff-q. They can be shown using the SHOW PROOF commutative-conj / ALL command.\nHere is the way the proof can be built with metamath-exe:\nMM> PROVE commutative-conj\nEntering the Proof Assistant.  HELP PROOF_ASSISTANT for help, EXIT to exit.\nYou will be working on statement (from \"SHOW STATEMENT commutative-conj\"):\n21 commutative-conj-prem $e |- ( * P Q ) $.\n22 commutative-conj $p |- ( * Q P ) $= ... $.\nUnknown step summary (from \"SHOW NEW_PROOF / UNKNOWN\"):\n1    commutative-conj=? $? |- ( * Q P )\nMM-PA> ASSIGN 1 conj-intro\n3 -1   conj-intro-min=?            $? |- Q\n4      conj-intro-maj=?            $? |- P\nMM-PA> ASSIGN 3 pi2\n5 -1     pi2-prem=?                  $? |- ( * $4 Q )\n7      conj-intro-maj=?            $? |- P\nMM-PA> ASSIGN 5 commutative-conj-prem\n7      conj-intro-maj=?               $? |- P\nMM-PA> ASSIGN 7 pi1\n 9        pi1-prem=?                     $? |- ( * P $6 )\nMM-PA> ASSIGN 9 commutative-conj-prem\nMM-PA> IMPROVE ALL\nA proof of length 1 was found for step 8.\nA proof of length 1 was found for step 7.\nA proof of length 1 was found for step 4.\nA proof of length 1 was found for step 3.\nA proof of length 1 was found for step 2.\nA proof of length 1 was found for step 1.\nSteps 1 and above have been renumbered.\nCONGRATULATIONS!  The proof is complete.  Use SAVE NEW_PROOF to save it.\nNote:  The Proof Assistant does not detect $d violations.  After saving\nthe proof, you should verify it with VERIFY PROOF.\n\n(this is the full output, the necessary commands are those after the interactive prompt MM-PA>)\nMore generally:\nIf you would like a proof format which has hypotheses one the left, before the turnstile, it is also possible to define it, see for example Mario's HOL database, where your statement would be written something like:\nancom $p |- ( R , S ) |= ( S , R ) $=\n\nThat's probably as close as you can get.\nIn practice, in the set.mm database, it common to use a \"deduction style\" format for proofs, where the implication -> takes the role of the inference rules, and a generic ph takes the role of the context. Then pm3.22 would correspond to your theorem. Its statement is written:\npm3.22 $p |- ( ( ph /\\ ps ) -> ( ps /\\ ph ) ) $=\n\nBut of course, from a proof theoretical point of view, that's a different statement.", "meta": {"post_id": 447, "input_score": 8, "output_score": 8, "post_title": "What is the closest you can get to introducing and discharging hypotheses in metamath?"}}
{"input": "What is the difference between intensional vs extensional type theories and how come the type checking is undecidable for extensional type theory? Also, how does it affect the expressiveness of theorem provers?", "output": "Extensional type theory is characterized by the reflection rule, which says that if the identity type ${\\rm Id}(a,b)$ is inhabited, then $a\\equiv b$ ($a$ and $b$ are judgmentally equal).  It is called extensional type theory because this means that the judgmental equality coincides with the identity type, and the latter is extensional (or, at least, more extensional than the judgmental equality would be in the absence of the reflection rule --- just how extensional it is depends on whether you have principles like function extensionality and univalence).  Intensional type theory is so-called because its judgmental equality is intensional, whereas its identity types can be even more extensional than those in extensional type theory (because the reflection rule is incompatible with the \"strongest extensionality principle\", namely univalence).\nIn a dependent type theory, type-checking is complicated because of the conversion rule that if $a:A$ and $A\\equiv B$ then $a:B$.  This essentially requires that a type-checking algorithm must include an algorithm for checking judgmental equality.  When combined with the reflection rule, this means a type-checking algorithm would have to include an algorithm for checking inhabitation of a type (namely the identity type).  But inhabitation of types can be used to encode the truth of arbitrary mathematical statements, so (e.g. by the halting problem) it is impossible to have a terminating algorithm for checking inhabitation of types, and hence impossible to have a terminating type-checking algorithm for extensional type theory.\nThere is a theorem, due I believe to Martin Hofmann (Conservativity of Equality Reflection over Intensional Type Theory), that extensional type theory is conservative over intensional type theory extended by axioms of uniqueness-of-identity-proofs and function extensionality.  Thus, there is no difference in the \"expressiveness\" of these theories.  Given the undecidability of type-checking for extensional type theory, therefore, very few proof assistants have tried to implement it.", "meta": {"post_id": 470, "input_score": 9, "output_score": 9, "post_title": "Type Checking Undecidable in Extensional Type Theory"}}
{"input": "How can I prove that the matched pattern is equal to the original argument?\nFor example, in the following function, what can I write instead of sorry to prove the equality?\ndef e1 (x: Option Nat): Nat :=\n  match x with\n  | none => 1\n  | some y =>\n    have : some y = x := sorry\n    y+1\n\n\nThe only option I could come up with was using a pair with an equality proof, but that seems a bit too complex:\ndef e2 (x: Option Nat): Nat :=\n  let x' := x\n  have x'_eq: x' = x := rfl\n  match x', x'_eq with\n  | none, _ => 1\n  | some y, y_eq =>\n    have : some y = x := y_eq\n    y+1\n\nIs there an easier way to do this?", "output": "In Lean 3, you can solve this using the following syntax:\ndef e1 (x : option \u2115) : \u2115 :=\nmatch _, rfl : \u2200 y, y = x \u2192 _ with\n| none, h := 1\n| some y, h :=\n  have some y = x := h,\n  y+1\nend\n\nThe match x, y : T with syntax means \"generate a function of type T called e1._match or similar, and then replace this match expression with e1._match x y\". T is usually therefore of the form X \u2192 Y \u2192 _, where _ is the type of the values within the match expression.\nNote that this is essentially the same as your solution, but the match expression lets us combine it all into one line.", "meta": {"post_id": 485, "input_score": 7, "output_score": 9, "post_title": "Preserve equality with match expression"}}
{"input": "Theorem to prove:\n\nThe sum of the binomial coefficients over an antidiagonal is a Fibonacci number. More specifically, the $n$th antidiagonal sums to the $n+1$th Fibonacci number, where the antidiagonal is counted from zero and the Fibonacci sequence starts with $F_0 = 0$ and $F_1 = 1$.\n$$ \\forall\\, n \\in \\mathbb{N}, \\quad \\sum_{i=0}^{n}{{i}\\choose{n-i}} = F_{n+1} $$\n(from this code golf challenge)\n\n\nImage source: Math is Fun - Pascal's triangle\nBy digging through the mathlib I could formulate this equality as the following:\ntheorem antidiagonal_choose_sum_is_fib (n : \u2115) :\n  ((list.nat.antidiagonal n).map (function.uncurry nat.choose)).sum\n  = n.succ.fib\n\nwith the proof:\nimport data.nat.choose.basic\nimport data.nat.fib\nimport data.list.defs\nimport data.list.nat_antidiagonal\n\nlemma fib_ind : forall (P : \u2115 -> Prop) (p0 : P 0) (p1 : P 1) (pss : forall m : \u2115, P m -> P m.succ -> P m.succ.succ), forall n : \u2115, P n :=\nbegin\n  intros P p0 p1 pss n,\n  have pns : P n /\\ P n.succ,\n  induction n, tauto, tauto, tauto\nend\n\nlemma sum_zip : forall (f g : \u2115 -> \u2115) (l : list \u2115), (list.map f l).sum + (list.map g l).sum = (list.map (\u03bbx, f x + g x) l).sum :=\nbegin\n  intros, induction l,\n  simp,\n  simp, rw <- l_ih, ring,\nend\n\nlemma sum_eq_elementwise : forall (f g : \u2115 -> \u2115) (l : list \u2115), (forall x, x \u2208 l -> f x = g x) -> (list.map f l).sum = (list.map g l).sum :=\nbegin\n  introv, induction l,\n  simp,\n  intro h, simp, rw h l_hd, rw l_ih,\n    introv mem_tl, apply h, simp, tauto,\n    simp,\nend\n\ntheorem antidiagonal_choose_sum_is_fib (n : \u2115) :\n  ((list.nat.antidiagonal n).map (function.uncurry nat.choose)).sum\n  = n.succ.fib :=\nbegin\n  induction n using fib_ind with n_m n_ind0 n_ind1,\n  simp,\n  simp,\n  rw nat.fib_add_two, rw nat.add_one, rw <- n_ind0, rw <- n_ind1,\n  unfold list.nat.antidiagonal, simp, repeat {rw nat.add_one},\n  rw list.sum_range_succ, rw list.sum_range_succ', rw list.sum_range_succ _ (n_m.succ), simp,\n  rw <- add_assoc, rw sum_zip, simp, apply sum_eq_elementwise,\n  introv in_range, rw list.mem_range at in_range, have le_n_m := nat.succ_sub (nat.le_of_lt_succ in_range),\n  rw le_n_m, apply nat.choose_succ_succ\nend\n\nOnline demo is here.\nProof outline:\n\nTheorem: $ \\forall\\, n \\in \\mathbb{N}, \\quad \\sum_{i=0}^{n}{{i}\\choose{n-i}} = F_{n+1} $.\nUse Fibonacci induction.\nFor $n = 0$ and $n = 1$, use direct evaluation.\nFor $n = k+2$, assume the equation holds for $n = k$ and $n = k+1$.\nExpand $F_{k+3}$ once and substitute the two equations to get $$ \\sum_{i=0}^{k+2}{{i}\\choose{k+2-i}} = \\sum_{i=0}^{k}{{i}\\choose{k-i}} + \\sum_{i=0}^{k+1}{{i}\\choose{k+1-i}} $$\nChop off the first and last terms of the leftmost summation and the last of the rightmost one to get $$ \\sum_{i=0}^{k}{{i+1}\\choose{k+2-(i+1)}} + {{0}\\choose{k+2}} + {{k+2}\\choose{0}} = \\sum_{i=0}^{k}{{i}\\choose{k-i}} + \\sum_{i=0}^{k}{{i}\\choose{k+1-i}} + {{k+1}\\choose{0}} $$\nConstant terms evaluate to 1.\nHandle the sums element-wise: $$ \\forall\\, 0 \\le i \\le k, \\quad {{i+1}\\choose{k+1-i}} = {{i}\\choose{k-i}} + {{i}\\choose{k+1-i}} $$\nwhich follows from the identity of Choose.\n\nAs a proof review question, any advice to improve the proof is appreciated, including but not limited to:\n\na possible better formulation of the theorem\na possible better translation of the hand-written proof\na suggestion for a more readable formal proof\nnaming of lemmas and theorems (I'm aware that a naming convention exists in mathlib, but I'm not quite sure how it works)\npossible use of better lemmas in the course of the proof (is a more general lemma or a more specific lemma better?)", "output": "I'll first go through piece-by-piece and suggest some local improvements.\nFor the induction principle, when you find yourself doing intros as the first step of a tactic proof, you should strongly consider putting arguments \"before the colon\" rather than leaving them within universal quantifiers.  Another thing is that when you have multiple goals (for example, as produced by have or induction) it is good practice to use curly braces to focus on the goal -- that way when the proof breaks you can more easily figure out what needs fixing. And, a tip: if you are using the same tactic on each goal produced by a tactic, you can use the ; operator.\nlemma fib_ind (P : \u2115 \u2192 Prop)\n  (p0 : P 0) (p1 : P 1) (pss : \u2200 m : \u2115, P m \u2192 P m.succ \u2192 P m.succ.succ) (n : \u2115) : P n :=\nbegin\n  have pns : P n /\\ P n.succ,\n  { induction n; tauto },\n  tauto,\nend\n\nFor naming, we can refer to mathlib itself: it's nat.two_step_induction, so we can replace the tactic proof with the term nat.two_step_induction p0 p1 pss n. Note: the P argument would more commonly be given as an implicit argument.\nThe next lemma gets a similar treatment, but, with list.map, you can make use of dot notation: l.map f means \"since l has type list, use list.map and have l be its first list argument.\" Since this lemma is about a homomorphism property, I think it would normally be written with the equality flipped, and for naming, this might be unnecessarily long, but sum_map_apply_add_apply would work (one way mathlib names arise is to read off relevant parts of the in-order traversal of the abstract syntax tree).  Also, non-terminal simps should be avoided, since if the simp set later changes, you can have a hard time fixing a proof.  I used squeeze_simp to figure out which lemmas it was using, and I inserted *, which means \"and rewrite using hypothesis from the local context.\"\nlemma sum_map_apply_add_apply (f g : \u2115 \u2192 \u2115) (l : list \u2115) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nbegin\n  induction l,\n  { simp, },\n  { simp only [list.sum_cons, list.map, *],\n    ring, },\nend\n\nHowever, searching the documentation for \"list sum map add\", I came across multiset.sum_map_add. The proof in the source code (after interpreting what the to_additive attribute does) essentially carries over to lists. After generalizing it appropriately, this should probably find its way into mathlib:\nlemma list.sum_map_add {\u03b9 \u03b1 : Type*} [add_comm_monoid \u03b1] {f g : \u03b9 \u2192 \u03b1} (l : list \u03b9) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nlist.sum_hom\u2082 _ _ add_add_add_comm (add_zero _) _ _\n\nThe next lemma boils down to the fact that l.map f = l.map g (a congruence lemma), so searching for \"list map congr\" I got this:\nlemma sum_eq_elementwise (f g : \u2115 \u2192 \u2115) (l : list \u2115) (h : \u2200 x, x \u2208 l \u2192 f x = g x) :\n  (l.map f).sum = (l.map g).sum :=\nbegin\n  rw list.map_congr h,\nend\n\nFor the main theorem, there is a lot of unfolding of definitions. It's better to encapsulate those into some additional lemmas, since mathlib style is to try to avoid relying on definitions unless they're considered part of the interface (this isn't always clear).  Two lemmas that seem useful are for popping terms off the front and back of the antidiagonal:\nlemma list.nat.antidiagonal_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 1) =\n  ((list.nat.antidiagonal n).map (prod.map id nat.succ)) ++ [(n + 1, 0)] :=\nbegin\n  simp only [list.nat.antidiagonal, list.range_succ, add_tsub_cancel_left, list.map_append,\n    list.append_assoc, tsub_self, list.singleton_append, list.map_map, list.map],\n  congr' 1,\n  apply list.map_congr,\n  simp [le_of_lt, nat.succ_eq_add_one, nat.sub_add_comm] { contextual := tt },\nend\n\nlemma list.nat.antidiagonal_succ_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 2) =\n  (0, n + 2) :: ((list.nat.antidiagonal n).map (prod.map nat.succ nat.succ)) ++ [(n + 2, 0)] :=\nbegin\n  induction n with n ih,\n  { refl, },\n  { rw [nat.succ_add, list.nat.antidiagonal_succ, ih],\n    simpa, },\nend\n\nIn these, I'm (somewhat) misusing a feature of simpa, which is that it basically tries refl at the end. (These lemmas should also probably find their way into mathlib, but I'm not too familiar with the list antidiagonal functions. Edit: mathlib#12028 and mathlib#12029)\nWith these, we can simplify the main proof.  I flipped the equality because I imagine that the main use would be to take existing fibonacci numbers and rewrite them into this other form. I also changed n.succ to n + 1 since that tends to be the normal form for arithmetic expressions. By the way, it's common to open function, so uncurry doesn't need to be fully qualified.\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = ((list.nat.antidiagonal n).map (uncurry nat.choose)).sum :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, list.nat.antidiagonal_succ_succ', list.nat.antidiagonal_succ'],\n  simpa [\u2190add_assoc, \u2190list.sum_map_add, uncurry, \u2190nat.choose_succ_succ],\nend\n\n\nNow that we've gone through everything, let's put it all together. We have three lemmas that arguably should be in mathlib already, and then your proof is pretty close to what you said in your outline:\nimport data.nat.choose.basic\nimport data.nat.fib\nimport data.list.defs\nimport data.list.nat_antidiagonal\n\nopen function\n\nlemma list.sum_map_add {\u03b9 \u03b1 : Type*} [add_comm_monoid \u03b1] {f g : \u03b9 \u2192 \u03b1} (l : list \u03b9) :\n  (l.map (\u03bb x, f x + g x)).sum = (l.map f).sum + (l.map g).sum  :=\nlist.sum_hom\u2082 _ _ add_add_add_comm (add_zero _) _ _\n\nlemma list.nat.antidiagonal_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 1) =\n  ((list.nat.antidiagonal n).map (prod.map id nat.succ)) ++ [(n + 1, 0)] :=\nbegin\n  simp only [list.nat.antidiagonal, list.range_succ, add_tsub_cancel_left, list.map_append,\n    list.append_assoc, tsub_self, list.singleton_append, list.map_map, list.map],\n  congr' 1,\n  apply list.map_congr,\n  simp [le_of_lt, nat.succ_eq_add_one, nat.sub_add_comm] { contextual := tt },\nend\n\nlemma list.nat.antidiagonal_succ_succ' (n : \u2115) :\n  list.nat.antidiagonal (n + 2) =\n  (0, n + 2) :: ((list.nat.antidiagonal n).map (prod.map nat.succ nat.succ)) ++ [(n + 2, 0)] :=\nbegin\n  induction n with n ih,\n  { refl, },\n  { rw [nat.succ_add, list.nat.antidiagonal_succ, ih],\n    simpa, },\nend\n\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = ((list.nat.antidiagonal n).map (uncurry nat.choose)).sum :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, list.nat.antidiagonal_succ_succ', list.nat.antidiagonal_succ'],\n  simpa [\u2190add_assoc, \u2190list.sum_map_add, uncurry, \u2190nat.choose_succ_succ],\nend\n\n\nAll that said, using lists isn't the usual way you work with sums in mathlib. Instead, you work with finset and the \"big operators.\" Contingent on the mathlib PR for manipulating antidiagonals, the theorem ends up being just this:\nimport data.finset.nat_antidiagonal\nimport data.nat.fib\nimport algebra.big_operators.basic\n\nopen_locale big_operators\nopen finset\n\ntheorem fib_eq_sum_choose_antidiagonal (n : \u2115) :\n  (n + 1).fib = \u2211 p in nat.antidiagonal n, nat.choose p.1 p.2 :=\nbegin\n  induction n using nat.two_step_induction with n h0 h1,\n  { refl, },\n  { refl, },\n  rw [nat.fib_add_two, h0, h1, nat.antidiagonal_succ_succ', nat.antidiagonal_succ'],\n  simp [nat.choose_succ_succ, add_assoc, add_left_comm, sum_add_distrib],\nend\n\nAfter doing the basic manipulations you described in your proof outline, simp ends up doing the heavy lifting. An interesting thing about this tactic is that it's able to put things into a normal form even if the lemmas can potentially form rewrite loops. The add_assoc and add_left_comm lemmas here are giving simp the capability of sorting the additions, as a weak kind of ring. (Usually you'd include add_comm, too, but it wasn't necessary here.)", "meta": {"post_id": 511, "input_score": 14, "output_score": 14, "post_title": "Proof review: Sum of nCk over antidiagonal = Fibonacci"}}
{"input": "The $\\lambda\\mu$-calculus is a variant of the $\\lambda$ calculus introduced by Parigot to capture classical logic. The Wikipedia article describing it in more detail is here. In modern proof assistants, it is expected that the user should opt-in to breaking canonicity via adding their own axioms if they want to enjoy classical reasoning.\nThere is an alternative world though, where classical logic is the basis for the core logic. The $\\lambda\\mu$-calculus is one such calculus that allows this to be possible. I am interested in it specifically because a $\\lambda\\mu$-cube was constructed for it without much difficulty. It is known that given the Calculus of Constructions that a proof assistant with inductive types is only a stone's throw away by adding on strong sigmas, bolting on an inductive datatype theory, or some other combinations of core types.\nThere are existing approaches that use classical reasoning such as HOL and other LCF logics where a classical logic can be embedded. However, I am interested in the class of proof assistants that are on equal footing with Coq, Agda, and Lean. That is, they ought to have dependent types at the very least.\nYet, I do not know of any proof assistants that have tried (and definitely none that have had any success!) in using the $\\lambda\\mu$-calculus as the computational basis. My question is thus in two parts: First, what difficulties might one run into when attempting this? Second, has anyone tried it?\nI want to dispense with the obvious objection of normal forms. The calculus does not have a strong notion of normal form (or at least, you have to choose between confluence and normal forms). This is not a show-stopper, a designer can pick confluence and use that for definitional equality while allowing propositional equality to \"repair\" the lack of strong normal forms. I do not think even usability will be hurt, as I imagine the $\\lambda$-calculus fragment can still enjoy $\\beta\\eta$-equality. If my assumptions here are wrong I would be interested to know.", "output": "I think that the first question can be generalized to \"What difficulties are there in adding side-effects to a proof assistant based on dependent type theory.\"\nFirst-class continuations are just one particular side-effect\nThe \u03bb\u03bc-calculus is only an instance of a more general class of extensions of the \u03bb-calculus with side-effects. There is no unique definition of what a side-effect is, but a good generic approximation is \"any feature of the language that makes the difference between call-by-name and call-by-value observable\". One can for instance mention exceptions, mutable state, IO, threads and whatnot.\nThe effect introduced by the \u03bb\u03bc-calculus is the notion of first-class exceptions. The \u03bc binder, indeed, allows to capture the current continuation,  which can be resumed at a later time with the [\u03b1]t construct. This is not possible in the standard \u03bb-calculus, and it is precisely what introduce classical abilities in this system.\nIn PLT, we ain't afraid of no effects\nThis other answer insists at length, and rightfully, on the fact that first-class continuations break the CbN-CbV equivalence and thus should be endowed with a stricter reduction strategy to preserve the equational theory. This is not a problem tied to dependent types specifically, but rather by definition the landmark of side-effects.\nNow, dependent types rely on the runtime semantics of the proofs, so the degeneracy of the equational theory percolates immediately in the logic. But the very same problem happens already with effectful non-dependent programming languages, and there, it is folklore that one needs to care about the exact reduction strategy. No sane programmer would expect a call-by-name equation to hold in their favourite ML implementation.\nSo in this regard, I believe that that answer is somewhat missing the really problematic point.\nI Can Haz callcc?\nDependent type theories are biased towards call-by-name. The one rule that allows lifting computation to typing is the conversion rule. But this rule bakes in the fact that conversion is call-by-name, as it is generated by the unrestricted \u03b2-rule.\nSo one could expect that throwing side-effects into MLTT is going to be fine, since it already made a choice of strategy for you. Obviously you have to be careful that indeed your effects are call-by-name, but apart from this technical detail it should be fine, shouldn't it?\nAlas! If you do this with callcc (which is essentially a flavour of \u03bb\u03bc-calculus) you get an inconsistent theory. What went wrong?\nThe real problem with effects and dependent types\nWhat really separates dependent type theory from other higher-order logics is that it features inductive types equipped with large dependent elimination. The latter is what allows the user to write weird types like forall (b : bool), if b then nat else empty where the shape of a type really depends on the value of a term.\nThis is precisely this feature that causes the issue with side-effects, and there is a simple intuitive and mostly correct explanation for this phenomenon.\nAs OP observed, side-effects break the canonicity property of the theory. A term, while reducing to a value, may perform side-effects that are observable. Thus, for instance, not all terms M : bool are convertible to either true or false in presence of effects. A typical PLT-centric way to phrase that is the progress lemma, which says that terms are either values or reduce. And indeed, values of an inductive type have the right shape, but by contrast terms can be wild.\nNow, large dependent elimination bakes in the fact that arbitrary terms behave like values. For booleans, for instance, it states that P true and P false are enough to prove forall b : bool, P b for any P : bool -> Type. But obviously, in presence of side-effects there are non-standard boolean terms that are neither true nor false.\nNo wonder why this principle results in an inconsistent theory!\nEmbracing effects\nEffectful dependent types are a pretty niche area. Still, we do have some proposals for it to work properly.\nSome effects can be added relatively straightforwardly and justified by a syntactic translation. As already argued, we need to do something with large elimination or face inconsistency. We believe that a simple linearity criterion is enough to separate the wheat from the chaff.\nLinearity is the critical insight of Baclofen Type Theory (BTT), a restriction of MLTT compatible with side-effects. In a nutshell, BTT restricts large elimination to predicates P : bool -> Type that are linear, i.e. which satisfy an equation ensuring that they actually behave as if they were evaluating their argument in a call-by-value / strict way.\nToo good to be true?\nUnfortunately, while BTT seems to validate a large class of effects, we still do not have a reasonable syntactic model of BTT for first-class continuations. The reason are technical and tied to the impredicative nature of the CPS monad. Yet, we believe that it may be possible to present a variant of BTT with first-class continuations as a standalone theory whose properties are proved directly on the syntax. This requires more work, and novel research, which is why there is no such thing yet.\nIn any case, BTT might not be a theory you want to work in.\n\nFrom a mathematical standpoint, it really puts emphasis on the potentially effectful nature of proofs-as-programs. This is clearly at odds with the standard mathematical practice.\nFrom a programming standpoint, mixing call-by-name and effects is not very nice. You always have to program defensively and rely on weird storage operators to ensure you actually get a value.\n\nIt might be more tenable in the long term to switch to a call-by-value (or more generally, call-by-push-value / polarized) type theory. Nonetheless, this move would only make the life of the computer scientist easier, while making the whole experience even more alien to the mathematically inclined user.", "meta": {"post_id": 541, "input_score": 19, "output_score": 13, "post_title": "What difficulties are there in basing a proof assistant on the $\\lambda\\mu$-calculus and has any proof assistant tried?"}}
{"input": "I want to simplify the expression 0 * 1 * 1 * 1 * 0 using simp only [mul_zero, zero_mul]. I would like mul_zero to fire first, so that the entire expression is simplified to zero in only one step. If mul_zero fires first, I would expect the simplification process to take 4 steps, due to there being one step to eliminate each 1.\nIs there a way I can tell Lean to prioritize which simplification lemma to try first?", "output": "Using mathlib's simp_rw, you can order your lemmas:\nexample (a : nat) : a + 0 * 1 * 1 * 1 * 0 = a := by simp_rw [mul_zero, add_zero]\n\nsimp_rw [lemma1, lemma2] does the hand-waved equivalent of simp only [lemma1], simp only [lemma2].", "meta": {"post_id": 550, "input_score": 10, "output_score": 9, "post_title": "Can Lean simp arguments be ordered?"}}
{"input": "Isabelle has the frontend Isar which mimics some features of the Mizar system.\nI'm curious if Coq has anything similar, i.e. an alternative to tactic scripts that's designed to be readable or similar to the mathematical vernacular.\nThis discussion on the Coq forums mentions Czar and SSReflect.", "output": "There indeed used to be a \"declarative language for Coq\" often referred to as Czar, but it was removed (from Coq's core) a long time ago. If it had had any actual users, it would have been easy at the time to restore it as an external plugin though.\nHaving tested it when it was still there, my personal impression was that it was actually very difficult to use, and nowhere near what Isar has achieved for Isabelle. My understanding is that one of the main reason why Isar was so successful is thanks to SledgeHammer.\nFortunately today, Coq has CoqHammer (which is even available as part of the Coq Platform). Furthermore, Coq's tactics are pretty flexible for different styles. So, with a combination of tactics such as assert, enough, now_show, and CoqHammer tactics, it is actually possible to reconstruct the Isar style in Coq (no need for a specific proof mode for this).\nAnother declarative-style set of tactic is provided by the coq-waterproof package (see an example in their README). The proof style looks very much like mathematical text. However, while Czar was suffering from a lack of IDE support (it's almost impossible to remember exactly how to write a \"natural language\" proof in what is in fact a controlled language), coq-waterproof is a support package for the Waterproof IDE (which is targeted at students). So this might be close to what you are looking for, but for pedagogical purposes. If the goal is to reproduce the Isar style with productivity in mind, then I would still rather recommend relying on Coq's tactics like assert and enough + CoqHammer.\nFinally, SSReflect was also mentioned. It is in fact very different from a declarative mode because it is very \"procedural\". However, the style in which SSReflect users write their proof is supposed to be close to the style of proofs in mathematical texts (one line per significant mathematical step) and reliance on the have: tactic to clarify the most important steps. The main difference between the SSReflect style and the Isar style is that SSReflect only relies on very small-scale automation, making the proofs in SSReflect very robust to evolution in the underlying Coq language, libraries or specifications.", "meta": {"post_id": 571, "input_score": 10, "output_score": 11, "post_title": "Is there a Mizar-like sublanguage for Coq?"}}
{"input": "Ever since the work by Gimenez for his PhD thesis, Coq has supported positive coinductive types. For example, the type of always-infinite streams containing elements of type A and accessor functions for it can be written:\nCoInductive Stream (A : Type) := Cons : A -> Stream A -> Stream A.\nDefinition hd A (s : Stream A) : A := match s with Cons _ x _ => x end.\nDefinition tl A (s : Stream A) : Stream A := match s with Cons _ _ s => s end.\n\nThis encoding allows us to prove from first principle:\nLemma Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\nProof.\nintros A s; case s; simpl; reflexivity.\nQed.\n\nHowever, since Coq version 8.9, the Coq manual advises to instead use negative coinductive types that require Coq's primitive projections:\nSet Primitive Projections.\nCoInductive Stream (A: Type) := Cons { hd : A; tl : Stream }.\n\nUsing this approach, Cons_hd_tl from above is no longer provable, and we must add it as an axiom:\nAxiom Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\n\nThe reason for nevertheless preferring negative coinductive types is that, unlike their positive counterparts, they do not break the subject reduction property of Coq's underlying formal system.\nEven though they are advised against, positive coinductive types are not officially deprecated in Coq's implementation, and remain in heavy use in high-profile projects such as CompCert. Converting such developments to use negative coinductive types would likely require significant effort.\nHow important is the breaking of subject reduction in practice, and can both negative and positive coinductive types be supported in Coq for the long term?", "output": "Intuition of the problem\nA good rule of thumb is to consider that in intensional type theory, coinductive types and function types share a lot of properties. In particular,  equality over streams is equally as evil as equality over functions, in so far as it almost never holds without assuming additional extensionality principles. The only way to prove such an equality is essentially by reflexivity.\nAs observed in the question, the current typing rules of coinductive types allow proving eta-expansion as a theorem\nAxiom Cons_hd_tl : forall A s, s = Cons A (hd A s) (tl A s).\n\nwhich does not hold by reflexivity for any closed stream s, thus breaking subject reduction.\nThe deep root of the issue comes from the dependent elimination rule of coinductive types, since it allows proving that any coinductive term is necessarily a constructor. This is clearly breaking parametricity, as there are closed terms of coinductive types that do not even start with a constructor, namely cofixpoints. The SR failure is a consequence of this.\nWhy Subject Reduction?\nThere are various reasons to desire SR, some of them theoretical, some of them practical.\n\nOn the theoretical side, it makes the semantics of the type theory hard to define naturally. The soundness of an implementation of a type-checker for such a theory becomes quite murky as a consequence. One has to defensively guard against SR breakages, or face weird ill-typed situations otherwise. The issue is much more important than in non-dependent type systems, because the typing rules themselves embed the runtime semantics of the program, hence any defect of the latter percolates into the former.\nOn the practical side, this is not a black-and-white issue. It actually depends on the magnitude of SR breakage. One can survive within a locally broken system (case in point: Coq), but the more failures of SR, the less practical the proof assistant. Indeed, failure of SR means that type-checking is not stable by the equational theory of the language, a clear source of non-modularity. A perfectly fine proposition may become ill-typed after substituting a subterm by another convertible one. This is particularly infuriating when it happens because it is virtually impossible to debug, as one has to manually apply the typing rules to see where it fails.\n\nThankfully, in Coq, the practical issues of SR are exceedingly uncommon, for two reasons. First, the use of coinduction in Coq is somewhat marginal compared to the rest of the features of the system. Second, even developments using coinduction rarely rely on the computational content of the proofs and most coinduction is performed in opaque Prop proofs. Yet, the theoretical defects are still annoying. For instance, the MetaCoq formalization of CIC uses SR in critical parts. Thus implementing coinductive types as found in today's Coq will be problematic.\nSolutions\nThere are two possible solutions to recover SR for positive coinductive types, a brutal one and a finer-grained one.\nThe not-reasonable one\nThe brutal one is to eschew decidability of type-checking altogether and allow infinite unfolding of (co)fixpoints. While fixing SR, this is not reasonable. Decidability of type-checking is pretty much the core of the claim of the foundational status of proof systems like CIC. Furthermore, any actual implementation of an undecidable type-system will eventually fail at some point, be it intentionally (e.g., if the system implements some kind of timeout) or unintentionally (e.g., because of stack overflow or out-of-memory errors). This is intuitively a failure of SR in disguise, and implicitly indicates that there are now two logical systems, the \"ideal\" one and the \"implemented\" one.\nThe reasonable one\nThe cleverer solution is to attack the problem at the root, which was argued above to be dependent elimination of positive coinductive types. It is evil to assume that a coinductive term is always a constructor, the only thing that is allowed is to depend on the observations that one can perform with it. In the case of negative coinductive types, observations are primitive notions defined to be the projections of that type, so this is guaranteed by construction. For their positive flavour, one has to be slightly more imaginative and stare at the typing rule of dependent elimination.\nI have a proposal implemented as a PR that, to the best of my understanding, is enough to restore SR. The idea is to restrict the return type of coinductive pattern-matching in a way that is reminiscent of the restriction of inductive dependent pattern-matching in presence of side-effects.\nThe criterion is the following: assume w.l.o.g. an indexed coinductive type T : forall i : I, Type. A pattern-matching with return clause P : forall i : I, T i -> Type is valid iff it satisfies the following definitional equation:\ni : I, x : T i \u22a2 P i x \u2261 P i (cofix F := x).\nNow, to see why this prevents the SR failure, one has to peek into the semantics of (co)fixpoints. In Coq, you cannot unfold arbitrarily (co-)fixpoints, otherwise this leads to non-decidability of typechecking. There is thus a syntactic guard for (co-)fixpoint unfolding, namely:\n\nfor inductive fixpoints, they only unfold when applied to something that starts with a constructor, e.g.\n(fix F := t) (S n) \u2261 t{F := (fix F := t)} (S n)\nfor co-inductive cofixpoints, they only unfold when they are inside a pattern-matching, e.g.\nmatch (cofix F := t) with p \u2261 match t{F := (cofix F := t)} with p\n\nThe latter corresponds intuitively to the fact that the cofix is being forced.\nNow, assume some dependent pattern-matching over a term of a coinductive type T. In general, this pattern-matching is of the shape match t as x return P x with p. Its type is thus P t. But now, assume that t \u2261 (cofix F := t\u2080), in which case the unfolding rule mandates that expression is convertible to match t\u2080{F := (cofix F := t\u2080)} as x return P x with p, which now has type P (t\u2080{F := (cofix F := t\u2080)}).\nTherefore, for SR to hold it is a necessary condition to have P (cofix F := t\u2080) \u2261 P (t\u2080{F := (cofix F := t\u2080)}).\nBecause t\u2080 is arbitrary, this could very well be a variable as well. The criterion is based on this observation. While I have no mechanized proof of this, I convinced myself that this is actually a sufficient condition for SR to hold in general.\nConsequences\nIf this PR gets merged into Coq, it will restore SR at the cost of making dependent elimination on positive coinductive types quite weird. This is expected, since one must not be able to prove the eta-expansion theorem in CIC. It is safe to add it as an axiom, still, just as function extensionality can be posited.\nI am not sure it will be that practical because it also means that many proofs, e.g., the ones using destruct over coinductive terms without care for the convertibility criterion, will need to be ported at some point. The PR provide a flag to allow local breakage of SR, but eventually this flag will be removed. Also, this side-condition makes the elimination rule of coinductive types non-modular, because one cannot abstract over an arbitrary return predicate, since it would not satisfy the criterion.\nThus, I still believe that in the long run it is more natural to switch to negative coinductive types, even if we restored SR.", "meta": {"post_id": 583, "input_score": 22, "output_score": 18, "post_title": "What is the state of coinductive types and reasoning in Coq?"}}
{"input": "The Stack Exchange bot reminded me that I had committed myself to asking some questions, but please allow a possibly naive question, possibly of a philosophical nature rather than mathematical/computer-theoretical.\nSo, is it conceivable that the development of proof assistants make mainstream mathematicians switch their framework, from a set theoretical one, to a type theoretical one.\nIt is not that mathematicians all know about the axioms of ZFC, I'm certain most don't and most don't care, but mathematics are full of idiosyncratic set-theoretical gimmicks such as saying that a group is a 3-tuple consisting of a set, a law, a neutral element, and some people (most students) pay attention to the fact that it is a 3-tuple and not something else. (I have been asked which component of a similar 5-tuple a submodule should be\u2026)\nType theory (especially how it is implemented) simultaneously departs from such constructions, and insists on the list of objects/axioms (so that the computer-definition of a group also contains a proof of associativity, etc.).\nOn the other hand, the concept of a set is clearly an important one.\nSimilarly, some constructions are built on sets in a way which might look unnatural \u2014\u00a0say defining the quotient set as a set of equivalence classes \u2014\u00a0but are now well understood, and can be avoided by a more categorical point of view.\nI also have in mind more advanced notions of algebra/number theory where set theoretical notions look less avoidable, such as the definition of an ideal. (One could try to replace them by the epimorphism to the quotient ring, but that does not look very easy to handle.)", "output": "The following is a made up opinion piece based on the observation of a statistically small sample of mathematicians. Please take it with a grain of salt.\nA typical working mathematician has little time for and only cursory interest in the foundations of mathematics. They are able to operate perfectly well without stressing over formalities of whatever foundation they imagine standing on.\nThey don't particularly care what foundation they are using, as long as it does not interfere with their work. They think of a foundation as a necessary thing that is useful on a grand scale of things, but is not really crucial in their everyday work. Often they profess using a particular foundation, but their mathematical texts are closer to another one. What I am saying is that mathematicians do not actually use foundations that they could switch between.\nWhen such mathematicians start using proof assistants, they are forced to pay attention to foundations. They become more proficient in logic, they learn about introduction and elimination rules, they learn that some proofs are better than others etc. But what really matters, in my opinion, is that they actually start to believe that proofs are mathematical objects. Instead of just converting coffee to proofs, they start thinking about proofs. Even better, instead of just applying techniques and tricks to do math, they start thinking about the techniques and tricks, and how to express them in proof assistants.\nTo address the question directly, I would say that the typical experience is not that of \u201cswitching from one framework or foundation to another\u201d but rather \u201cactually learning a foundation and adopting it as a real working tool\". If you write your proofs in LaTeX then you have little direct use for foundations, but if you write your proofs in a proof assistant then the foundation and the proof development techniques are your new everyday tools.\nLet me also point to Penelope Maddy's paper What Do We Want a Foundation to Do? in which she nicely identifies the different purposes and roles that a foundation may have. While I do not entirely agree with her assessment of the role of univalent foundations, I appreciate her point that there's a difference between a foundation that studies what can be formalized in principle, and one that is supposed to be used to actually formalize mathematics. This difference may explain why the foundations of the early 20th century do not fit our current needs.", "meta": {"post_id": 596, "input_score": 8, "output_score": 12, "post_title": "Can the development of proof assistants make mathematicians switch their framework?"}}
{"input": "I noticed that Coq 8.15 (possibly 8.14) made some significant changes to divmod. In particular, Nat.divmod_0q0 seems to have been removed and the div_mod lemma with signature y<>0 -> x = y*(x/y) + x mod y has effectively been replaced with div_mod_eq which drops the y<>0 precondition.\nDoes divmod (and hence div and mod) now behave differently? Could somebody summarize the changes? It looks like some 8.15 theorems simply don't hold in 8.13 and vice-versa (with Nat.divmod_0q0).", "output": "This was changed in Coq 8.14 (MR: https://github.com/coq/coq/pull/14086), and the changes are summarized in https://coq.inria.fr/refman/changes.html#id310. Quoting:\n\nChanged: set n mod 0 = n uniformly for nat, N, Z, int63, sint63, int31 such that m = (m / n) * n + (m mod n) holds (also for n = 0)\n\nAs you can see, in many cases this gives stronger theorems with fewer side conditions \u2014 but in any case they're more uniform across nat, N and Z.", "meta": {"post_id": 600, "input_score": 11, "output_score": 11, "post_title": "What did Coq 8.15 change about divmod?"}}
{"input": "What are some useful and reliable resources for a mathematician interested in learning Isabelle/HOL? Could be online (websites) or physical (books).", "output": "Once you installed Isabelle (following these instructions), you will have all the materials listed on the tag isabelle as a local copy. Definitely go through this one for a good (field agnostic) introduction:\n\nProgramming and Proving in Isabelle/HOL\n\n(you won't need to go through the whole document)\nOnce you are familiar with its syntax, the most common tactics and Isar you will find it most helpful to read actual code. You will find good examples by pressing Ctrl and clicking on types or constants. (as an exercise, you could click on set which leads you to HOL-Set)\nSimilarly, you can find other good examples in HOL-Examples.\n\nAnother most useful resource for proofs is the AFP where you will find proofs of all sorts. Many of the theories there will come with an actual paper (not just the LaTeX generated by Isabelle).\n\nJust now, I also found this resource: Formalising Mathematics \u2013 in Praxis; A Mathematician\u2019s First Experiences with Isabelle/HOL and the Why and How of Getting Started which shows some very beautiful Isar proofs and could be useful (I only skimmed it).\nIt also has some good references that are worth to track down, for example Larry Paulson's formalization(s) of G\u00f6del's Incompleteness theorem (arxiv) one of which makes use of Isabelle/HOL's Nominal 2 framework (there is also a less powerful version shipped with Isabelle which can be found here).", "meta": {"post_id": 623, "input_score": 8, "output_score": 8, "post_title": "What are some useful resources for a mathematician interested in learning Isabelle/HOL?"}}
{"input": "Are there tools in mathlib which let you give computations of integrals which would roughly follow standard methods for solving them? For now let me restrict attention to some undergrad-level integrals, like $\\int_0^1xe^x\\,dx$, and not anything excessively crazy.\nOf course, if our goal is merely to have a proof of a statement of the form $\\int_a^bf(x)\\,dx=c$ for some known $c$, then proving this in Lean should be possible by finding a function $F(x)$, check $F'(x)=f(x)$, evaluating $F(b)-F(a)$ and appealing to FTC. But this is an approach which is very backwards compared to how this is done by hand (which, at least somewhat implicitly, involves finding such an $F$).\nMy question is, does Lean have any tools which would let you perform such \"by hand\" calculations within your code? At the very least it would require support for integration by parts and integration by substitution, and I'm not sure those are in mathlib. Ideally I would also imagine some kind of environment like the calc mode.\nIf not in Lean, are there similar tools available in other proof assistants?\nJust to clarify, I do not mean automated computation of integrals like in CAS. I just mean tools which would let you painlessly perform IBP and substitutions (and perhaps other operations) on given integrals to simplify them.", "output": "I'm going to inflate Mario's comment to an answer: You can absolutely write tactics in Lean to do this. What you'll be doing is creating a large part of a CAS, but as a Tactic. This would be largely a waste of effort to do in isolation.\nWhile Lean, especially Lean 4, would be an excellent language in which to create a CAS, this would likely be a rather independent project from mathlib. mathlib is explicitly about classical mathematics. Closed-form integration may be studied in first-year calculus, but it sits in a really odd place that straddles constructive, computational and classical mathematics. This hybrid universe of mathematics has not been studied much at all.", "meta": {"post_id": 629, "input_score": 21, "output_score": 11, "post_title": "How hard is computing integrals in Lean?"}}
{"input": "For most proof assistants the name alone is common, E.g. Coq, Lean but with Isabelle it is often seen as Isabelle/<xyz>?\nWhy the two parts?\nWhat is\n\nIsabelle/HOL\nIsabelle/FOL\nIsabelle/ZF\nIsabelle/ISAR\nIsabelle/Proof General\n\nAre there others?", "output": "Isabelle the program is designed as a \"generic\" LCF-style proof assistant/logical framework; it gives you a very minimal base logic (based on Church's simple type theory (pdf) and called Isabelle/Pure) and some infrastructure to declare new types, typeclasses (ref), constants, and judgment forms, postulate new axioms and rules, and basic tactics for performing logical reasoning therein.\nUsing this logical framework, and writing ML code (Poly/ML) to interface with it, one can develop object logics (pdf) and other applications x within Isabelle; these are then called \"Isabelle/x\".\nx = HOL, FOL, ZF, ... are object logics, they encode the rules of the particular logic in the language of Isabelle/Pure, and extend the framework with extra functionality for e.g. defining recursive functions, new tactics, theory libraries of formalizations, etc.\nx = Isar is an extension of the original Isabelle framework to allow for declarative proofs; it's now been very tightly integrated into Isabelle, but in the beginning proofs were done tactic-style, similar to how it's still done (I believe) in the HOL family of ITPs.\nx = Proof General is, I assume, the PG interface of the Isabelle prover.\nTo name a couple more interesting applications, there's Isabelle/auto2 by Bohua Zhan, it's a saturation-based theorem prover tool that enables very declarative proofs that don't require explicit tactic invocations after each claim. This allows for proofs that read quite similar to ones on paper, see e.g. here.\nI myself have tried to develop HoTT as an object logic, but it's not currently being actively worked on.\n\nI suppose this is similar to how Metamath works, but I'm not an expert on that.", "meta": {"post_id": 651, "input_score": 19, "output_score": 17, "post_title": "Why is Isabelle usually not just Isabelle but Isabelle/<xyz>?"}}
{"input": "As shown in an exercise here, we can prove the Gauss summation (that $\\Sigma_{i=0..n}{i} = n \\cdot (n + 1)/2$ ) in Isabelle/Isar using mathematical induction, as follows.\ntheorem\n  fixes n::nat\n  shows\"2 * (\u2211i=0..n. i) = n * (n + 1)\" \n  by (induct n) simp_all\n\nThe proof is a one-liner with much details hidden or handled automatically.\nHowever, this seems to be very different from the intuitive proof by Gauss that basically adds the sequence of numbers $1...n$, and a reversed copy of itself. The above Isabelle proof is a backward one, which is perfectly fine and is a textbook example of mathematical induction. But it still differs from the ideas of Gauss's famous proof, which seems to be a forward construction and then verification that it's what is wanted.\nI am just wondering:\nIs there a way to do a forward style proof that faithfully replicates Gauss's proof in Isabelle or similar proof assistants?\n(This example almost changed my perception of which proof style is simpler.)", "output": "Like @Bubbler, I think a nice forward proof can be done using a chain of equalities, to spell out each step:\ntheorem\n  fixes n::nat\n  shows\"2 * (\u2211i=1..n. i) = n * (n + 1)\" \nproof-\n  have \"2 * (\u2211i=1..n. i) = (\u2211i=1..n. i) + (\u2211i=1..n. i)\" by simp\n  also have \"\u2026 = (\u2211i=1..n. i) + (\u2211i=1..n. (n + 1 - i))\"\n    by (subst sum.atLeastAtMost_rev) simp\n  also have \"\u2026 = (\u2211i=1..n. (i + (n + 1 - i)))\"\n    using sum.distrib[symmetric].\n  also have \"\u2026 = (\u2211i=1..n. n + 1)\" by simp\n  also have \"\u2026 = n * (n + 1)\" by simp\n  finally show ?thesis.\nqed\n\nI took the liberty to start the summation at 1, not at 0, because I presumptuously expect that that\u2019s how Gauss thought about this list of numbers.", "meta": {"post_id": 682, "input_score": 8, "output_score": 10, "post_title": "How to prove Gauss summation of integers with proof assistants using forward reasoning?"}}
{"input": "I will describe a problem which I have in fact solved, but with considerable pain.  My question is whether there are better methods for dealing with similar issues.\nLean was giving me messages that did not seem to make sense.  After various manoeuvres to try to simplify and isolate the problem, I got a message like this (in which n1, r and k are all natural numbers).\ninvalid type ascription, term has type\n  (n1 * r) ^ k = n1 ^ k * r ^ k\nbut is expected to have type\n  (n1 * r) ^ k = ?m_1\n\nVisually, it appears that the term h = mul_pow n1 r k in question has precisely the expected type, so this message is confusing.  Eventually I entered set_option pp.all true, and thereby got a much longer and more explicit version of the above message.  From this I saw that in the goal, multiplication and powers were defined in the usual way for natural numbers.  However, in the term h there was all sorts of stuff about linear orders that I did not expect.  I later worked out that I had inadvertently given \u2115 an alternative monoid structure based on its structure as a lattice under the divisibility relation, and that that had been used by mul_pow when I defined h.  After removing the import that defined this unwanted instance, the problem went away.", "output": "People ask these on the Lean Zulip and because they're hard to debug my instinct is usually to try to help. Here's a checklist which I think describes my usual approach to the problem.\n\nWhen faced with issues of this nature the problem is almost always that the visually identical issues are not actually identical, like in your example; however sometimes they really are identical and the problem is a typeclass issue (e.g. the theorem you're trying to apply doesn't apply because even though the match should work, the theorem only applies to additive groups and you're trying to apply it to the naturals; unfortunately the error message you get is the same). So that's the first gotcha -- check that everything else is OK.\n\nSet pp.all true and verify (by eye, or perhaps using a text editor) that the visually identical terms really are not identical. Sometimes this is harder than it looks, because definitionally equal diamonds in mathlib (e.g. a field is a ring is a semigroup v a field is a monoid is a semigroup or whatever) might not look syntactically equal. You can look at the error message with pp.all true on and then literally copy the terms which you suspect that Lean is claiming are not definitionally equal and see if Lean can prove they're equal with rfl. If it can't, you know you've isolated the problem. Sometimes the terms really still do look equal; when this happens it can be a universe issue.\n\nSometimes the problem is easy to solve: e.g. if one term mentions _inst_4 and other one doesn't, but the other term mentions _inst_5 and the first one doesn't, then you have done something like made R a normed_ring and a division_ring, and both of these extend ring so you've given R two distinct ring structures and that's your problem. The correct approach here would be to make R a normed_division_ring.\n\nIf the terms are small then you can usually stare at them and figure out what's going on. You might have found a diamond in mathlib, and if you have then we'd be interested to know about it on the Lean Zulip; please minimise and post a fully working example.\n\nIf the terms are huge then go back to step 2, take those two terms which Lean is complaining that it can't prove are equal, and try and prove they're equal in tactic mode using the congr' tactic. This will match all the parts of the terms which do align and you might well be left with two much simpler terms which aren't definitionally equal; that's your problem.\n\nIf all else fails, remember that set_option pp.all true is just actually the following 11 options:\n\n\n* `pp.implicit true`\n* `pp.proofs true`\n* `pp.coercions true`\n* `pp.notation false`\n* `pp.universes true`\n* `pp.full_names true`\n* `pp.beta false`\n* `pp.numerals false`\n* `pp.strings false`\n* `pp.binder_types true`\n* `pp.generalized_field_notation false`\n\nYou can switch these options on one by one to see when things start diverging. The ones I usually start with are pp.notation, pp.implicit and pp.proofs.", "meta": {"post_id": 715, "input_score": 12, "output_score": 10, "post_title": "Lean: problems with visually indistinguishable instances"}}
{"input": "It seems to me that there are no real reasons to not like cumulativity (the example given here seems to not be too relevant, according to the comments), and yet most proof assistants (apart from Coq?) don't have cumulativity. Why is this so? This seems a very nice property mathematically, and is akin to how most people would think of universes intuitively.", "output": "The biggest problem in my head is that cumulativity opens the door of subtyping in type theory, which, IMO, a potential calamity to type theory. If you elaborate cumulativity to non-cumulativity + lifting, then it's probably fine, but let me say something about the subtyping-based approach.\nOnce you have subtyping, you'll start thinking about covariance and contravariance, which brings a lot more things to the type theory. Also, subtyping itself distinguishes two notions: consider $(a:A)\\in\\Gamma$, then \"the type of $a$\" and \"$A$\" are no longer the same under the context $\\Gamma$. Instead, \"the type of $a$\" is a subtype of $A$, and when you want to, for example, apply $a$ to the identity function with the type argument an implicit argument, then probably you can't solve the metavariable because $A$ won't be the most accurate solution (I thank Pavel for sharing some ideas with me about this).\nApart from that, cumulativity generates more complex universe level equations, which are harder to solve.", "meta": {"post_id": 716, "input_score": 24, "output_score": 10, "post_title": "Why is cumulativity not a given in proof assistants?"}}
{"input": "In the F* proof assistant, they use refinement types together with dependent types. Based on my impression of F*, it seems to me that refinement types are just predicates in dependent type theory that are like \"something something of type boolean should equal to true\". I think I'm probably (extremely) biased (and probably wrong), as I have only ever seen refinement types in one language.\nI am slightly more experienced with dependent types, and I'm wondering how are dependent types and refinement types related. Is there something that is only possible with refinement types, but not possible (or extremely hard to imitate) in dependent types?", "output": "Is there something that is only possible with refinement types, but not possible (or extremely hard to imitate) in dependent types?\n\nYes. Refinement types make the notion of logical or ghost term available. These are terms which can occur in types, but cannot directly be used for any computationally relevant purpose.\nConsider the following type for a function which takes a vector and returns how long it is.\nlength : \u03a0n: Nat \u2192 Vector n A \u2192 \u03a3m:Nat. m \u2261 n\n\nThis says that if you give the function a vector, it will return a number equal to the length of that vector. You can implement it like this in Agda:\nlength _ [] = (0, refl)\nlength _ (x :: xs) = let (k, pf) = length xs in \n                     (suc k, cong suc pf)\n\nwhich is a function which traverses the list and counts how many cons cells there are.\nUnfortunately, you can also write it like this:\nlength n _ = (n, refl)\n\nand just immediately return the index. In other words, you can't call the length function unless you tell it how long the list is! So there's no point to this function even existing.\nWith a refinement type discipline, you can write a function type like the following.\nlength : \u2200n: Nat \u2192 Vector n A \u2192 \u03a3m:Nat. m \u2261 n\n\nBy switching from pi to forall, I mean to indicate that the length argument is computationally irrelevant. (Note that this is not  proof-irrelevance! E.g., note the occurrence of n in m \u2261 n in the return value.) As a result, the first definition of length typechecks, but the terrible second definition won't.\nThis is super useful! (For example, termination metrics like Bove-Capretta accessibility predicates really want to be computationally irrelevant.)\nThe general theory of these things can be found in Noam Zeilberger and  Paul-Andr\u00e9 Melli\u00e8s POPL 2015 paper, Functors are Type Refinement Systems. Noam also has a set of OPLSS notes, which introduces these ideas more gently.", "meta": {"post_id": 755, "input_score": 9, "output_score": 11, "post_title": "What's the relationship between refinement types and dependent types?"}}
{"input": "When writing this answer, I had a hard time finding the lemma I needed to prove 2 \u2264 3. In Agda, when I have a goal, I can press refine and Agda will either insert a constructor or tell me she doesn't know which constructor to use, so I can pick one from the list. In Lean4 if I place a metavariable in the goal, she simply say \"don't know how to synthesize this\" (I'm paraphrasing, but it's something like that). Even without these error messages, I can click in the IDE to jump to the definition of the type, and I can read the definition to know the available constructors.\nIn Lean4 the situation is totally different. I looked up the definition of < and it's a macro or a notation definition, which I cannot jump to further definition (I tried clicking but it doesn't do anything). I looked around the code and found the definition -- it's LT, a class. So, to work with it, I need to know which instance I'm using in order to find the available constructors. However, I didn't find an easy way to lookup the instance used in my incomplete proof.\nI guess this is probably not the preferred way of programming in Lean4. I guess Lean4 wants me to work with mathematics abstractly, but I am unfamiliar with the Lean4 library and the Lean4 best practices. I can work pretty well with Cubical Agda library without prior knowledge to it by using the IDE extensively, and I wish to learn how can I do similar things in Lean4.\nPlease tell me anything I can do, even a bunch of keyboard shortcuts will help me a lot.", "output": "Here are a few ways of getting this goal without really knowing anything about what the definition of LT is etc.\nIn rough order of low-tech to high-tech, we have:\n\na tactic that tries to apply constructors of inductive types directly\nusing the simplifier (which is one of the main Lean workhorses)\na mathlib4 tactic that will search all available declarations for something that can be applied to your goal\n\nimport Mathlib.Tactic.LibrarySearch -- only for theorem c\n\ntheorem a : 2 \u2264 3 :=\nby\n  constructor\n  constructor\n\ntheorem b : 2 \u2264 3 :=\nby\n  simp\n\ntheorem c : 2 \u2264 3 :=\nby\n  library_search -- exact Nat.le_succ 2", "meta": {"post_id": 783, "input_score": 9, "output_score": 8, "post_title": "How to quickly look up what constructor/lemma I should use in Lean4?"}}
{"input": "The possibility of using proof assistants in combinatorics has been mentioned here and in more detail here.  I'm interested in automating that part of some discrete problems which reduces the search space \"without loss of generality\" by exploiting various symmetries.\nI'll mention for context a couple of problems I'm interested in:\n(1) Counting (doubly) diagonal latin squares invites reduction of a search space by the centralizer of a specific permutation. A latin square of size $n\\times n$ has the numbers $1,2,\\ldots,n$ in each row and each column without repetition.  Such a square is doubly diagonal iff the main diagonal and the anti-diagonal also have each $n$ distinct entries.  For example:\n$$\\begin{array}{|c|c|c|c|}\n\\hline\n1 & 4 & 2 & 3 \\\\\n\\hline\n3 & 2 & 4 & 1 \\\\\n\\hline\n4 & 1 & 3 & 2 \\\\\n\\hline\n2 & 3 & 1 & 4 \\\\\n\\hline \\end{array} \\;\\text{ and }\\;\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n5 & 3 & 4 & 1 & 2 \\\\\n\\hline\n4 & 5 & 2 & 3 & 1 \\\\\n\\hline\n2 & 4 & 1 & 5 & 3 \\\\\n\\hline\n3 & 1 & 5 & 2 & 4 \\\\\n\\hline \\end{array}\n$$\n(2) An absorbing Markov chain might be naively formulated using many equivalent states and transitions, and this invites for some purposes (e.g. expected steps until an absorbing state is reached) pruning the search space by symmetries of states.\nMy intuition is that proving \"without loss of generality\" cases is similar to proving program correctness.  Prolog code has often helped me with checking that \"all bases are covered,\" but I experience a certain amount of frustration with reinventing the wheel because of the weak(!) datatyping in Prolog.  So I'd be interested if setoids or some other construction would provide rigorous analysis of cases.", "output": "John Harrison had a nice paper about \u201cwlog\u201d reasoning and the logical requirements behind it.  The paper talks about various ways in which these can be automated.  This is in HOL Light, and I know there is an analogous tactic in HOL4. Given how foundationally simple HOL is, I expect  the logic works in other systems too.", "meta": {"post_id": 814, "input_score": 17, "output_score": 10, "post_title": "WLOG reasoning and Proof Assistants"}}
{"input": "I've seen in mathlib several cases where the universes are explicit, that is Type u instead of Type*. Is there any advantage in doing so and are there any (non-category theory) examples where universes have to be named explicitly in lean even though the mathematics does not run into universe issues?", "output": "Type* is just a shorthand for Type _, where the _ is a wildcard (or more accurately, a universe metavariable) that lean will try to fill in for you. Lean's options are to:\n\nCreate a new universe for you, which it will give a name like u_1.\nReuse an existing universe variable.\nUse max and similar to combine universe levels.\n\nUsually then, explicit universe variables are used either when control over the names is wanted, or when the automatic behavior does the wrong thing.\nTo demonstrate the automatic behavior a little:\n\nIn a variables line, lean will almost always create new variables; so\nvariables {\u03b1 \u03b2 : Type*}\n\nis expanded to\nuniverses u_1 u_2\nvariables {\u03b1 : Type u_1} {\u03b2 : Type u_2}\n\n\nIn something like\nexample {\u03b1 \u03b2 : Type*} (h : \u03b1 = \u03b2) : true := sorry\n\nLean is forced during elaboration of h to put both in the same\nuniverse, so it generates\nuniverses u_1\nexample {\u03b1 \u03b2 : Type u_1} (h : \u03b1 = \u03b2) : true := sorry", "meta": {"post_id": 818, "input_score": 11, "output_score": 8, "post_title": "Explicit vs implicit universes in lean"}}
{"input": "Various type theories have, over the years, explored extending the definitional equality with a variety of eta-laws and various forms of proof irrelevance.\nQuite a lot of systems manage eta for functions, because (as Thierry Coquand observed), you can eta-expand on a tit-for-tat basis, even if your equality test is not type directed. Whenever you are checking f = \\ x -> t, you may readily test \\ y -> f y = \\ x -> t instead (choosing y fresh), if f is not already eta-expanded. Crucially, if neither side is already of the form \\ x -> t, then eta-expansion makes no difference to the problem.\nAgda certainly manages eta for records, because its equality is type-directed. Two records are equal exactly if they are equal fieldwise, which makes it a delight to define the unit type as the record type with no fields. (You can do syntactic eta-expansion tit-for-tat for pairs, but not for unit. For unit, if neither side is already canonical, eta-expansion makes a big difference to the problem!)\nAre there any systems (in a good state of repair) which identify all terms in the empty type? That's a little trickier to manage, but I know some systems (because I built them) which did it. It's nice to have, because it means you can form the set of things which are not bad as dependent pairs of a thing and a proof that it's not bad: to prove that two such pairs are equal, you need only prove that the things are equal, because all inhabitants of Bad thing -> False are equal by eta for functions then falsity.\nIf you're working on a type theory for a proof assistant, have you got definitional equality for an absurd type? If so, I'd like to know (and I'm guessing Lean steps up, here). If not, what's stopping you? (I seem to remember that there was some sort of snag in Agda, but I can't for the life of me remember what it was.)", "output": "If you are happy to have your False live in a separate sort, then you can do this with strict propositions in Coq (and any other system that have those, which I guess includes quite a lot of them nowadays). Here is a small rough example:\nInductive False : SProp := .\n\nVariable Bad : nat -> SProp.\n\nInductive ok_nat : Type :=\n  | c (n : nat) (p : Bad n -> False) : ok_nat.\n\nVariable (o1 o2 : Bad 0 -> False).\n\nGoal (c 0 o1 = c 0 o2).\nProof.\n  reflexivity.\nQed.", "meta": {"post_id": 829, "input_score": 22, "output_score": 13, "post_title": "In what intensional type theories can absurdity be made definitionally proof irrelevant?"}}
{"input": "What is a universe?\nMore specifically,\n\nIs a type system without a term-type distinction, are universes just ordinary types?\nIf (1) is true, can we the system-builders freely choose which types are \"labeled\" as universes or is there a specific property that the collection of universes must satisfy?\n(An elaboration of (2)). If we label every type as a universe is it wrong or just misleading?\nWhat are universes in systems that possess a term-type distinction?\nGiven a type, is it inherently associated with exactly one universe? (The same way that a term is associated with exactly one type.)\nDo universes \"exist globally\" or are they like a \"local fiction\" (like universe levels in NFU ... that exist purely for the purpose of stratifying formulas and don't exist otherwise).\n\nNOTE: I'll reserve the word sort for use in a non-formal/meta-level sense; in this question a sort is not the type of a kind.\n\nWhat follows is my attempt to make sense of the material.\n\nAt time of writing, the universe tag wiki refers to the Wikipedia page and nLab.\nWikipedia says the following about universes in the context of type theory specifically.\n\nIn some type theories, especially in systems with dependent types, types themselves can be regarded as terms. There is a type called the universe (often denoted ${\\mathcal {U}}$) which has types as its elements.\n\nHere's how I interpret this.\n\nIn type systems that have a weak or nonexistent term-type distinction, a universe $\\mathcal{U}$ is just a type.\nIn type systems that have a term-type distinction, $\\mathcal{U}$ is a new sort of type that has types as elements. These systems now have a term-type-universe distinction.\n\nThe Wikipedia article gets most of its content from this article on homotopy type theory.\nIn this setting, it appears that a universe is a type for which we have an inference rule kind of like the following.\n$\\varphi$ is a fixed well-formed formula (that might be empty or might be an interpretation function) and $U$ is a universe.\n$$ \\frac{A : U}{\\varphi(A) \\;\\; \\text{type}} $$\nSection 4 of that article covers some concrete choices made by different proof assistants.\nThat and the section before about typical ambiguity are, to me at least, strongly reminiscent of the set theory NFU ... which can informally be obtained by taking the Russellian theory of simple types and just identifying stuff at every level of the hierarchy by neglecting type indices on defining formulas.\nHowever, in the NFU case, the different universes in the universe hierarchy do not exist globally. They spring into existence when we need to stratify a formula and then disappear.", "output": "Note: I'm approaching this from a purely syntactic perspective. I'm sure that a deeper answer could be given from the semantic perspective.\nIn all type theories I'm aware of, a universe is a type that classifies types. You mention the idea of a type-term distinction. I'm interpreting this as the presence or absence of a judgment of the form $A \\ type$, separate from $x : A$. This naturally leads to two styles of adding a universe to a type theory: universes ala Russel (Russel universes) and universes ala Tarski (Tarski universes).\nA Russel universe is some type $U$, whose elements are other types. If we have some type $\u2115$, then we can derive a the judgment $\u2115 : U$. The presence of a Russel universe allows us, if we wish, to completely eliminate any separate $A \\ type$ judgment, replacing it with $A : U$ (assuming we have some type for $U$ itself). This solution is conceptually straightforward and is used in languages like Coq and Agda.\nA Tarski universe is also some type $U$, but instead of its elements being types, they are terms, which we call type codes. Basically, instead of being able to derive $\u2115 : U$, we have some new term, which I will call $`\u2115$, for which we can derive the judgment $`\u2115 : U$. We do this for each type former. Notably, $`\u2115$ is not a type. We cannot derive $`\u2115 \\ type$ or $zero : `\u2115$. Instead, we introduce a special syntax which allows us to transform a type code into a type. This syntax is usually called $El$ (short for element), and allows us to derive that $El(a) \\ type$ given that $a : U$. In order to make $El$ useful, we introduce the \"obvious\" judgmental equalities: for instance that $El(`\u2115) = \u2115 \\ type$. If instead we made these judgmental equalities into mere equivalences, where an explicit coercion is required to transform a code into a type, we would have what are called weak Tarski universes. For a language that actually implements weak Tarski universes (though they are essentially hidden by elaboration), check out cooltt: https://github.com/RedPRL/cooltt.\nSo to answer your questions with all this in mind:\n\nYes\nA type is a universe when its elements classify other types in some meaningful way\nI would say that this is wrong, but I do not have enough knowledge of semantics to back up why it must be\nTarski universes\nNot necessarily, if we have a cumulative hierarchy of universes, we could have a rule classifying a type in any universe level\nUniverses exist globally in all type theories with them that I know of", "meta": {"post_id": 856, "input_score": 11, "output_score": 10, "post_title": "What is a universe?"}}
{"input": "I have heard that an extension to a type theory can be said to be conservative, which means it may add new formulae to the original type theory, for example new type formers and their intro/elim rules and equations, but the original type theory is left untouched in a sense that we do not create new inhabitants to existing types.\nSo here are my questions. I tried to approach the idea, but wikipedia defines conservative extension in terms of 'theory', not type theory. A (modern) type theory talks about not just theorems, but also data (like computation of mathematical objects, like numbers) using types, so:\n\nIf an extension adds new inhabitants to a data type instead of a proposition, is it considered conservative?\nIf an extension adds new inhabitants to an inhabited type, is it considered conservative?\n\nAlso, what does conservativity tell us about the extension? We usually say normalization tells us the decidability of type checking (and much more), canonicity tells us the type theory is a programming language (citation needed), etc., but what about conservativity?", "output": "Anja Petkovi\u0107 Komel in her PhD thesis Meta-analysis of type theories with an application to the design of formal proofs studied transformations of type theories among other things.\nDefinition 9.3.3 on page 66 gives the notion of a conservative transformation. This is more general than a conservative extension, which is a conservative inclusion.\nLet me give a quick summary. To understand the general definition, we need to know what a boundary is. Every type-theoretic judgement has a boundary and a head:\n\nthe boundary of $\\Gamma \\vdash A \\; \\mathsf{type}$ is $\\Gamma \\vdash \\Box \\; \\mathsf{type}$ and $A$ is the head.\nthe boundary of $\\Gamma \\vdash t : A$ is $\\Gamma \\vdash \\Box : A$ and $t$ is the head.\nthe boundary of $\\Gamma \\vdash A \\equiv B$ looks the same as the judgement itself because its head is trivial (judgemental equality is proof irrelevant), but we might write the judgement as $\\Gamma \\vdash A \\equiv B \\;\\mathsf{by}\\; \\star$, the boundary as $\\Gamma \\vdash A \\equiv B \\;\\mathsf{by}\\; \\Box$ and take $\\star$ to be a trivial token that serves as the head.\nsimilarly for $\\Gamma \\vdash t \\equiv s : A$.\n\nA boundary $\\Gamma \\vdash \\mathscr{b}$ may be filled with a head $e$ to give a judgement $\\Gamma \\vdash \\mathscr{b}\\boxed{e}$. For example, filling the boundary $\\Gamma \\vdash \\Box : A$ with $t$ produces $\\Gamma \\vdash t : A$.\nBoundaries are an important concept that is often left implicit, but they deserve to treated properly. For example, a goal in a proof assistant is precisely a well-formed boundary. The goal is solved when its head is filled to give a derivable judgement.\nGiven a type-theoretic transformation $f : \\mathcal{T} \\to \\mathcal{U}$, we say that $f$ is conservative when the following holds: for any well-formed boundary $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$ in $T$, if the boundary $f \\Gamma \\vdash_\\mathcal{U} f \\mathscr{b}$ can be filled with some $e$ to give a derivable judgement $f \\Gamma \\vdash_\\mathcal{U} (f \\mathscr{b})\\boxed{e}$, then there is $e'$ such that $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}\\boxed{e'}$.\nConcretely, suppose $f : \\mathcal{T} \\to \\mathcal{U}$ is an inclusion so that $f(e) = e$. Then conservativity of $f$ amounts to: if $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$ is well-formed in $\\mathcal{T}$ and $\\Gamma \\vdash_\\mathcal{U} \\mathscr{b}\\boxed{e}$ is derivable in $\\mathcal{U}$, then there is $e'$ in $\\mathcal{T}$ such that $\\mathcal{T}$ derives $\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}\\boxed{e'}$. In particular, when we specialize this condition to term judgements and term equations, we get:\n\nif $\\Gamma \\vdash_\\mathcal{T} A \\; \\mathsf{type}$ and $\\Gamma \\vdash_\\mathcal{U} e : A$ then there is $e'$ such that $\\Gamma \\vdash_\\mathcal{T} e' : A$\nif $\\Gamma \\vdash_\\mathcal{T} s : A$ and $\\Gamma \\vdash_\\mathcal{T} t : A$ and $\\Gamma \\vdash_\\mathcal{U} s \\equiv t : A$ then $\\Gamma \\vdash_\\mathcal{T} s \\equiv t : A$\n\nYou should not take the above as precise definitions, because I skipped over the treatment of meta-variables, which matters. The details are in Petkovi\u0107 Komel's dissertation. She also gives examples of conservative transformations, such as definitional extension.\nThe concept of a conservative transformation subsumes the usual notions of conservativity that one sees in logic books, because first-order theories are special cases of type theories.\nLet us apply the concept to your specific question: if an extension adds a new element to a type, is it conservative? Concretely, suppose we have a type $A$ in theory $\\mathcal{T}$ to which we add a new constant $\\vdash c : A$ to obtain an extended theory $\\mathcal{T}'$.\nIf $A$ was empty beforehand, then this is not conservative, consider the boundary $\\vdash \\Box : A$. Otherwise, consider a closed term $t : A$ and define a further extension $\\mathcal{T}''$ which, in addition to $\\vdash c : A$, also has the equation $\\vdash c \\equiv t : A$. Example 9.3.5 of Petkovi\u0107 Komel's thesis shows that $\\mathcal{T}''$ is a conservative extension of $\\mathcal{T}$. We thus have a chain of extensions\n$$\\mathcal{T} \\to \\mathcal{T}' \\to \\mathcal{T}''$$\nwhich can be used to show that $\\mathcal{T}'$ is a conservative extension: given a boundary $$\\Gamma \\vdash_\\mathcal{T} \\mathscr{b}$$ and a filling $$\\Gamma \\vdash_{\\mathcal{T}'} \\mathscr{b}\\boxed{e},$$ we also get\n$$\\Gamma \\vdash_{\\mathcal{T}''} \\mathscr{b}\\boxed{e},$$\nand hence by conservativity of $\\mathcal{T}''$ over $\\mathcal{T}$ we find $e'$ such that\n$$\\Gamma \\vdash_{\\mathcal{T}} \\mathscr{b}\\boxed{e'}.$$\nWorking through Example 9.3.5 tells us how $e'$ is related to $e$: we just replace all occurrences of $c$ with $t$, as one would expect of a definitional extension.\nThe wheels are turning, the mechanism works.\nYou asked how the concept is useful. In chapter 10 of the dissertation, a general elaboration theorem is proved: every finitary type theory can be elaborated to a standard type theory. Conservativity plays a role in this theorem: the fact that the retrogression map (the opposite of elaboration) is conservative means that the elaboration does not reconstruct information that is not really there.\nConservativity also plays a role in the design of PAs. Suppose we use an SMT solver $S$ to solve goals in a proof assistant $P$. The situation amounts to a transformation $f : P \\to S$ that converts boundaries in $P$ to boundaries in $S$. To say that $f$ is conservative means that it is safe to use the solver $S$: for if $S$ manages to fill a boundary $f \\mathscr{b}$ then we know that the original boundary $\\mathscr{b}$ can also be filled in $P$. Of course, we need an explicit way of converting the head that $S$ found back to a head in $P$ \u2013 but that is precisely a computable witness of conservativity. And also, note that we need not translate back entire derivations, we only need to be able to reconstruct the head.", "meta": {"post_id": 857, "input_score": 8, "output_score": 12, "post_title": "What's \u201cconservativity\u201d in terms of type theory and how is it useful?"}}
{"input": "When I first learned Haskell, I was told that you use data keyword to define an \"algebraic data type\", or a \"sum type\". Then I learned the idea of catamorphism, which is related to the \"initial F-algebra\" -- that all such \"algebraic data types\" can be encoded as the colimit of its finite approximations. The name \"algebraic data type\" is an attribute to the fact that we're using an algebra to generate the type.\nThen, I saw an article by Jason Hu (in Chinese) that Haskell does not have algebraic data types, because Haskell does not enforce strict positivity. I'm not sure if this is true, because some types that are not strictly positive may also be generated \"algebraically\", for instance coinductive types can be generated by \"final F-coalgebra\".\nI tried to find some relevant resources on the internet. Wikipedia says algebraic data types are types generated by taking sums or products of basic types, which I think is true, but it doesn't mention recursive cases, while positivity can only be discussed when you have type recursion.", "output": "[Note: After some helpful off-line remarks by Mart\u00edn Escard\u00f3 I have edited the part about general recursive types. It was at best misleading by trying to paint too simple a picture.]\nI will start with the classic notion of universal algebra and develop from it algebraic datatypes. Then I will discuss how they relate to coalgebraic datatypes and recursive types found in programming languages. Hopefully this will bring some clarity to what is what.\nSignatures in universal algebra\nIn universal algebra a signature is a list of operations with their arities\n$$\\sigma = [(\\mathsf{op}_1, n_1), \\ldots, (\\mathsf{op}_k, n_k)].$$\nHere $\\mathsf{op}_i$ are symbols and $n_i \\in \\mathbb{N}$.\nUniversal algebra also has equations but those play no role here so we shall ignore them. For example, the signature for a group might be $[(\\mathsf{e}, 0), (\\mathsf{i}, 1), (\\mathsf{m}, 2)]$, for unit, inverse and multiplication, respectively.\nWe define the set $T_\\sigma$ of terms inductively as:\n\n$\\mathsf{op}_i(t_1, \\ldots, t_{n_i}) \\in T_\\sigma$ for $i \\leq k$ and $t_1, \\ldots, t_{n_i} \\in T_\\sigma$\n\nThere better be some nullary operations or else $T_\\sigma$ is empty. Also, in most applications we would include among the terms some variables, but we're ignoring that point also.\nNote that $T_\\sigma$ satisfies the isomorphism\n$$T_\\sigma \\cong T_\\sigma^{n_1} + \\cdots + T_\\sigma^{n_i}.$$\nIf we write $P_\\sigma(X) = X^{n_1} + \\cdots + X^{n_i}$ then the above is written more succinctly as $T_\\sigma \\cong P_\\sigma(T_\\sigma)$. There are many such fixed points of $P_\\sigma$, but $T_\\sigma$ is special because it is the smallest one (in a sense to be made precise below).\nInitial algebras for polynomial functors\nAt least this is how an old textbook on algebra might do it. Let us bring in some category theory. First, we replace arities as numbers with arities as sets. Thus, instead of saying that we have $n_i$ terms $t_1, \\ldots, t_{n_i} \\in T_\\sigma$ we shall say that we have a map $t : N_i \\to T_\\sigma$ where $N_i$ has $n_i$ elements (and henceforth we use $N_i$ directly, forgetting about $n_i$).\nNext, let us allow operations with parameters. Each symbol $\\mathsf{op}_i$ shall have a parameter set $S_i$ and an arity $N_i$. For instance, scalar multiplication in a real vector space would have $S_i = \\mathbb{R}$ and $N_i = \\{\\star\\}$, because it is a unary operation on vectors parameterized by a real number. An operation without a parameter set is one whose parameter set is a singleton set.\nThus a signature looks like this:\n$$\\sigma = [(\\mathsf{op}_1, S_1, N_1), \\ldots, (\\mathsf{op}_k, S_k, N_k)].$$\nIt determines a functor $P_\\sigma : \\mathsf{Set} \\to \\mathsf{Set}$ given by\n$$P_\\sigma(X) = S_1 \\times X^{N_1} + S_2 \\times X^{N_2} + \\cdots + S_k \\times X^{N_k},$$\nknown as a polynomial functor (for obvious reasons). In category-theoretic language, the set of terms $T_\\sigma$ is just the initial $P_\\sigma$-algebra. It is a fixed point of $P_\\sigma$,\n$$T_\\sigma \\cong\n  S_1 \\times T_\\sigma^{N_1} + S_2 \\times T_\\sigma^{N_2} + \\cdots + S_k \\times T_\\sigma^{N_k}.\n$$\nNote that $P_\\sigma$ may have many fixed points. For example, the final $P_\\sigma$-coalgebra is also such a fixed point.\nAlgebraic and coalgebraic datatypes\nAs a datatype, $T_\\sigma$ would be described as follows (I am dropping the subscript $\\sigma$):\ndata T : Set where\n  op_1 : S_1 \u00d7 (N_1 \u2192 T) \u2192 T\n  ...\n  op_k : S_k \u00d7 (N_k \u2192 T) \u2192 T\n\nSo this is just a different notation for an algebraic signature. It therefore makes sense to call the type determined by it an algebraic or inductive datatype, provided such a datatype really is inductive, i.e., it satisfies a suitable induction or recursion principle witnessing its initiality.\n(A side remark: the \"strict positivity\" requirement that proof assistants impose on inductive definitions is just their way of making sure that the underlying functor determined by the given signature is nice enough to have an initial algebra.)\nActually, we could describe the final coalgebra with the exact same data, by writing something like\ncodata T : Set where\n  op_1 : S_1 \u00d7 (N_1 \u2192 T) \u2192 T\n  ...\n  op_k : S_k \u00d7 (N_k \u2192 T) \u2192 T\n\nThe information, namely the algebraic signature, is the same, but the constructed type is now the final coalgebra, so it makes sene to call it a coinductive datatype, provided it satisfies suitable principles witnessing the fact that it is a final coalgebra.\nGeneral recursive types\nObserve that we may generate all polynomial functors $P_\\sigma$ using pre-existing types, the type constructors $\\times$ and $+$, and powers by pre-existing types $X \\mapsto X^S$. In fact, we get precisely the polynomial functors this way.\nGeneral recursive types arise when we allow other type constructors, for example we include function spaces $\\to$. This leads to various complications, because it is not even clear at first how to turn something like\n$$Q(X) = (\\mathsf{Nat} \\to X) + (X \\to \\mathsf{Bool})$$\ninto a functor. Is it covariant or contravariant? And once we do figure out how to turn $Q$ into a functor, it is not clear anymore whether we should be taking the initial algebra or the final coalgebra. Nevertheless, all these complications can be resolved with the aid of algebraically compact categories.\nWithout going too much into the details of general recursive types, let me say that the standard way to treat them is to model them in a category where the initial algebra and the final coalgebra coincide. These categories are typically categories of domains or predomains and require some getting used to and cannot be thought of naively as sets.\nTypes in programming languages\nA programming language might therefore support some selection of the following:\n\nalgebraic datatypes, also known as inductive datatypes,\ncoalgebraic datatypes, also know as coinductive datatypes,\ngeneral recursive types.\n\nThere may be further nuances that arise from the fact that a real-world programming language is not the same thing as a mathematical model, but let us not worry about that too much.\nProof assistants: Agda data and Coq's Inductive are algebraic datatypes. Proof assistants make provisions for coalebraic datatypes by other means. They do not have general recursive types as those allow arbitrary recursive definitions, which makes it possible to inhabit all types (and thereby ruin the logical content of types). Here is how one can use recursive types in Haskell to inhabit every type without any explicit recursive calls:\ndata Magic a = Abracadabra (Magic a -> a)\n\nevil :: a\nevil = spell (Abracadabra spell)\n  where\n    spell :: Magic a -> a\n    spell (Abracadabra x) = x (Abracadabra x)\n\nProgramming languages: they do not care about propositions-as-types, so they are free to admit general recursive types, which they often do. When the general recursive types are specialized to algebraic signatures, they may yield something that looks like an inductive datatype, or a coinductive datatype, or neither, depending on the details of the underlying semantics.\nHaskell data definitions allow general recursive types. In particular, Haskell's\ndata Nat = Zero | Succ Nat\n\ngives the inital/final solution for the functor $X \\mapsto 1 + X$, which is not the natural numbers, but rather the domain of lazy natural numbers whose elements are of the form:\n\nSucc (Succ (Succ (\u22ef Succ Zero) \u22ef))) \u2013 the natural numbers\nSucc (Succ (Succ (\u22ef Succ \u22a5) \u22ef))) \u2013 the partial numbers\nSucc (Succ (Succ (\u22ef \u22ef))) \u2013 infinity\n\nI am explaining all this to make a point: Haskell's natural numbers are both initial and final, but are computed in a different category so we cannot just pretend that they live in sets.\nOCaml and SML recursive type definitions also allow general recursive types. In particular, OCaml's\ntype nat = Zero | Succ of nat\n\ngives the initial/final algebra for the functor $X \\mapsto 1 + X$, which is the natural numbers, because it is computed in a different category.\nDisclaimers:\n\nAs Pierre-Marie points out in the comments, OCaml allows certain limited cases of coinductively defined values, such as let rec infinity = Succ infinity. However, these do not provide general coinductive types, only certain regular trees, so let us ignore them.\n\nIn the spirit of the previous disclaimer, we should be aware that Haskell and OCaml as real-world languages do not actually have a complete mathematical semantics, or at least not one that we would want to discuss here. The above remarks about recursive types in Haskell and OCaml should be understood as applying to smallish well-defined fragments of the languages.", "meta": {"post_id": 879, "input_score": 29, "output_score": 29, "post_title": "What is an algebraic data type?"}}
{"input": "I am familiar with De Bruijn indices, but not with De Bruijn levels. In my understanding, indices index variables from the top of the context/stack, whereas levels index variables from the bottom. So indices might be more attractive for dealing with bindings, but levels might be more attractive for dealing with weakening. But how does this play out from a broader perspective, for example when building an evaluator or a typechecker?\nIn what instances should De Bruijn levels be used over De Bruijn indices, and vice versa?", "output": "If we want to do evaluation in the presence of free variables (\"open\" evaluation), then De Bruijn levels are essential, precisely because of their support for cost-free weakening.\nGenerally, the best practice to implement substitutions and beta-reductions with open terms, is to have\n\nSyntactic terms, which can be evaluated to values in a given value environment. These may use any variable representation, but usually De Bruijn indices are the simplest and most efficient.\nValues, which contain closures instead of binders. Values must use levels or some other variable representation which is not relative to the size of the scope (e.g. globally fresh names are also workable).\n\nThe combination of levels and closures is what allows free weakening of values. If we have levels in plain terms, the bound variables have to be shifted during weakening. But here we don't have bound variables, we have closures instead, so there is nothing to shift.\nFor open evaluation of pure lambda terms, it looks like the following (in Haskell):\ntype Ix  = Int\ntype Lvl = Int\n\ndata Tm      = Var Ix | Lam Tm | App Tm Tm\ndata Closure = Close [Val] Tm\ndata Val     = VVar Lvl | VApp Val Val | VLam Closure\n\neval :: [Val] -> Tm -> Val\neval env t = case t of\n  Var x   -> env !! x\n  App t u -> case (eval env t, eval env u) of\n               (VLam (Close env' t), u) -> eval (u:env') t\n               (t                  , u) -> VApp t u\n  Lam t   -> VLam (Close env t)\n\nWe may also add quoting to normal forms, to get normalization-by-evaluation:\nquote :: Lvl -> Val -> Tm\nquote l t = case t of\n  VVar x             -> Var (l - x - 1)\n  VApp t u           -> App (quote l t) (quote l u)\n  VLam (Close env t) -> Lam (quote (l + 1) (eval (VVar l:env) t))\n\nnf :: [Val] -> Tm -> Tm\nnf env t = quote (length env) (eval env t)\n\nSome sources which use this basic evaluation setup:\n\nThierry Coquand: An algorithm for type-checking dependent types\nAbel, Coquand, Pagano: A Modular Type-checking algorithm for Type Theory with Singleton Types and Proof Irrelevance\nCoquand et al.: A simple type-theoretic language: Mini-TT\nDavid Christiansen: Checking Dependent Types with Normalization by Evaluation: A Tutorial\ncubicaltt\ncooltt\nMy elaboration zoo", "meta": {"post_id": 900, "input_score": 18, "output_score": 22, "post_title": "When should I use De Bruijn levels instead of indices?"}}
{"input": "W-types are said to be \"a well-founded inductive type\" that is parameterized over arities and constructors, similar to a \"tree of possible constructions\". On nlab, it is said that a lot of inductive types can be encoded with W-type.\nHowever, from that description I still don't understand what's well-foundedness in terms of the syntax of inductive types. I can see that Agda's non-mutual inductive families are \"well-founded\" because it's a rooted tree, but what about inductive-recursive and inductive-inductive types? I think these are known to be not equivalent to non-mutual inductive families and they have a more complex model. I think someone told me that strictly positive inductive families are well-founded, but what exactly makes a(n inductive) type well-founded?\nCan we encode all well-founded inductive types with the W-type?", "output": "\"Well-founded\" and \"inductive\" mean roughly the same thing. I think the reason different terminology tends to get used for W-types is that their definition looks similar to notation for ordinals (imagining that the branching of the trees is like a limit of the children). They're distinct from the other inductive types given in Martin-l\u00f6f type theory in that they are potentially infinitely wide. But they're still supposed to be (intuitively) finitely deep, which is what the \"inductive\" and \"well-founded\" refer to\nWhether or not you can encode various other sorts of inductively specified types as W-types is a complicated question.\n\nIt's pretty easy to show that you can encode plain inductive types using W-types if you have various sorts of extensionality. The typical schemas for what constitutes an 'inductive definition' are like finite polynomials, while W-types are trees built from any (infinite) polynomial definable in type theory. The main discrepancy is how the 'products' act. The finite products in inductive schemas have a canonical form, while the functions used in W-types do not, and the most obvious way to fix that is with function extensionality.\n\nHowever, you can also get by without extensionality. You only need some eta rules. The idea is to use W-types to build the right sort of trees, then define the subtype of 'canonical' trees by (W-type) induction. It happens that the recursion rule for this subtype has the judgmental behavior matching the schema for inductive definitions.\n\nYou can also encode indexed W-types using normal W-types and the identity type using a similar strategy. You use W-types to build a larger type of trees on the 'total spaces' involved, and then define a subtype of well-indexed trees by induction. This file shows how to do it.\n\nYou can (I believe) encode indexed inductive types/families using indexed W-types, including the encoded version of those, using a strategy similar to the Why Not W? paper. My Agda file above shows how to do this for a fairly simple indexed type that was mentioned in another question here.\n\nYou can encode mutual inductive and inductive-inductive types with indexed inductive types. For mutual inductive types, you just add a finite index type to turn N definitions into a single N-indexed definition. For induction-induction, you follow a similar strategy as for building indexed W-types: define mutual inductive types that contain too many values, then define the subtypes with proper indexing afterward.\n\nYou cannot encode all inductive-recursive definitions as (indexed) inductive definitions. I-R definitions were invented as a schema that would let you write down the definition of universes as a special case. However, the additional power comes from simultaneously being able to define a recursive type family. If instead you just simultaneously define a recursive function into an existing type, I believe they are encodable using a strategy like above. This might mean that if you have enough universes, you can use them with inductive definitions to encode everything you could write by just having a theory that admits inductive-recursive definitions (but with no pre-specified universes). I'm unsure about this, though.\n\nHaving a universe that classifies inductive-recursive definitions is even stronger, and isn't itself an instance of induction-recursion. It's actually inconsistent to be able to do induction on such a universe (while the I-R definable universes in 6 can have an associated induction principle).\n\nQuotient/higher inductive types can't be encoded as any of the previous sorts of definition in general. Quotients where you can compute a canonical representative for each equivalence class can be defined, but not all quotients are like that.\n\n\nI don't think strict positivity has anything to do with being inductive/well-founded (negativity does, though; see the comments). It's necessary to guarantee that 'all' inductive definitions are meaningful in various sorts of models. For instance, you can't have a classical set theoretic model of an inductive type $T \\cong 2^{2^T}$ (which is positive, but not strictly positive), because $2$ classifies the propositions, and you can't have a type equivalent to its double power type. Constructively you might be able to admit some such types, and for instance, $\u03bb \\_ \u2192 0$ and $\u03bb\\_ \u2192 1$ give you a starting point for building up your finitely-deep values. However, these sorts of non-strictly positive types can conflict with other features than just classical mathematics, so you need to be very careful.\nVarious bits of the above are subject to caveats about the details of what counts as an \"inductive definition.\" There's literature out there rigorously defining various schemata for what constitutes an (indexed) inductive(-recursive/inductive) definition (etc.). Agda (for instance) is not super rigorous, and runs a checker that lets you conveniently do things that could probably be encoded in those more rigorous schemata in a more inconvenient way. Or perhaps you couldn't, but it's still fine; or isn't.", "meta": {"post_id": 926, "input_score": 10, "output_score": 13, "post_title": "What are well-founded inductive types?"}}
{"input": "I find my mathematics and programming background$^*$ do not endow me with much understanding of type theory as it pertains to proof assistants.  To remedy this shortcoming I don't expect a Royal Road to understanding, and my strategy is mixed.\nI ordered Type Theory and Formal Proof -- An Introduction by Nederpelt and Geuvers (2014), a real hardcover book which Barendregt's Foreword claims to provide \"a gentle, yet profound, introduction to systems of types and their inhabiting lambda-terms.\"  Although the book stretches out to some four hundred pages, it gets there in chapters of twenty to forty pages, each capped by exercises to assess ones comprehension.\nI'm also installing Lean 4 and pursuing as a tutorial Mathematics in Lean hoping to see the hierarchy of types in a familiar enough perspective.\nThis is me \"hitting it high, hitting it low\".  I'm interested in other approaches and ways to expedite learning.\n\nMy math background includes formal logic, algebra, combinatorics, category theory and a good bit of numerical methods, and my programming skills are weighted toward object-oriented procedural languages (C++), Prolog, and database stuff.", "output": "I find my mathematics and programming background do not endow me with much understanding of type theory as it pertains to proof assistants.\nSo many data types, so little time\n\n\nOregon Programming Languages Summer School \u2014 July 16-28, 2012\nThe program consists of 80 minute lectures presented by internationally recognized leaders in programming languages and formal reasoning research.\n(ref) - Free access to all of the videos of the lectures.\nTechnical Lectures\n\nLogical relations \u2014 Amal Ahmed\nCategory theory foundations \u2014 Steve Awodey\nProofs as Processes \u2014 Robert Constable\nPolarization and focalization \u2014 Pierre-Louis Curien\nType theory foundations \u2014 Robert Harper\nMonads and all that \u2014 John Hughes\nCompiler verification \u2014 Xavier Leroy\nLanguage-based security \u2014 Andrew Myers\nProof theory foundations \u2014 Frank Pfenning\nSoftware foundations in Coq \u2014 Benjamin Pierce\n\n\nAs noted in a comment by Albert ten Napel\nSoftware Foundations\nThe Software Foundations series is a broad introduction to the mathematical underpinnings of reliable software.\nThe principal novelty of the series is that every detail is one hundred percent formalized and machine-checked: the entire text of each volume, including the exercises, is literally a \"proof script\" for the Coq proof assistant.\n(ref)\n\nTypes and Programming Languages\nby Benjamin C. Pierce\n(WorldCat) (Site)\nMost of the type theories derive from Lambda Calculus. This is book is Lambda Calculus on steroids.\nList of types from the book\nName                   Extends/Based on Figure                          Figure                                                   Page\nB untyped                                                                3-1 Booleans (B)                                         34\nB \u2115 (untyped)          Extends 3-1 B                                    3-2 Arithmetic expressions (\u2115B)                          41\n\u2192 (untyped)                                                             5-3 Untyped lambda-calculus (\u03bb)                           72\nB (typed)              Extends 3-1 B                                    8-1 Typing rules for Booleans (B)                        93\nB \u2115 (typed)            Extends 3-1, 8-1 B                               8-2 Typing rules for numbers (\u2115B)                        93\n\u2192 (typed)              Based on5-3 \u03bb                                    9-1 Pure simply typed lambda-calculus (\u03bb\u2192)               103\n\u2192 Unit                 Extends 9-1 \u03bb\u2192                                   11-2 Unit type                                           119\n\u2192 as                   Extends 9-1 \u03bb\u2192                                   11-3 Ascription                                          122\n\u2192 let                  Extends 9-1 \u03bb\u2192                                   11-4 let binding                                         124\n\u2192 x                    Extends 9-1 \u03bb\u2192                                   11-5 Pairs                                               126\n\u2192 {}                   Extends 9-1 \u03bb\u2192                                   11-6 Tuples                                              128\n\u2192 {}                   Extends 9-1 \u03bb\u2192                                   11-7 Records                                             129\n\u2192 {} let p (untyped)   Extends 11-7, 11-4                               11-8 (Untyped) record patterns                           131\n\u2192 +                    Extends 9-1 \u03bb\u2192                                   11-9 Sums                                                132\n\u2192 +                    Extends 9-1 \u03bb\u2192                                   11-10 Sums (with unique typing)                          135\n\u2192 <>                   Extends 9-1 \u03bb\u2192                                   11-11 Variants                                           136\n\u2192 fix                  Extends 9-1 \u03bb\u2192                                   11-12 General recursion                                  144\n\u2192 B List               Extends 9-1 \u03bb\u2192 with 8-1 Booleans                 11-13 Lists                                              147\n\u2192 Unit Ref             Extends 9-1 \u03bb\u2192 with 11-2 Unit                    13-1 References                                          166\n\u2192 error                Extends 9-1 \u03bb\u2192                                   14-1 Errors                                              172\n\u2192 error try            Extends 9-1 \u03bb\u2192 with 14-1 Errors                  14-2 Error handling                                      174\n\u2192 exceptions           Extends 9-1 \u03bb\u2192                                   14-3 Exceptions carrying values                          175\n\u2192 <: Top               Extends 9-1 \u03bb\u2192                                   15-1 Simply typed lambda-calculus with subtyping (\u03bb<:)   186\n\u2192 {} <:                Extends 15-1 \u03bb<: and 11-7 Records                15-3 Records and subtyping                               187\n\u2192 <: Bot               Extends 15-1 \u03bb<:                                 15-4 Bottom type                                         192\n\u2192 <> <:                Extends 15-1 \u03bb<: and 11-11 Simple variant rules  15-5 Variants and subtyping                              197\n\u2192 {} <:                Extends 15-1 \u03bb<: and 15-3 Records and subtyping  16-1 Subtype relation with records (compact version)    211\n\u2192 {} <:                                                                 16-2 Algorithmic subtyping                               212\n\u2192 {} <:                                                                 16-3 Algorithmic typing                                  217\n\u2192 u                    Extends 9-1 \u03bb\u2192                                   20-1 Iso-recursive types (\u03bbu)                            276\n\u2192\u2200                     Based on 9-1 \u03bb\u2192                                  23-1 Polymorphic lambda-calculus (System F)              343\n\u2192\u2200\u2203                    Extends 23-1 System F                            24-1 Existential types                                   366\n\u2192\u2200\u2203 Top                Based on 23-1 System F and 15-1 simple subtyping 26-1 Bounded quantification (kernel F<:)                 392\n\u2192\u2200\u2203 Top full           Extends 26-1 F<:                                 26-2 \"Full\" bounded quantification                       395\n\u2192\u2200<: Top \u2203             Extends 26-1 F<: and 24-1 unbounded existentials 26-3 Bounded existential quantification (kernel variant) 406 \n\u2192\u2200<: Top                                                                28-1 Exposure Algorithm for F<:                          418\n\u2192\u2200<: Top               Extends 16-3 \u03bb<:                                 28-2 Algorithmic typing for F<:                          419\n\u2192\u2200<: Top               Extends 16-2 \u03bb<:                                 28-3 Algorithmic subtyping for kernel F<:                422\n\u2192\u2200<: Top full          Extends 28-3                                     28-4 Algorithmic subtyping for full F<:                  424\n-\u21d2                     Extends 9-1 \u03bb\u2192                                   29-1 Type operators and kinding (\u03bb\u03c9)                     466\n-\u2200\u21d2                    Extends 29-1 \u03bb\u03c9 and 23-1 System F                30-1 Higher-order polymorphic lambda-calculus (F\u03c9)       450\n-\u2200\u2203\u21d2                   Extends 30-1 F\u03c9 and 24-1                         30-2 Higher-order existential types                      452\n-\u2200\u21d2<: Top              Based on 30-1 F\u03c9 and 16-1 kernel F<:             31-1 Higher-order bounded quantification (F\u03c9<:)          470\n-\u2200<: Top {}\u2190           Based on 26-1 F<: with 11-7 records              32-1 Polymorphic update                                  485  \n\n\nAdvanced Topics in Types and Programming Languages\nEdited by Benjamin c. Pierce\n(WorldCat) (Site)\n\nPractical Foundations for Programming Languages Second Edition\nby Robert Harper\n(WorldCat) (Site)\n\nThis text develops a comprehensive theory of programming languages based on type systems and structural operational semantics. (from publisher site)\n\n\nAs noted in a comment by Scott Morrison\n\nIf you have a more mathematical background, I think the first section of the \"Homotopy Type Theory\" book (freely available on line) is excellent. That section is just about dependent type theory, and one can stop before meeting the \"homotopy\" part.\n\nHomotopy Type Theory - Univalent Foundations of Mathematics\nA collaborative effort.\nContents\nI Foundations\n\nType theory\nHomotopy type theory\nSets and logic\nEquivalences\nInduction\nHigher inductive types\nHomotopy n-types\n\nII Mathematics\n\nHomotopy theory\nCategory theory\nSet theory\nReal numbers\n\nAppendix\n\u00a0\u00a0\u00a0A Formal type theory\nIndex of symbols\nThe pdf version of the book now a bit harder to find starting from Google because of bad links. So here is a valid link.\n(pdf) (LaTeX) (GitHub)\n\nProofs and Types\nby Jean-Yves Girard\n(pdf) (WorldCat) (PLS Lab)\nBased on a short graduate course on typed lambda-calculus given at the Universit\u00e9 Paris VII in the autumn term of 1986-7. (ref)\n\nSense, Denotation and Semantics\nNatural Deduction\nThe Curry-Howard Isomorphism\nThe Normalisation Theorem\nSequent Calculus\nStrong Normalisation Theorem\nG\u00f6del's system T\nCoherence Spaces\nDenotational Semantics of T\nSums in Natural Deduction\nSystem F\nCoherence Semantics of the Sum\nCut Elimination (Hauptsatz)\nStrong Normalisation for F\nRepresentation Theorem\n\n\nHandbook of Practical Logic and Automated Reasoning\nby John Harrison\n(Site) (WorldCat)\n(Code and resources)\nHOL Light was written by John Harrison (ref) who also authored \"Handbook of Practical Logic and Automated Reasoning\". Think of the book as a very detailed introduction to the code for HOL Light. (GitHub)\nThe book takes one from Boolean Logic all the way up to Interactive theorem proving.\n\nLectures on the Curry-Howard Isomorphism\nby M.H. Sorensen and P. Urzyczyn\n(WorldCat)\nContents\n\nType-free \u03bb-calculus\nIntuitionistic logic\nSimply type \u03bb-calculus\nThe Curry-Howard isomorphism\nProofs as combinators\nClassical logic and control operators\nSequent calculus\nFirst-order logic\nFirst-order arithmetic\nG\u00f6del's system T\nSecond-order logic and polymorphism\nSecond-order arithmetic\nDependent types\nPure type systems and the \u03bb-cube\n\n\nML for the Working Programmer 2nd Edition\nby L.C. Paulson\n(WorldCat) (Site)\nMost of the book is a standard book for learning a programming language.\nChapter 10 A Tactical Theorem Prover is the icing on the cake.\n\nHandbook of Automated Reasoning\nEdited by Alan Robinson and Andrei Voronkov\n(WolrdCat)\n2 volume set.\nPublished in 2001, most of it is foundational and still of value today.\n\nAs you noted recommended tutorials are worth their weight in gold.\n\nTyping rules of proof assistants\n\nLean\n\"The Type Theory of Lean\"\nby Mario Carneiro\n(pdf)\n\nCoq\nDocs \u00bb Core language \u00bb Typing rules\n\n\n\nLambda Cube\n(Wikipedia)\n\nIn mathematical logic and type theory, the \u03bb-cube (also written Lambda Cube) is a framework introduced by Henk Barendregt to investigate the different dimensions in which the calculus of constructions is a generalization of the simply typed \u03bb-calculus. Each dimension of the cube corresponds to a new kind of dependency between terms and types.\n(from Wikipedia)\n\n\n\nIf you cover all of that then you can jump into this Q&A.\nWhat are the bases for different Proof Assistants?\n\nObviously this list can go on an on. My main purpose with this answer was to get one going with their bootstraps.\n\n\nBonus section for those who do not have a background in logic.\nforall x - An Introduction to Formal Logic\nby P.D. Magnus\n(pdf) (Site)\n\nforall x is an Open Education Resource (OER) introductory textbook in formal logic. It covers translation, proofs, and formal semantics for sentential and predicate logic. (from homepage)\n\nSince this is an OER there are many follow on books based on this. (Google search)\n\n\nBonus section for those who do not  have a background in \u03bb-calculus.\nThe Lambda Calculus\nStanford Encyclopedia of Philosophy\n\"An Introduction to Functional Programming Through Lambda Calculus\"  by Greg Michaelson\n(WorldCat) (Site)\n\"The Lambda Calculus Its Syntax and Semantics\"\nby H.P. Barendregt\n(WorldCat) (Site)\n\n\nBonus section for those lost in the terminology\nStanford Encyclopedia of Philosophy\nnLab\nProofWiki\nPLS Lab\nWolfram MathWorld\nEncyclopedia of Mathematics\n\n\nBonus section for those needing the history of types.\nFunctions and Types in Logic, Language and Computation\nby Fairouz Kamareddine\n(pdf)\n\n\nBonus section of food for thought.\nCounterexamples in Type Systems\n\nThe \"counterexamples\" here are programs that go wrong in ways that\nshould be impossible: corrupt memory in Rust, produce a\nClassCastException in cast-free Java, segfault in Haskell, and so on.\nThis book is a collection of such counterexamples, each with some\nexplanation of what went wrong and references to the languages or\nsystems in which the problem occurred.\nIt's intended as a resource for researchers, designers and\nimplementors of static type systems, as well as programmers interested\nin how type systems fit together (or don't).\n\n\n\nBonus section for Prolog programmers.\nHint: SWI-Prolog\nCOT5315 Foundations of Programming Languages and Software Systems\nby Robert van Engelen and Steven Bronson\n(ref)\nProlog code that does rules of inference (think typing rules). The code no longer works with the current version of SWI-Prolog but if one knows Prolog it is not hard to update.\n\nFound this today on GitHub and gave it a try. It is a very gentle way to get started with Prolog and natural deduction. As the code is for a bachelor\u2019s project thesis don't expect much but the core prover (system.pl) seems to work. Still have to hand verify the examples.\nnatural-natural-deduction\nby Flip Lijnzaad\nRelated paper: Towards Automated Natural Deduction in Prolog (pdf)\nNote: I did not try to get the Python code working; I just edited the code into unit test and ran all the test.", "meta": {"post_id": 930, "input_score": 17, "output_score": 20, "post_title": "So many data types, so little time"}}
{"input": "In Agda, irrelevance is an annotation which marks a parameter, record field, or definition which \"will only be typechecked but never evaluated\", with the consequence that irrelevant arguments and record fields are ignored when checking equality.\nYou can define records with irrelevant fields, e.g. this \"Squash\" type whose elements are definitionally equal:\nrecord Squash {\u2113} (A : Set \u2113) : Set \u2113 where\n  constructor squash\n  field\n    .unsquash : A\nopen Squash\n\nand these fields are counted in absurd patterns, e.g.\nfoo : Squash \u22a5 \u2192 \u22a5\nfoo ()\n\nbut by default the projection .unsquash : Squash A \u2192 A doesn't actually exist, meaning you can't directly use irrelevant fields, even in an irrelevant context!\nThere is an option --irrelevant-projections which adds irrelevant projections for irrelevant record fields, but it's incompatible with --safe. Being able to use an irrelevant field in an irrelevant context seems pretty innocuous, so why is --irrelevant-projections unsafe?", "output": "--irrelevant-projections can be used to prove the irrelevance axiom mentioned in the Agda manual:\n.irrAx : \u2200 {\u2113} {A : Set \u2113} \u2192 .A \u2192 A\nirrAx x = unsquash (squash x)\n\nand conversely, irrelevant projections are justified by the irrelevance axiom:\n.unsquash' : \u2200 {\u2113} {A : Set \u2113} \u2192 Squash A \u2192 A\nunsquash' (squash x) = irrAx x\n\nIrrelevant projections and the irrelevance axiom do not compute (so e.g. unsquash (squash x) does not reduce to x). In a previous version of Agda, irrelevant projections computed, which made them inconsistent.\nAccording to Jespr Cockx, although irrelevant projections are no longer inconsistent, the irrelevance axiom is still non-conservative and implies some form of non-constructive choice.\nFor example, this function looks like a formulation of the axiom of choice:\nchoice : \u2200 {\u2113\u2081 \u2113\u2082} {X : Set \u2113\u2081} {Y : X \u2192 Set \u2113\u2082}\n       \u2192 (\u2200 x \u2192 Squash (Y x)) \u2192 Squash (\u2200 x \u2192 Y x)\nchoice f = squash \u03bb x \u2192 unsquash (f x)\n\nThe introduction of a non-conservative, non-constructive axiom which does not compute by necessity is, to the best of my knowledge, why --irrelevant-projections remains unsafe despite no longer being inconsistent.\nSee \"Why does it matter if canonicity holds for irrelevant types?\"\nfor more information on the potential implications of an irrelevant non-constructive axiom.\nAnyone interested in definitional proof-irrelevance may wish to consider using Prop instead, which can be used for the same purpose as irrelevance, but without so many caveats.\nSome of the information in this answer has since been included in the Agda manual.", "meta": {"post_id": 931, "input_score": 8, "output_score": 8, "post_title": "Why is `--irrelevant-projections` unsafe in Agda?"}}
{"input": "In the concluding statement of \"universe hierarchies\", Conor McBride calls it\n\n[...] that key lesson which I learned from James McKinna: never resort to polymorphism when initiality will do.\n\nHow can you recognize when you could use initiality instead of polymorphism, and why should you prefer to use initiality?", "output": "Initiality comes with a powerful universal property which allows you to, internally, prove statements about the constructions you perform. If you give me an element of data Nat = Z | S Nat, I can perform induction on it.\nPolymorphism on the other hand requires you to appeal to external principles such as parametricity (*) to recover similar reasoning principles. If you give me a function of type (a -> a) -> a -> a, I will know it is bound to be a natural number in disguise but will be powerless to exploit that fact.\nAdditionally in predicative systems with a tower of universes, initiality is typically size-respecting whereas an encoding via polymorphism will land you a definition that lives one level up in the hierarchy due to the universal quantification. If you want to iterate these constructions, that's bad news.\n(*) caveat: there are type theories attempting to internalise parametricity", "meta": {"post_id": 939, "input_score": 15, "output_score": 21, "post_title": "Why should you \"never resort to polymorphism when initiality would do\"?"}}
{"input": "From my rough understanding, an (external/internal) realignment property says that given a type $A$, a proposition $p : \\Omega$, and a partial isomorph $B : p \\to \\sum_{A'} A' \\cong A$, we can extend it to a total one, such that it is strictly equal to $B$ given that $p$ is true.\nSterling et al. used this technique when building models for cubical type theories. However, in Sterling's thesis, he also extensively used realignment to adjust various definitions when setting up Tait's computibility for MLTT.\nI cannot (yet) fully appreciate the power of realignment. It seems to resemble the cubical gluing operation, so I'd expect it to be useful there. But is it necessary (or to put it another way, does it greatly simplify various arguments) for non-univalent type theories?\nMore specifically, even if assuming the realignment property holds in some topos helps, we still need to prove this property when actually instantiating a concrete topos, right? I cannot locate such a proof anywhere in Sterling's thesis, perhaps there's something that I overlooked? Any help is appreciated!", "output": "This is a very good question.\nWhat is realignment for?\nOne application of realignment that is particularly useful is to construct a cumulative hierarchy of universes; what cumulativity means in the \"objective\" sense is that you have a hierarchy U1 >-> U2 >-> U3 ... where each coercion is injective, and you have a choice of type operations at each level i like Pi_i x:A. B(x)  such that the coercion from Ui to Uj commutes with the chosen type operations. It is semantically very hard to get this to happen!\nRealignment is, however, much stronger than cumulativity; so if you have a hierarchy of universes linked by injective coercions but you have chosen type operations that do not commute with these operations, realignment can be used to replace all these type operations with ones that do commute with the coercions.\nThe cumulativity example is nice --- since if you have realignment in a sheaf topos (which we, in fact, do have in every sheaf topos as I will explain in the second half of my answer) then we have a strict interpretation of full MLTT with cumulative universes. Thus we can use a very naive denotational semantics to prove a variety of independence results for MLTT quite easily. This contradicts almost a decade of regrettable misinformation and confusion on the topic of universes in sheaf topoi.\nIn addition to cumulativity, realignment is, as you point out, the critical ingredient for constructing univalent universes in models of homotopy type theory. It is also useful for synthetic Tait computability, as you mention. Can we find uses for it in \"ordinary\" work-a-day type theory? My work recently has focused on finding applications for the modalities of synthetic Tait computability that have nothing to do with Tait computability --- for instance, the cost aware logical framework and my recent account of type based security/information flow control with Bob Harper. In these cases, we can imagine a type theory where realignment would play some useful role internally --- i.e. a new \"glue\" connective that lets you make a type that restricts up to definitional equality to some other type underneath a certain security level or phase, etc. It's unclear how useful this will be, but it could be useful.\nWhat models of realignment do we have?\nYour other question about whether you can in fact validate realignment in semantic models is also excellent. The state of play is that so long as your ambient mathematical universe is boolean (as $\\mathbf{Set}$ is), then any Grothendieck topos has a hierarchy of universes satisfying realignment. This is proved by Daniel Gratzer, Mike Shulman, and myself in our recent manuscript Strict universes for Grothendieck topoi --- the results were complete at the time that I claimed \"realignment is fine in topoi\" in my thesis, but the paper was not ready to be released unfortunately.\nThere are a number of open questions remaining.\n\nHofmann and Streicher give a simple construction of universes in any presheaf topos, and this is perfectly constructive. However Andrew Swan has proved that this particular universe construction cannot satisfy realignment for all monomorphisms unless the ambient mathematical universe is boolean. Meanwhile Gratzer, Shulman and Sterling have given a different construction of universes that applies in any Grothendieck topos, but their construction is very classical and employs both LEM and AC. Is there a modification of the GSS construction that works constructively and hence over a non-boolean base topos? We expect that the base topos will nonetheless need to satisfy a weak / constructively acceptable choice principle such as WISC.\n\nA less ambitious but also reasonable question is whether there is a universe construction specifically for realizability topoi that satisfies realignment, assuming perhaps that we are looking at $\\mathbf{RT}(\\mathbb{A})$ for a pca $\\mathbb{A}$ internal to a boolean topos like $\\mathbf{Set}$.\n\n\nAs far as I am aware, both of these questions are open. Daniel and Mike and I tried really hard to find a constructive version of our work, but got stuck.", "meta": {"post_id": 942, "input_score": 10, "output_score": 11, "post_title": "What is realignment and is it useful in non-univalent theories?"}}
{"input": "You can encode ordinals in Coq as\nInductive ord := O | S (n: ord) | Lim (s: nat -> ord).\n\nSuppose you use the following encoding instead\nCoInductive stream A := {\n  head: A ;\n  tail: stream A ;\n}.\nInductive ord := O | S (n: ord) | Sup (s: stream ord).\n\nCan you make an induction principle go through?\nI tried in both Coq and Agda but the obvious induction principle doesn't seem to work\n{-# OPTIONS --guardedness #-}\nmodule ord where\n\nrecord stream (A : Set) : Set where\n  coinductive\n  constructor _::_\n  field\n    head : A\n    tail : stream A\n\nrecord Forall {A : Set} (P : A \u2192 Set) (x : stream A) : Set where\n  coinductive\n  constructor _:>_\n  field\n    forhead : P (stream.head x)\n    fortail : Forall P (stream.tail x)\n\ndata ord : Set where\n  o : ord\n  s : ord \u2192 ord\n  sup : stream ord \u2192 ord\n\nopen stream\nopen Forall\n\nind : (P : ord \u2192 Set) \u2192 P o \u2192 ((x : ord) \u2192 P x \u2192 P (s x)) \u2192 ((x : stream ord) \u2192 Forall P x \u2192 P (sup x)) \u2192 (x : ord) \u2192 P x\nind P onO onS onSup = loop where\n  loop : (x : ord) \u2192 P x\n  loop o = onO\n  loop (s x) = onS x (loop x)\n  loop (sup x) = onSup x (gen x) where\n    gen : (y : stream ord) \u2192 Forall P y\n    forhead (gen y) = loop (head y)\n    fortail (gen y) = gen (tail y)\n\n(Agda termination checker complains about the recusive calls to gen and loop.)", "output": "It makes sense to want something like this, but Agda's termination/productivity checker does not actually validate this interpretation of the types. The reasoning behind your induction principle is presumably:\n\n$\\mathsf{stream}\\ A$ is an infinitely long stream of whatever $A$ is.\n$\\mathsf{ord}$ is a well-founded tree that delegates to $\\mathsf{stream}$ to achive an infinitely wide branching factor.\n\nHowever, Agda accepts the following definition:\nbad : ord\nbads : stream ord\n\nbad = lim \u03bb where\n  .head \u2192 zero\n  .tail \u2192 bads\n\nbads .head = bad\nbads .tail = bads\n\n$\\mathsf{bad}$ is not a well-founded tree, and contradicts the induction principle:\nP : ord \u2192 Set\nP zero = \u22a4\nP (suc o) = \u22a4\nP o@(lim s) = o \u2261 head (tail s) \u2192 \u22a5\n\nlemma : \u22a5\nlemma =\n  ind P _ (\u03bb _ _ \u2192 _)\n    (\u03bb s fp eq \u2192 subst P (sym eq) (forhead (fortail fp)) eq)\n    bad refl\n\nThe issue is that coinduction in Agda (and I suppose also Coq) does not really act in a 'compositional' way. You can't just say that $\\mathsf{ord}$ is the initial algebra of the functor $1 + o + \\mathsf{stream}\\ o$; it actually has some more complicated specification owing to $\\mathsf{stream}$ being defined coalgebraically.\nThere are some (not great) justifications you could use for this behavior. The above functor is not strictly covered by the more rigorous foundational schemas for (co)inductive definitions, so in that sense it requires some interpretation, and that interpretation could well be one that matches Agda's checker. Also, if the two types were mutually defined, it's a lot less clear whether it should mean something primarily algebraic or coalgebraic. But it's unfortunate that some mixed (co)inductive specifications just can't be written in Agda/Coq.\n(Note, I'm not very familiar with coinduction in Coq; just assuming based on the question that it behaves similarly to Agda.)\nAddendum: if you want to more reliably predict what definitions Agda will accept for types involving coinduction, it helps to imagine that they are actually picking out certain well-defined values of an analogous domain-theoretic definition (where coinduction is more 'lazy'). But even that is not completely straight forward. For instance, using\nbad = lim bads\n\nabove would be rejected.", "meta": {"post_id": 961, "input_score": 12, "output_score": 8, "post_title": "Is induction over mutually inductive coinductive types possible?"}}
{"input": "The Markov's principle states that if a Turing machine does not run forever, then it halts. Equivalently, if I have a function $f : \\mathbb N \\to \\mathrm{Bool}$, such that I have proved that $\\neg\\neg \\exists n. f(n)=\\mathrm{true}$, then I can find $\\exists n. f(n)=\\mathrm{true}$.\nThis principle is sometimes considered as constructively valid, since we can give a program that just enumerates $\\mathbb N$ and searches for the solution, and if our meta-logic is classical, we can see that this program halts, which means that it is constructively true for each concrete function $f$.\nWhat are the best practices to incorporate this principle into various proof assistants? The most na\u00efve approach is, of course, assume it as an axiom. But I would also like to have computational properties. So for each concrete function $f$, I can do this in Agda:\nf : Nat -> Bool\nf = blah blah\n\nf-good : \u00ac \u00ac \u2203[ x \u2208 Nat ] f x \u2261 true\nf-good = blah blah\n\nprivate\n  {-# TERMINATING #-}\n  helper : Nat -> \u2203[ x \u2208 Nat ] f x \u2261 true\n  helper n with f n in eq\n  ... | true = n , eq\n  ... | false = helper (suc n)\n\nmagic : \u2203[ x \u2208 Nat ] f x \u2261 true\nmagic = helper 0\n\nI have to seal helper as private, because it indeed does not terminate on some input. But given f-good, we can be (meta-classically) sure that magic will halt.\nHowever, this has some drawbacks:\n\nIf the normalization strategy is wrong, then this program will not terminate. For instance, if Agda chose to first fully expand all the branches before picking one, then it will keep entering the false branch.\nIf I try to state the general Markov's principle instead of on concrete $f$'s, then Agda will indeed loop in certain cases.\nI might accidentally make some unsafe leaks.\n\nIs there any better way of doing this? Also, is there any proof assistant that actually supports this with a flag or something?", "output": "There are different ways to implement Markov's principle, and they're not equivalent. I know of at least three techniques to implement MP.\nMP as a loop\nI personally find this to be a hack inherited from untyped realizability. As you observed already, it breaks the good metatheoretical properties of your system, in particular strong normalization. It also requires some form of MP in the metatheory, which is not a blocker but not great either. I guess that if your favourite proof assistant is some variant of PRL this is not so much a problem, but if you only swear by MLTT this is not going to do the job.\nMP via Dialectica\nIt is well-known that the Dialectica interpretation realizes MP. The underlying implementation is very different from an unbounded loop. Morally you can picture it as some form of delimited continuations triggered by the access to bound variables in the calculus. I wrote a Dialectica model of CoC in my PhD, and we have an unpublished account of a Dialectica model of MLTT with Andrej Bauer, but it's not very implementable. It relies on a structure of finite multisets that enjoy definitional equations and it is not even clear we can get a decidable type system out of it.\nMP by static exceptions\nThe third technique is, I believe, a simplification of the Dialectica approach that is easy to explain computationally and implement. Essentially, it consists in adding a special kind of exceptions to the ambient type theory. In order to preserve consistency you have to forbid exceptions at toplevel though, and ensure that they are always statically caught.\nThere are two ways to understand this, either in direct style or through a program translation. These two papers only handle a weak logical system, but the technique can actually be lifted to MLTT.\nThe resulting theory can be described as the composition of an exceptional model with a strict presheaf model. Both processes are relatively simple compilation phases so in the end you can just compile away MLTT + MP into a reasonable variant of MLTT. The result is described in this paper.\nSince the presentation is synthetic, you can actually dump the syntactic model phase altogether and simply add the corresponding combinators with the reduction rules inherited from the underlying computational model. Dynamic exceptions are fairly easy to add through rewrite rules so I guess it is only a matter of correctly implementing the static try-catch handlers.", "meta": {"post_id": 972, "input_score": 12, "output_score": 15, "post_title": "Incorporating Markov's principle in various proof assistants"}}
{"input": "I've been looking at syntactic multicategories for mechanizing some type theory stuff.\nBut multicategories are pretty messy to work with in a two sorted definition like the usual dependently typed encoding.\nStating the operations alone without the laws is a mess. You need extra Forall and concat helpers too to iterate over g the list of argument Homs.\nIn practice you'd also add another layer of finickyness with setoids.\nClass Multicategory := {\n  Obj: Set ;\n  Hom: list Obj -> Obj -> Set ;\n\n  id A: Hom [A] A ;\n  compose {B C}:\n    Hom B C ->\n    forall (g: Forall (fun o => { a & Hom A o }) B),\n    Hom (concat g) C\n}.\n\nThe two sorted definition as a monad in a multispan is more sensible but loses some ease of use.\nI'm thinking about trying the single sorted definition or the colored operad approach.\nI'd be curious if anyone has any concrete experience with mechanizing this sort of thing.", "output": "[I'm posting this by proxy for Astra, who is having technical difficulties posting on this site.]\nHey! I recently completed a large scale formalisation in which I independently rediscovered cartesian multicategories as being the exactly correct and most economic way of encoding the structure of non-dependent type theory. (I refer to these as Simple Contextual Categories.) This notion is completely central to all of my constructions.\nMy project was to formalise Altenkirch's paper Categorical reconstruction of a reduction-free normalisation proof. The main idea is that simply typed lambda calculus is the language of cartesian closed simple contextual categories. We obtain a normalisation proof by looking at two such (CCSC) categories: presheaves and twisted glueings. I think that the main contribution of the formalisation is a particularly nice formulation of simple contextual categories, which seems to be what you're asking about. The paper on this formalisation is currently in the process of being written, but the code has been online for some time now.\nIn order to read the definition, first read the first 28 lines, plus the definition of derive, in the following file:\nhttps://github.com/FrozenWinters/stlc/blob/main/lists.agda\nSubsequently, you can go directly to the definition of a simple contextual category in this file:\nhttps://github.com/FrozenWinters/stlc/blob/main/contextual.agda\nThe main definition is in lines 28-63 of the second file, and we then go on to develop the theory of weakenings and variables that is inherent to every simple contextual category.\nThe bulk of the project consists of constructing examples of such structures.\nI believe that there is a dependent analogue of this construction; this is a current research question on which some progress has been made.", "meta": {"post_id": 985, "input_score": 11, "output_score": 8, "post_title": "Ergonomic use of multicategories in proof assistants"}}
{"input": "Is there a theory of typing proof objects or proof trees?\nPartially inspired by this question and answer, I was wondering about separating type checking into two phases: the first phase infers a full proof tree and the second phase checks if the proof tree is well formed and conclusions follow from premises.\nPotentially, this sort of separation allows a smaller kernel. Also the kernel can be written as a total function within a prover of similar power without extra axioms. I think it's the driver which infers the proof trees which would require extra axioms.\nWhat I mean is you can provide explicit witnesses for typing judgements.\nFor the STLC, something like this:\n$$\\frac{\\begin{split} q_0 \\colon [ \\Gamma \\vdash e_0 \\colon \\tau_0 \\rightarrow \\tau_1 ]  \\\\\nq_1 \\colon [ \\Gamma \\vdash e_1 \\colon \\tau_0 ]\n\\end{split}}{ q_0 q_1 \\colon [ \\Gamma \\vdash e_0 e_1 \\colon \\tau_1 ] } $$\n$$\\frac{q \\colon [ \\Gamma , x \\colon \\tau_0 \\vdash e \\colon \\tau_1 ]}{ \\lambda x \\colon \\tau_0. q \\colon [ \\Gamma \\vdash \\lambda x \\colon \\tau_0 . e \\colon \\tau_0 \\rightarrow \\tau_1 ] } $$\n$$\\frac{x \\colon \\tau \\in \\Gamma}{ x \\colon \\tau \\in \\Gamma \\colon [ \\Gamma \\vdash x \\colon \\tau ] } $$\nThe meat and potatoes is when you have more complicated features like dependent types. You'd want explicit witnesses for judgemental equalities something like:\n$$ \\frac{\\begin{split}q \\colon [ \\Gamma , x \\colon \\tau \\vdash e \\colon  \\tau' ] \\\\\nq' \\colon [ \\Gamma \\vdash \\colon e' \\colon \\tau ]\\end{split}}{\\beta(x \\colon \\tau \\mathrel{:=} q' , q) \\colon [ \\Gamma \\vdash (\\lambda x\\colon \\tau. e) e' \\triangleright_\\beta [x \\mathrel{:=} e'] e ]} $$\n$$ \\frac{\\begin{split} q \\colon [\\Gamma \\vdash \\tau \\equiv \\tau'] \\\\\nq' \\colon [\\Gamma \\vdash e \\colon \\tau]\n\\end{split} }{ \\textbf{J}(q, q') \\colon [ \\Gamma \\vdash e \\colon \\tau' ]} $$\nAnyhow, it quickly gets very redundant, confusing and unreadable.\nI'm confused about several aspects of this.\n\nYou want proof trees to be trivial to check but you also want them to not be redundant. What should it mean for it to be \"trivial\" to infer a property of the root of the proof tree like the environment or type?\n\nIt's really confusing to notate proof trees. Is there a better way of writing this silliness?\n\nAre there existing references or work in this direction?\n\nIt's hard to come up with a reasonable meaning for proof trees. I guess you can see them as terms annotated with explicit environments and types?", "output": "What you are describing already happens to a large extent in a proof assistant with a trusted kernel.\nSide remark: I prefer to say \"derivation\" instead of \"proof tree\" because derivations need not be about proofs, they can also be about constructions.\nUser input typically requires elaboration to a more explicitly annotated type theory (the process of elaboration is intertwined with the process of type-checking), let us call it the core type theory. The core type theory is designed in such a way that the judgements already encode the proof trees, except for judgemental equalities.\nFor example, in the simply-typed $\\lambda$-calculus with explicitly annotated types of variables ($\\lambda (x : A) . e$ instead of just $\\lambda x . e$), the derivation of $\\Gamma \\vdash e : A$ can be read off immediately from $e$. There is no point in storing any extra information anywhere, as it is all there already. One only has to process $e$ to either reconstruct the derivation or decide that $e$ does not encode a valid one. This is the meaning of \u201cterms as proofs\u201d.\nProofs of judgemental equalities are typically not recorded for two reasons:\n\nThey are proof irrelevant: it does not matter which proof is provided, as long as there is one. Consequently, if we have the means to reconstruct them when they exist, we need not store them.\n\nIt is often deemed that derivations of judgemental equalities are too large to be stored. Instead, the core type theory is designed to have decidable equality checking so that the missing derivations can be reconstructed algorithmically. In a sense we are storing the derivations in time rather than in space.\n\n\nTo specifically address your questions:\n\nThere are techniques for making derivations less redundant, such as bidirectional type-checking which synthesizes some information. You are correct that redundant information is a nuisance because we need to check that the redundant copies agree.\n\nWith the exception of judgemental equalities, the derivations are already encoded by the terms, at least when the core type theory is explicit enough. (See the notion of standard type theory in the reference given in the next point.)\n\nThe idea that the derivation is directly reconstructible from the proof terms is manifested as an inversion principle, see Theorem 3.24 in Finitary type theories with and without contexts by Philipp Haselwarter and myself, and the subsequent paragraph.\n\nAs already said, when the theory is nice enough the derivation is encoded by the judgement, so its meaning can be taken to be the same as the meaning of the judgement. If the theory is not nice enough, then there is another theory which is nice enough that covers the original one, in which case the meaning of the former can be derived from the meaning of the latter. This is the point of the Elaboration theorem in Chapter 10 of Anja Petkovi\u0107 Komel's doctoral dissertation.", "meta": {"post_id": 994, "input_score": 7, "output_score": 9, "post_title": "Typing proof trees or proof objects"}}
{"input": "I'm working up from elaboration zoo and noticed that you don't use fixed point if you've got type level computation. It causes unification/equality check to hang up. Now, this means that I need inductive data structures of some kind if I want to compute on type-level.\nWhat would be the easiest way to introduce inductive types?\nW-type has a description that could be interpreted, but it is potentially cumbersome in practice.\nW : {A : Set} \u2192 (B : A \u2192 Set) \u2192 Set\nsup : (a:A) \u2192 {B} \u2192 (B a \u2192 W B) \u2192 W B\nelim : {B} \u2192 (C : W B \u2192 Set) \u2192 ((a:A) \u2192 (f : B a \u2192 W B) \u2192 (c : (b : B a) \u2192 C (f b)) \u2192 C (sup a f)) \u2192 (w:W B) \u2192 C w\n\nFor instance, with nat we'd have:\nNat : W 2 {empty, unit}\n\nzero : Nat\nzero = sup 0 absurd\n\nsucc : Nat \u2192 Nat\nsucc n = sup 1 (\u03bb_. n)\n\nElimination without pattern matching would be inconvenient, but doable.\nI have a hunch that there's not many ways to put this together. It's some variation of W-type that I would have to implement. But which variant?", "output": "In principle, every indexed inductive type can be defined from $\\bot$, $\\top$, $\\mathsf{Bool}$, $\\Pi$, $\\Sigma$, $\\mathsf{W}$, $\\mathsf{Id}$ and universes, with exactly the expected eliminators and computation rules: https://jashug.github.io/papers/whynotw.pdf. But this has a lot of encoding overhead.\nIf we want ergonomic support for inductive types, that's not simple at all and we have a number of design choices. There's a basic trade-off between ergonomics and implementation complexity.\n\nA very basic choice is between eliminators and recursive definitions (including fixpoints). Eliminators are formally the easiest, but can be very inconvenient in practice. Recursive definitions tend to require a lot more effort to implement.\nCoq has inductive signatures, case splitting and fixpoints. As you mention, fixpoints diverge if we try to naively compute them. In Coq, termination checking marks an argument of a fixpoint as decreasing, and fixpoints only compute when applied to a canonical value in the decreasing argument position.\nMini-TT allows recursive definitions, without termination checking. The obvious downside is that users have to perform termination checking instead (when necessary), but it's much easier to implement. However, we still have to avoid divergent unfolding. The Mini-TT solution is essentially to disallow unfolding of recursive names in certain contexts, so that e.g. we can't prove that a renamed version of a recursive function is convertible to the original version. Compare Coq, where fixpoint bodies can be compared for conversion. However, comparing recursive function bodies is not super useful in practice, because $\\beta\\eta$-conversion of definitions is far less common than extensional (pointwise) equality of functions. We usually want function extensionality, and richer $\\beta\\eta$-conversion by itself is not sufficient.\nAgda has recursive definitions and termination checking, and no fixpoints and no conversion checking for recursive function bodies. Agda has by a fair margin the most powerful termination checker and the most powerful pattern matching elaboration. This is often very convenient, but the implementation is not for the faint of heart: https://dl.acm.org/doi/10.1145/3236770\n\nAnother variation is the range of allowed inductive types. Here, the easy choice is to stick to the inductive types which are reducible to W-types; these are roughly Dybjer's classic inductive families: https://www.researchgate.net/publication/226035566_Inductive_families.\nGoing beyond this is another way of complicating implementations. Induction-recursion and induction-induction are the most notable generalizations. These can be also implemented using eliminators or recursive definitions, but right now only Agda supports them. In Agda, induction-recursion and induction-induction arise as a \"side effect\" of positivity checking, termination checking and mutual definitions, but nothing prevents us from doing an eliminator-based implementation which is more amenable to small core syntaxes.", "meta": {"post_id": 999, "input_score": 11, "output_score": 13, "post_title": "Easy ways to introduce inductive types"}}
{"input": "In A Cubical Language for Bishop Sets, the authors write:\n\nAs a consequence of its fundamentally untyped nature, formalizing a\ntheorem in Nuprl does not imply the correctness of the corresponding\ntheorem in standard classical math-ematics (the global mathematics of\nconstant or discrete sets), nor even in most forms of constructive\nmathematics (the local mathematics of variable and cohesive sets). It\nis worth noting that the problem is not located in the presence of\nanti-classical principles (which are interpretable in logic over a\nvariety of topoi), and rather arises from the commitment to untyped\nontology.\n\nWhat is meant by this statement? I would wager that the NuPRL team disagrees, as some of their publications are about mathematical theorems, including: Formalizing Category Theory and Presheaf Models of Type Theory in Nuprl and Constructing Analysis and Experimental Mathematics using the NuPRL Proof Assistant.\nIs there an example of a theorem that is provable in NuPRL, but that a mathematician would reject as false? Also, it shouldn't be conjured up in ZFC versus MLTT, as theorems like $1 \\in 2$ are generally understood and happily ignored.", "output": "As an author of this paper, I think I would write it a little bit differently if I had the chance today. Before I begin, let me echo Andrej's comment that Nuprl is a very significant moment in the history of type theory and even constructive mathematics, and none of what I say (then or now) is meant to detract from that. I obviously have immense respect for Nuprl and the people who built it, who were in many ways pioneers in type theory and in interactive proof assistants. Much of what we have today (like Coq and Agda) owes its present shape to the ideas developed by the PRL Group.\nTo me the difficulty with relating Nuprl to mathematics is basically one of methodology. As Andrej says, Nuprl's Computational Type Theory is based on \"truth in one model\"; as a result, there are many things that are true in this specific model that are false in the category of sets, false in many categories of presheaves, and false in many categories of sheaves. This is not the fault of (e.g.) realizability semantics, but rather the fault of confounding syntax and semantics. Both are important, but semantics benefits from multiplicity --- and the multiplicity of semantics is embodied in syntax. We can therefore expect strange results if we say that syntax is just a way to speak about one single example of semantics.\nSo my aim is not to say \"realizability is bad\" --- realizability is clearly very good. But I think it is bad on balance to base a proof assistant on one single model (bad in ways that COULD NOT have been anticipated [clarification: by that community] in the early 1980s when this was going on!) because it limits the applicability of your results.\nBecause Nuprl incorporates axioms that are not true in ordinary math, nor in the relative ordinary math of topoi, we cannot take a Nuprl proof about groups and use it as evidence to a \"proper mathematician\" for the truth of that statement about groups in a way that applies to that mathematician's work. This limits the ability to communicate and re-use results, but that is to me the entire point of mathematics.\nI want to end by saying that my perspective on mathematics is not the only one. Nuprl is much inspired by the ideas of L.E.J. Brouwer who took a very different viewpoint --- a proof in Brouwer's style about groups also does not necessarily lead to evidence that a mathematician would accept for the truth of that statement about groups. But Brouwer's perspective was that all the mathematicians were wrong, and that only he was right. If that was actually so, then one could not blame him for doing his proofs in a way that was not backward compatible.\nTherefore, the question that Nuprl raises is nothing less than: is mainstream mathematics wrong? Back when I was building tools based on Nuprl, I believed that normal mathematics was wrong. I no longer believe that though.\nLastly, I want to say that the above applies to Nuprl when viewed as a general-purpose language for mathematics --- for the reasons I describe, it cannot be a general-purpose language in that sense. But this is not the only thing Nuprl has been: Nuprl has also been a \"synthetic domain theory\" (an implementation of the slogan \"domains are just types, don't worry about continuity\" --- albeit quite different from the one pursued by the category theorists). Nuprl's synthetic domain theory aimed to let you reason about programs that use partial functions in ways that don't terminate, like LCF. One can argue today whether their approach was the best or not, but I think this is a very interesting aspect of Nuprl that is less well known than it should be. Nuprl qua synthetic domain theory doesn't claim to be general purpose, only useful for a specific purpose. So I think that's a good way to think about Nuprl that respects the great accomplishments of that team.", "meta": {"post_id": 1012, "input_score": 15, "output_score": 16, "post_title": "Can mathematical formalizations in NuPRL be trusted as correct in the greater mathematical practice?"}}
{"input": "I've heard about a folklore claim that\n\nIf all terms of \u2115 are literals, all closed terms admit canonical form.\n\nIn MLTT-style type theories.\nI am assured that it's true for Bool if one also assumes consistency, namely the canonicity for empty type. Unfortunately, I don't know much about the general case, and couldn't find references. I suppose that it's related to encoding terms by natural numbers or something similar...\nDoes anyone know a rigorous formulation or a proof (or a disproof)?", "output": "Your question is quite vague, so let me give you both an intuition on why this ought to be true, and a counterexample.\nAs for intuition, let me show how supposing only canonicity\nfor $\\mathbb{N}$ (natural numbers) one can deduce canonicity for $\\mathbb{B}$ (booleans). Suppose we are given a closed boolean $\\vdash b : \\mathbb{B}$, and consider\n$$\\mathtt{if}~b~\\mathtt{then}~0~\\mathtt{else}~0$$\nThis is a natural number in the empty context, so it must reduce to a canonical form. But to do so it must be the case that $b$ also reduces to\na canonical form, because this is the only way for the $\\mathtt{if}$ statement to reduce!\nAbstracting a bit, if we can construct a context $\\vdash C[\\cdot] : T \\Rightarrow T'$, that is a term with a hole such that whenever a term $\\vdash t : T$ is plugged in said hole the whole term is of type $T'$, and such that $C[t]$ has a canonical form only if $t$ has, then canonicity for $T'$ implies canonicity for $T$.\nBut there are perfectly valid cases where such a context cannot be built, and this is intentional! My favorite example is the sort of (proof-irrelevant) propositions $\\mathtt{SProp}$. As shown in a recent article (and its predecessor), you can prove canonicity for natural numbers in MLTT extended with $\\mathtt{SProp}$ without proving canonicity of inhabitants of strict propositions \u2013 you only have to assume consistency. The trick is to control the way one can use those to build relevant terms (in our case, natural numbers) so that no context like the above can be built. The idea is roughly to allow for an eliminator from $\\mathtt{SProp}$ to $\\mathbb{N}$ only for the false proposition. This ensures that a natural number in the empty context cannot be stuck on this eliminator since that would mean that one has an inconsistency in the theory \u2013 the term on which the eliminator is applied would be a proof of falsity in the empty context.", "meta": {"post_id": 1026, "input_score": 13, "output_score": 19, "post_title": "Does the canonicity of natural number imply that of all types?"}}
{"input": "Mostly when I read about impredicativity I see people bemoaning its downsides.\nBut it's not clear to me why I would want impredicativity in the first place.\nImpredicativity is useful for analyzing impredicative systems. And impredicative Pi types can be used to encode recursive datatypes without going up a universe. However, from what I understand Cedille has to go through a few contortions to have induction principles over impredicative encodings of datatypes.", "output": "There are some tricks that only work when you have access to an impredicative universe. They tend to construct \"the smallest object\" of some kind, without an explicit construction, i.e. a textbook impredicative encoding.\nA fancy one that comes to my mind is Mendler encoding. In a nutshell, given some function P : (Prop -> Prop) -> (Prop -> Prop) in CIC, this encoding constructs the \"free strictly positivization\" of P. This can be used to compose inductive predicate constructions in a modular way, since one effectively abstracts over the syntactic criterion of strict positivity.", "meta": {"post_id": 1037, "input_score": 17, "output_score": 8, "post_title": "What are the advantages to impredicativity?"}}
{"input": "I've been trying to prove this in Agda:\nlemma : (P : \u2115 \u2192 Set)\n  \u2192 ((n : \u2115) \u2192 Dec (P n))\n  \u2192 Dec (\u03a3[ n \u2208 \u2115 ] \u00ac (P n))\n\nIn words:  We have an infinite sequence of decidable propositions.  Can we conclude that either a) all the propositions are true or b) you can find a counterexample?\nI'm 70% sure this is not a conclusion you can draw without some extra axiom.  Is there a name for this, or some papers I can read?  If you need an extra axiom, what's a good one to go with?\n(I came to this after working for a bit on Konig's Lemma, which I also suspect can't be proven without some additional assumptions.)", "output": "This is equivalent to solving the Halting Problem (perhaps relativized to an oracle), so this is a very strong assumption.\nFix a Turing machine M, suppose P n is the statement that M does not halt after being run n steps. This is decidable by simply running the machine. However, \u03a3[ n \u2208 \u2115 ] \u00ac (P n)) asks whether it is possible to find an n such that M halts in n steps. That is undecidable for a general Turing machine M.\nAs a general principle, being able to solve the Halting Problem relative to any oracle, is equivalent to the Limited Principle of Omniscience (LPO) as mentioned by Trebor. K\u00f6nig's Lemma (KL) for general finitely branching trees is known to be equivalent to this, provided one has an enumeration of the nodes. In the most general possible setting, one might first need a weak form of choice \u2014 namely, to choose one from each of a countably infinite sequence of nonempty finite sets \u2014 and then use LPO to prove the full form of K\u00f6nig's Lemma.\nHowever, the Weak K\u00f6nig's Lemma (WKL), which is limited to trees of finite $0,1$-sequences is known to be strictly weaker. In fact, it is related to the Lesser Limited Principle of Omniscience (LLPO) in the sense that for any infinite decidable tree T of finite $0,1$-sequences there is an infinite sequences of LLPO instances such that any simultaneous solution to all of these can be used to compute an infinite path through T.", "meta": {"post_id": 1039, "input_score": 7, "output_score": 8, "post_title": "Decidability for an infinite sequence of decidable propositions"}}
{"input": "I have the following problem. I have a type $\\alpha$, function $f : \\alpha \\to \\mathbb{N}$ and predicate $P : \\alpha \\to \\mathrm{Prop}$ and I want to prove that for all $a : \\alpha, P a$.\nHow could prove this with induction on the value $f a$, i.e. cases where $f a = 0$ and $f a > 0$?\nMy current trial in Lean looks like this:\nlemma foo (\u03b1 : Type) (f : \u03b1 \u2192 \u2115) (P : \u03b1 \u2192 Prop)\n  (base : \u2200 a : \u03b1, f a = 0 \u2192 P a)\n  (ind : \u2200 a : \u03b1, f a > 0 \u2192 \u2203 b : \u03b1, f b < f a) :\n  \u2200 a : \u03b1, P a\n\nIn the inductive case, I want to get the hypothesis P b.\nI tried to use nat.strong_induction_on, but I can't figure out what to apply it on.\nAm I doing something wrong?", "output": "Here's how to prove Andrej Bauer's corrected statement using the induction tactic:\ntheorem foo (\u03b1 : Type) (f : \u03b1 \u2192 \u2115) (P : \u03b1 \u2192 Prop)\n  (ind : \u2200 a, (\u2200 b, f b < f a \u2192 P b) \u2192 P a) :\n  \u2200 a, P a :=\nbegin\n  intro a,\n  induction hn : f a using nat.strong_induction_on with n ih generalizing a,\n  apply ind,\n  intros b fb_lt_fa,\n  rw hn at fb_lt_fa,\n  exact ih _ fb_lt_fa _ rfl,\nend\n\nWe're using lots of its optional features at once here:\n\nhn : _ lets us remember that the variable we're inducting on is equal to f a\nusing nat.strong_induction_on tells it to use a non-default recursion scheme\ngeneralizing a ensure that a is in a binder in our inductive hypothesis ih\n\nNote that we don't need base.", "meta": {"post_id": 1049, "input_score": 8, "output_score": 10, "post_title": "Strong induction on \u2115 with function \u03b1 \u2192 \u2115"}}
{"input": "Sometimes, trying to use rw in Lean, we get an error saying\nmotive is not type correct\n\nWhat does this mean? Often simp_rw succeeds, so what we want to rewrite can be rewritten and the end: why rw is not enough?", "output": "This tends to show up when trying to rewrite a term that appears as a dependent argument.\n\nTo understand this, let\u2019s see how rw actually works, by way of a small example:\nI\u2019m using list.last with type\nlist.last {\u03b1 : Type u} (l : list \u03b1) : l \u2260 list.nil \u2192 \u03b1\n\nas an example for a function with a dependent argument: The second argument is a proof that mentions the first argument l.\nLet\u2019s try to prove the following lemma:\nopen list\n\nexample (\u03b1 : Type*) (l : list \u03b1) (h1 : l \u2260 nil) (x : \u03b1) (hx : l = [x]) :\n  last l h1 = x :=\nbegin\n\nWe might want to start to rewrite with that equality, but\n  rw hx,\n\nfails with\nrewrite tactic failed, motive is not type correct\n  \u03bb (_a : list \u03b1), l.last h1 = x = (_a.last h1 = x)\n\nAnd the error message sheds more light into why it fails: The rw tactic takes the goal, which is\nl.last h1 = x\n\nfinds the occurrences of the left-hand side of the equality, i.e. l, and then tries to equate the goal as we have it with the goal where l is replaced by a parameter _a. Adding one set of parenthesis clarifies things maybe a bit:\n  \u03bb (_a : list \u03b1), (l.last h1 = x) = (_a.last h1 = x)\n\nNormally rw can use this function to turn the equality given to it (hx) into an equality between the existing goal and the goal you\u2019d expect after the rw, but it doesn\u2019t even get that far: The expression above is not type-correct! In _a.last h1, the h1 is still l \u2260 nil, but it now should be _a \u2260 nil.\n\nUnfortunately, rw is too limited to take care of this automatically. Work-arounds are, as you say\n\nuse simp_rw, which is able to generalize goals even in these cases.\nBut has it\u2019s own shortcomings \u2013 you cannot rewrite with equalities like l = head l @ tail l that mention the LHS on the RHS. This is because simp_rw is based on simp, and as a simplification rule, this would be bad.\n\nuse simp only [hx] {single_pass := tt}, which is similar to simp_rw, but doesn\u2019t complain about such looping rewrites.\n\nIf the equality has just a variable on one of its side (as it does here), then\nsubst hx\n\nworks nicely.\n\nIf the dependent argument that gets in the way is a local hypothesis (such as the hx here), it can help to move it into the goal;\nrevert h1, rw hx, intro h1,\n\nworks here, and may work in your case. But it may not always be possible to revert all relevant hypotheses.\n\nIf none of these help, you can try to extract a helper lemma where the thing you want to replace is a variable, use subst to prove that lemma quite directly, and then use that lemma.", "meta": {"post_id": 1062, "input_score": 16, "output_score": 15, "post_title": "What does the \"motive is not type correct\" error mean in Lean?"}}
{"input": "I was looking at the MSC2020 database and I find it hard to identify a field suitable for works about proof assistants and formalised mathematics. 03B70 (\"Logic in Computer Science\") might be the closest one, but it does not look optimal to me. My questions are\n\nDo you think a better subject class exists?\nIf we end up agreeing that there is none (and that one might be needed), is there a way to propose this to the AMS?", "output": "TL;DR: Based on the descriptions in the MSC2020 alone, I would use the new section 68Vxx, specifically the subject classification 68V20 (or maybe 68V15) if one is formalizing mathematics in a proof assistant.  (However, 68 is CS, so if one is concerned that their work be recognized as pure mathematics, then maybe it shouldn't be the primary classification of the work.)\n\nThere appears to be a new section in the 2020 MSC Subject Classification:\n\n68Vxx Computer science support for mathematical research and practice\n\n68V05 Computer assisted proofs of proofs-by exhaustion type {For rigorous numerics, see 65Gxx; for proofs employing automated or interactive theorem provers, see 68V15}\n68V15 Theorem proving (automated and interactive theorem provers, deduction, resolution, etc.) [See also 03B35]\n68V20 Formalization of mathematics in connection with\ntheorem provers [See also 03B35, 68V15]\n68V25 Presentation and content markup for mathematics\n68V30 Mathematical knowledge management\n68V35 Digital mathematics libraries and repositories\n68V99 None of the above, but in this section\n\n\n\nNote some of the above subject classifications also cross reference 03B35, but this classification (from 2010) seems quite vague:\n\n03B35 Mechanization of proofs and logical operations [See also 68V15]\n\nEven, 03B70, the subject classification you mentioned (which seems to be the de facto standard in the small, biased selection  of arXiv papers I've looked at) also cross references 68-XX:\n\n03B70 Logic in computer science [See also 68-XX]\n\nLast, the computer science section 68-XX also mentions the availability of the classifications -04 for \"papers containing software, source code, etc. in a specific mathematical area\", e.g.\n\n14-04 Software, source code, etc. for problems pertaining to algebraic geometry\n\n\nThere is a small discussion on the Lean Zulip about this new section.\n\nHere are papers on arXiv using 68V20 and 68V15.\n\nI'd also love to get an answer from someone involved in the creation of 68Vxx or from anyone who feels that 68Vxx doesn't work for them.", "meta": {"post_id": 1071, "input_score": 16, "output_score": 14, "post_title": "Proof assistants and formalised mathematics in the MSC database"}}
{"input": "I keep seeing these phrasing in some proof assistants/elaborators and their issues/internal discussions (e.g. Github search results in cooltt), that seems not that related to the actual proofs/programs. So I guess it is some implementation/semantic specific stuff and \"fibration\" seems related to Pi types, and \"cofibration\" to intervals?\nI'm also curious about their intuitions/origins. It's always cool but kinda confusing to see these categorical constructs in the implementation for some naive code readers like me.", "output": "Let me provide a slightly watered up supplement of Gratzer's answer.\nIn the beginning, mathematicians need to study spaces by considering how other spaces cover(*) them. For example, if you have a line and a circle, you can wind up that line around the circle completely. In mathematical terms, you are given $\\mathbb R$ and $\\mathbb S = \\{z\\in\\mathbb C: |z|=1\\}$. You can wind it up with $x\\mapsto e^{ix}$. This is good because locally, i.e. in a small area $U$ around each point, this map looks like the projection function $\\mathbb Z \\times U \\xrightarrow{\\pi_2}U$.\nI'll steal a picture here:\n\nNote that if you restrict your attention on a small region around one of the black arrows (the space \"above\" a point in $\\mathbb S$), it just looks like $\\mathbb Z$ copies of the same thing overlaid on that region.\nNow, what's good about it? Generally, we want to study the geometry of the base space $\\mathbb S$ by \"projecting\" down from the total space $\\mathbb R$. So we need the covering to be relatively well-behaved. For instance, when we are studying paths, we may want to require the property that, no matter how I walk along the base space, you can follow along in the total space, while keeping yourself strictly above me. As an example (lots of examples!), if I start at $\\color{blue}1$, and you start at $\\color{red}{-1}$; when I go counterclockwise along $\\color{blue}{1 \\to i \\to -1 \\to 1}$, you must follow along. And you can: $\\color{red}{-1\\to-\\frac34\\to-\\frac12\\to0}$.\nActually, a notion that came earlier is fiber bundles, which puts a stricter requirement: If you look at a small enough region, then the whole thing just looks like $F \\times B \\to B$. This is slightly easier to work with, but it is too restrictive in our use cases.\nTo formalize the notion of fibration, we can use a continuous function $[0,1]\\to B$ to represent a path in $B$. Also, we have a point $*\\xrightarrow0 [0, 1]$. (Using an arrow from a singleton space to another space is a common category speak to \"pinpoint\" a point in space; Here we want to get the point $0$. Also, I use $*$ instead of the categorical $1$, because it clashes with the real number $1$ here.) We start with a path $q:[0, 1]\\to B$, and a \"covering\" $p:E\\to B$ (our picture above is a covering $\\mathbb R \\to \\mathbb S$). We also specify our starting point in $E$ with an arrow $*\\to E$. This starting point must lie above our starting point of the path $q$ in the base space. So it can be written in a commutative diagram:\n$$\n\\begin{matrix}\n*&\\to&E\\\\\n\\downarrow & & \\downarrow\\\\\n[0, 1]&\\color{blue}{\\xrightarrow q} & B\n\\end{matrix}\n$$\nBy saying that \"you can follow along the path $q$ in $E$\", we really mean that you can lift the path into $\\hat q : [0, 1]\\to E$, and the commutativity of the diagram ensures that this lifted path lies \"above\" the original one. So:\n$$\n\\begin{matrix}\n*&\\to&E\\\\\n\\downarrow &\\color{red}{\\nearrow} & \\downarrow\\\\\n[0, 1]& \\color{blue}{\\to}& B\n\\end{matrix}\n$$\nIn this case, we say that the arrow $p$ lifts against the arrow $*\\xrightarrow 0 [0,1]$. But of course, we may\nwish to use other arrows than $0$. So in general, if $p$ lifts against a certain set of arrows,  we call $p$ a fibration.\nOn the other hand, we can consider the dual notion. Since fibrations are really good coverings, by reversing the arrows, we get that cofibrations are really good inclusions.\nWe invert our diagram:\n$$\n\\begin{matrix}\nY&\\leftarrow&B\\\\\n\\uparrow &\\color{red}{\\swarrow} & \\uparrow\\\\\nX& \\color{blue}{\\leftarrow}& A\n\\end{matrix}\n$$\nHere $X \\to Y$ is some other given arrow which we want to \"colift\" against. This is called an extension property.\nThe archetypical example is when $X \\to Y$ is $S^{[0,1]} \\xrightarrow {p_0} S$, which maps each path in the space of paths in $S$ to its starting point. Suppose $A = *$, and $* \\to B$ is a point of $B$, then the commutative square gives\n$$\n\\begin{matrix}\nS&\\leftarrow&B\\\\\n\\uparrow &\\color{red}{\\swarrow} & \\uparrow\\\\\nS^{[0,1]}& \\color{blue}{\\leftarrow}& *\n\\end{matrix}\n$$\n\nA map $\\color{blue}{H : * \\to S^{[0,1]}}$, which is equivalent to a path $\\color{blue}{[0,1] \\to S}$.\nA map $B \\to S$, mapping $B$ inside $S$.\n\nThe extension property gives the diagonal arrow $\\color{red}{B \\to S^{[0,1]}} \\cong B \\times [0,1] \\to S$, which describes how $B$ \"moves\" continuously in $S$. $\\color{blue}H$ gives the trajectory of a single point of $B$. So the extension property roughly says that, whenever you have the starting position of $B$, and describe how a single point moves inside $S$, then you can always \"drag $B$ along that trajectory\". Another way to put it is you can always extend the orbit of the point to the orbit of the whole of $B$.\nOf course, extension of a point sounds trivial. But we can immediately create another example: If you replace $A = *$ with $A = 2$, the space with two points, then the extension property says that given the trajectory of two points, you can always extend the trajectory to the whole of $B$.\nThis is very useful in type theory. Suppose you have proved three equalities:\n$$\n\\begin{matrix}\na&&d\\\\\n\\|& & \\|\\\\\nb& =& c\n\\end{matrix}\n$$\nIn cubical type theory, this is represented as three paths, similar to $[0,1] \\to X$. To compose them, you invoke a version of the extension property: Let $B$ be the path $b=c$, and you have the trajectory of the two endpoints $b$ and $c$: they move to $a$ and $d$, respectively. Now \"dragging $B$ along\", you will end up with a path $a = d$. This is how path composition is proved (**).\n(*): Covering spaces is a related term, but I don't want to talk too much about that. I'm using this word to represent intuition.\n(**): In some flavors of CuTT, at least. There are type theories that try to achieve this with different methods.", "meta": {"post_id": 1073, "input_score": 17, "output_score": 9, "post_title": "What are \"fibration/cofibration\" in type theory and what are their intuitions?"}}
{"input": "I have often seen the claim that in a classical setting, well-foundedness of a relation > defined as the absence of an infinite descent x_0 > x_1 > x_2 > ... is equivalent to the constructive definition using an accessibility predicate as in Coq's Acc:\nInductive Acc (x: A) : Prop :=\n  Acc_intro : (forall y:A, R y x -> Acc y) -> Acc x.\n\nDefinition well_founded := forall a:A, Acc a.\n\nwhere R : A -> A -> Prop.\nIt seems it requires some version of choice but I don't know which is weakest one has to assume to construct such a proof.", "output": "This is not a Coq proof, but it's a proof sketch using the axiom of dependent choice and LEM to exhibit an equivalence. I believe this is the weakest choice principle you can get away with, but I cannot find a reference. It's too long for a comment unfortunately.\nEdit: it's no longer just a paper proof! @kyo dralliam has formalized the below argument in Coq and gone further to show that the choice additional axioms (dependent choice, LEM) are also necessary as well as sufficient.\n\nLet's define $\\mathsf{idc}(A,R,a)$ to be the proposition which states that there is an infinite descending chain in $(A,R)$ starting with $a$:\n$$\n\\mathsf{idc}(A,R,a) = \\exists f : \\mathbb{N} \\to A.\\ f(0) = a \\land (\\forall n.\\ f(n) \\mathrel{R} f(n + 1))\n$$\nWe will show that $\\neg \\mathsf{idc}(A,R,a) \\iff \\mathsf{Acc}(A,R,a)$. The $\\Leftarrow$ direction follows by induction on $\\mathsf{Acc}$ and requires no non-constructive principles. The inductive step can be summarized as \"if $\\mathsf{idc}(A,R,a)$ holds, there is some $b$ such that $a \\mathrel{R} b$ and $\\mathsf{idc}(A,R,b)$ holds.\"\nFor the other direction, let's use LEM and assume $\\neg \\mathsf{Acc}(A,R,a)$ to derive a contradiction. We will achieve this contradiction by constructing an infinite descending chain and this is where we will use the axiom of dependent choice.\nTo that end, consider the type $B(a) = \\{x : A \\mid a \\mathrel{R^*} x \\land \\neg \\mathsf{Acc}(A,R,x)\\}$ and write $S$ for the restriction of $R$ to this type. Let's prove that $S$ is entire, that is if $b : B(a)$ then there exists some $c$ such that $b \\mathrel{S} c$. I've deferred this lemma to the bottom of the answer, to not clutter up the proof. Since $(B(a),S)$ is entire and non-empty with $a : B$ by assumption, the axiom of dependent choice gives us an infinite descending chain within $B(a)$: $b_0 \\mathrel{S} b_1 \\mathrel{S} b_2 \\mathrel{S} b_3 \\dots$. By definition of $S$, this induces a chain $b_0 \\mathrel{R} b_1 \\mathrel{R} b_2 \\dots$. Moreover, by definition of $B(a)$ we can extend this chain so that it starts with $a$. We now derive our contradiction from $\\neg\\mathsf{idc}(A,R,a)$.\n\nLemma. $(B(a),S)$ is entire.\nProof. Assume we are given $b : B(a)$. By assumption, $\\neg \\mathsf{Acc}(A,R,b)$. Unfolding this, we see that it's equivalent to the following:\n$$\n\\neg (\\forall b. a \\mathrel{R} b \\to \\mathsf{Acc}(A,R,b))\n$$\nBy LEM again, we obtain $c : A$ such that $a \\mathrel{R} c$ and $\\neg \\mathsf{Acc}(A,R,c)$. Therefore, $c : B(a)$ and $b \\mathrel{S} c$ as required.", "meta": {"post_id": 1077, "input_score": 16, "output_score": 16, "post_title": "Well-foundedness: classical equivalence of no infinite descent and accessibility"}}
{"input": "I am new to the categorical semantics for dependent type theories, so it is surprising for me to see nLab introduces so many variants of categorical models, including comprehension categories, display map categories, category with attributes, category with families, natural models and contextual categories.\nI wonder what are the motivations for these different models? For example, IMHO, I think category with families is one of them that is quite close to the syntax and developed quite early, I am curious what are the disadvantages of CwF that lead to the development of other notions?", "output": "I would divide these models into three general groups.\n\nStructures that are more \"categorical\", arising naturally from categories \"in nature\" without the need for strictification theorems.  In particular, in these models substitution/pullback is only functorial up to isomorphism.  These include display map categories and comprehension categories.  The variations among this group arise in part from how much of the rest of the structure of type theory one wants to retain, e.g. whether a dependent type is treated as a separate notion, or is identified with its projection map, and whether that projection map is equipped with a structure or just a property.\n\nStructures that faithfully represent the entire syntax of type theory, with strict substitution, and including the fact that contexts are inductively generated as lists of types.  These are contextual categories, a.k.a. C-systems; there's really only one notion here, although it can be named and defined in different-looking ways.\n\nStructures that faithfully represent the syntax of type theory, with strict substitution, but with a separate class of things called \"contexts\" that aren't necessarily inductively generated as lists of types.  These are categories with families, categories with attributes, and natural models, which are also really just equivalent ways to define the same thing, so I would say there's really only one notion here too.\n\n\nArguably the difference between the last two points is just a difference in the type theory they correspond to: in case (3) the type theory has a judgment $\\Gamma \\,\\mathsf{ctx}$ with rules like\n$$\\frac{\\Gamma\\,\\mathsf{ctx} \\qquad \\Gamma \\vdash A\\,\\mathsf{type}}{\\Gamma,x:A \\,\\mathsf{ctx}}$$\nwhile in case (2) there is no such judgment, and an ordinary rule like\n$$\\frac{\\Gamma\\vdash a:A \\qquad \\Gamma \\vdash b:B}{\\Gamma \\vdash (a,b):A\\times B}$$\nis actually regarded as a schema of rules parametrized by an external natural number length of context:\n$$\\frac{\\cdot\\vdash a:A \\qquad \\cdot \\vdash b:B}{\\cdot \\vdash (a,b):A\\times B}\n\\qquad\n\\frac{x:X\\vdash a:A \\qquad x:X \\vdash b:B}{x:X \\vdash (a,b):A\\times B}\n\\qquad\n\\frac{x:X,y:Y\\vdash a:A \\qquad x:X,y:Y \\vdash b:B}{x:X,y:Y \\vdash (a,b):A\\times B}\n\\qquad\\cdots$$\nHowever, even when using type theories without a context-judgment, structures of type (3) occupy an intermediate stage between those of type (1) and type (2).  Namely, the way to make (1) into (2) is to first apply a strictification theorem to make it into (3), and then apply the \"contextual coreflection\" to make it into (2).  In particular, any type theory without a context judgment can be interpreted in a (3) structure via this contextual coreflection.", "meta": {"post_id": 1086, "input_score": 18, "output_score": 17, "post_title": "What are the motivations for different variants of categorical models of dependent type?"}}
{"input": "What is bidirectional type checking and why would I want to implement it?\nIt feels like the name is a bit of a misnomer and the syntactic separation kind of resembles stuff like ANF, call by push value or CPS?", "output": "Bidirectional type checking is a popular technique for implementing type checking/inference algorithms. AFAIK it originated from the paper \"local type inference\".\nThe idea of bidirectional type checking follows from the following observation: there are two ways to typecheck a program, type checking, where type information is provided externally, and the program checks if some term matches the type, and type inference, where type information is missing, and the algorithm must deduce it from only the term itself.\nObviously, inference is much harder than checking. In fact, in many complex type systems only the latter is decidable. However, requiring type annotation everywhere is very awkward. And here comes bidirectional type checking: its ultimate goal is to propagate type information/annotation through the syntactic structure of terms.\nThere are two modes in bidirectional type checking, synthesis and checking. synthesis corresponds to type inference, and no external information is available. So an annotation is usually needed here. On the other hand, checking mode, as its name indicates, receives a type from the outside world and performs type checking, so usually no type annotation is needed.\nAt nodes like function application f(t), where the subterms' types are related, bidirectional type checking can propogate type information from one to the other, reducing the number of annotations needed. For example, if f synthesis to a type A -> B, then t can be checked with A.\nYou mentioned CPS/ANF/CBPV. IMO they are not related to local type inference. Local type inference shares with these transformations/NFs the same nature that it reveal certain ordering on terms. However, CPS/ANF reveal evaluation order, while what bidirectional type inference reveals is how type information flows, which is something different.", "meta": {"post_id": 1090, "input_score": 18, "output_score": 23, "post_title": "What is bidirectional type checking?"}}
{"input": "Agda has a reflection mechanism (not equality reflection or reflexivity, but something related to metaprogramming based on goals and contexts to generate terms) and people have developed some libraries that simulate Coq tactics, for example acata.\nHowever, some people still don't consider Agda to have tactics (for example in Kevin Buzzard's \"the future of mathematics\" talk, the Q&A section).\nOn the other hand, Coq and Lean are considered to \"have tactics\", but what I see is just another syntax for writing reflection code like in Agda. I think it's because I don't understand tactics in these languages good enough. So, what particular feature do they support so that they can be considered to \"have tactics\"? I have a clear understanding of what reflection is (like in Agda -- the compile time reflection with access to the type-checking monad and bidirectional type checking functions), but I am unsure what do people mean by \"tactics\".", "output": "Consider a reasonable type theory $T$ with decidable checking. Think of it as the core type theory implemented in the kernel of a proof assistant, i.e., with fully elaborated and annotated judgements that nobody wants to write with bare hands.\nA central task of formalization is inhabitation of a type: given a derivable judgement $\\Gamma \\vdash A \\; \\mathsf{type}$ (the goal), the user provides an expression $e$ (the solution), and the kernel checks whether $\\Gamma \\vdash e : A$ is derivable. How can we do this in a practical way?\nIt is not practical for the user to write down a fully annotated term $e$. Instead, they write down an expression $c$ in a surface language $V$ (the vernacular) which is elaborated or evaluated by the proof assistant to give a fully annotated expression $e$. In order to account for the possibility of a user error, the proof assistant can be seen as a map\n$$\\textstyle\nP : \\prod_{\\Gamma, A} \\, (\\Gamma \\vdash A \\; \\mathsf{type}) \\to V \\to \\{\\mathsf{error}\\} + \\sum_{e} \\, (\\Gamma \\vdash e : A).$$\nWe read the above as: given a derivable type $A$ in context $\\Gamma$ (the goal) and an expression $c$ of type $V$ (the suggested solution), the proof assistant either recognizes that $c$ evaluates to an expression $e$ such that $\\Gamma \\vdash e : A$, or it reports an error.\nActually, apart from errors, there could also be other computational effects, such as non-termination, state, user interaction, etc. We thus generalize the above to a suitable computational monad $M$:\n$$\\textstyle\nP : \\prod_{\\Gamma, A} \\, (\\Gamma \\vdash A \\; \\mathsf{type}) \\to V \\to M(\\sum_{e} \\, (\\Gamma \\vdash e : A)).$$\nWith the general setup explained, we can address the question. What kind of proof assistant one gets depends on the design of the vernacular $V$ and the computational monad $M$. Some possibilities are:\n\nAgda: to give the user the impression that they are working with $T$ directly, we design the vernacular $V$ to look like an abridged version of $T$ and call the expressions of $V$ (proof) terms. The computational monad $M$ supports interactive construction of terms through \u201dholes\u201c that represent subgoals. (Agda also support meta-programming through which the user can implement proof search and other techniques that people call \u201ctactics\u201d.)\n\nCoq and Lean: make the vernacular $V$ look like a command-based programming language. The commands are called tactics. The monad $M$ incorporates the current goal as state that is manipulated by the user through tactics. The command-based language allows general recursion and construction of non-terminating programs.\n\nIsabelle/HOL: make the vernacular $V$ look like a meta-level programming language. There is a built-in abstract data type of judgements, which is controlled by a trusted kernel. The user writes programs that evaluate to judgements. There is a library of useful functions for constructing judgements, called tactics. The user is free to implement their own functions, as well as Tetris.\n\n\nReflection is the ability for the vernacular $V$ to access the abstract syntax trees of computed judgements. Again, there is more than one way of doing this:\n\nAgda reflects judgements into a datatype of abstract syntax trees (this is also known as reification)\nCoq has matching on goals and hypotheses\nLean metaprogramming also allows one to decompose judgements\nIsabelle/HOL has reflection too, though the abstract data type of judgements. In addition, there is also a mechanism for matching on goals and hypotheses similar to the one for Coq.\n\n(If someone can provide better descriptions and links to reflection for Lean and Isabelle, that would be grand.)\nReflection is not to be confused with Boolean reflection, which is a proof technique for (ab)using the equality checker as a decision procedure.  Please ask a separate question if you'd like to know more.", "meta": {"post_id": 1103, "input_score": 18, "output_score": 18, "post_title": "What's the difference between reflection and tactics?"}}
{"input": "Is there a library (for any proof assistant) which provides formalized definitions of unsolved problems? To clarify, I mean some collection that correctly defines unsolved problems in the language of a proof assistant, without attempting to provide any solutions.\nBy unsolved problems, I mean one which haven't been solved by any human mathematician (like the Riemann hypothesis) rather than just ones which have been proven but not formalized in a proof assistant.", "output": "The closest to what you are asking for might be the Formal abstracts project whose aim is to formalize the statements (but no proofs) of results from papers.\nFormalizing statements without proofs is not as trivial as one might think at first. Some challenges are:\n\nThe statement of the proof might require formalization of a complicated constructions. For instance, consider a theorem about cohomology \u2013 presumably we need to define cohomology before we can state it.\n\nWithout proofs and without any applications of the theorem there is no sanity check as to whether the statement is formalized correctly. There is real danger of mistakes going undetected.\n\nNot only should one formalize the statements itself, but also keep track of provenance, i.e., information of where in the non-formalized world the theorem originated.\n\n\nSimilar challenges will present themselves with a library of open problems.", "meta": {"post_id": 1125, "input_score": 16, "output_score": 10, "post_title": "Formalizations of unsolved problems"}}
{"input": "I'm reading Girard's Proofs and Types, and in section 4.4 he writes:\n\nLemma: t is strongly normalisable iff there is a number \u03bd(t) which bounds the\nlength of every normalisation sequence beginning with t.\n\nHe later uses this lemma in his proof that the STLC is strongly normalizing in section 6.2.3:\n\n(CR 3) Let t be neutral and suppose all the t' one step from t are\nreducible. Let u\nbe a reducible term of type U; we want to show that t u is reducible. By\ninduction hypothesis (CR 1) for U, we know that u is strongly normalisable;\nso we can reason by induction on \u03bd(u).\n\nThe lemma uses a version of Konig's Lemma which IIUC is not provable constructively.  Is there a constructive proof of strong normalization for the simply typed lambda calculus?  Specifically something I can (try to) translate into Agda?", "output": "As Dan Doel says in the comments, the reason K\u0151nig's lemma is used is that Girard's definition of strong normalization is constructively too weak. It's defined as the non-existence of infinite reduction sequences.\nWe can use the following definition instead: a term is strongly normalizing if every one-step reduct of it is strongly normalizing (inductively). In Agda:\ndata SN {\u0393 A} (t : Tm \u0393 A) : Set where\n  sn : (\u2200 {t'} \u2192 t ~> t' \u2192 SN t') \u2192 SN t\n\nNote that if a term is normal then it has no one-step reduct so SN t holds vacuously.\nThis is also called accessibility of t with respect to ~>. I believe it originates from Aczel's An Introduction to Inductive Definitions. This is a common SN definition in formalizations. The oldest formalized example that I know is in Altenkirch's thesis.\nThe advantage of SN t is that we can do induction on it. Operationally, we can make a \"recursive call\" on any one-step reduct of a strongly normalizing term. You could also look at my Agda formalization of Girard's Chapter 6, which uses this SN.", "meta": {"post_id": 1135, "input_score": 10, "output_score": 17, "post_title": "Constructive proof of strong normalization for simply typed lambda calculus"}}
{"input": "Andrea Vezzosi said Cubical Agda made cubical subtyping coercion-based (inS and outS) because it's a bad idea to introduce more subtyping relations which complicates the system. Also Agda did not have cumulative universes (it is added now).\nSo does Agda support any subtyping relations? Without using rewriting and cumulativity options of course.", "output": "The current subtyping rules that Agda uses are for sized types (when --sized-types is enabled) and cumulativity (when --cumulativity is enabled). Currently neither of these options are allowed under the --safe flag, so with --safe there is no subtyping in Agda.\nThere used to be subtyping rules for irrelevant and erased function types as well, but these were removed recently (see This successful Pull Request which was merged into Master on GitHub) because they were causing performance problems and anyway only save the user a simple eta-expansion.", "meta": {"post_id": 1148, "input_score": 5, "output_score": 10, "post_title": "What subtyping rules does Agda support?"}}
{"input": "Ulf Norell wrote this in his PhD thesis (figure 1.6):\n\nThis contradicts my stereotype on pi & sigma types, where pi parameter should be contravariant and sigma parameter is covariant. Why is Agda designed this way? Is it actually implemented this way?\nP.S. I have two guesses why pi has invariant parameter:\n\nThat contravariant parameter makes the type theory complex, because you keep track of the actual type and the type in the telescope -- two types -- of a binding. Without contravariant pi parameter we can think of them as the same thing.\nAccording to Conor McBride, it's Zhaohui Luo's Extended CoC that made this design choice for a simpler set-theoretical semantics (that subtyping is modelled by subsetting).\n\nWhich of these is true? Or are they both unrelated to the Agda design choice?", "output": "There are subtleties here, when type annotations are present, depending in quite a brittle way on where they must be placed. (I'm half-remembering conversations about this with Zhaohui Luo.) Suppose we demand (as many do) type annotations on lambda, with a typing rule like this\nS : Type_i     x : S |- t : T\n-----------------------------\n\\ x : S -> t  :  (x : S) -> T\n\nIn particular, we may readily have\n\\ X : Type_1 -> X : (X : Type_1) -> Type_1\n\nNow, if cumulativity were contravariant in the input type, we should allow\n(X : Type_1) -> Type_1  <=  (X : Type_0) -> Type_1\n\nand, indeed, one might imagine that the identity function for Type_1 should work perfectly well for those of its inhabitants which happen also to live in Type_0. But there's a snag, if you want cumulativity to be, in a suitable sense, reducible to universe inclusion. The typing rule for lambda will only ever give\n\\ X : Type_1 -> X\n\nfunction types with the domain Type_1, because that is what the domain type annotation says, and we're stuck with it. There is a function that does thejob we want, but it's\n\\ X : Type_0 -> X\n\nand that ain't the function we first thought of.\nNow, if you're me, you arrange never to synthesize the types of lambda abstractions, only to check them, removing the need for the type annotation. Then, both Type_0 -> Type_1 and Type_1 -> Type_1 will accept \\ X -> X. In the bidirectional setting, the \"change of direction\" rule says that to check that T accepts a term whose synthesized type is S, then an S thing should be able to do all a T thing's jobs. That clear directedness yields great temptation to relax from equality to a subsumptive notion of subtyping, and in that setting contravariance in function domains is not a problem.", "meta": {"post_id": 1152, "input_score": 9, "output_score": 8, "post_title": "What's the benefit of having pi and sigma types with an invariant parameter?"}}
{"input": "Fifty years ago, few would have imagined that the process of verifying the correctness of a known proof of a mathematical theorem might be so costly that the mathematical community would hesitate to do it more than once. Of course, nowadays, there is a growing list of theorems whose proofs require an enormous amount of computation.  In many cases, these theorems amount to calculating the first few values of an infinite sequence of values, and maybe not many people would care to have those computations formally verified.  In other cases, however, there is an attractive theorem statement that many might want to see a formal proof of. The Kepler conjecture is an obvious example; perhaps less well known to the proof-assistant community is the ternary Goldbach problem, solved by Harald Helfgott, who reduced the problem to a feasible finite computation.  A complete proof therefore requires doing this computation; to give some idea of how big this computation is, let me quote a paragraph from Helfgott's preprint:\n\nIn December 2013, I reduced $C$ to $10^{27}$. The verification of the ternary Goldbach conjecture up to $n \\le 10^{27}$ can be done on a home computer over a weekend,\nas of the time of writing (2014). It must be said that this uses the verification of the\nbinary Goldbach conjecture for $n \\le 4 \\cdot 10^{18}$ [OeSHP14], which itself required computational resources far outside the home-computing range. Checking the conjecture\nup to $n \\le 10^{27}$ was not even the main computational task that needed to be accomplished to establish the Main Theorem\u2014that task was the finite verification of zeros of $L$-functions in [Plab], a general-purpose computation that should be useful elsewhere.\n\nProofs like these seem to pose a significant knowledge management problem if we envisage a future in which the majority of interesting mathematical results are archived in some machine-checkable library. It seems that if we want to check the proof of a new theorem that relies on an existing theorem whose proof is extremely expensive, then we are faced with the choice of either redoing the expensive computation, or taking it on faith that someone else checked it and that we haven't made a bookkeeping error somewhere along the way.  If there are only a few expensive theorems then maybe there isn't a problem, but if expensive theorems proliferate, and we want to rely on our records of whether they were checked or not, then it seems that the chances of mistakenly thinking that something was proved (when it really wasn't) start to become non-negligible.  This is especially true if we have software \"upgrades\" every couple of years that could introduce bugs, or at least confusion over the exact meanings of various terms and theorems.\nWith the above remarks as background, I have a twofold question.\n\nDo existing systems have a plan for managing this kind of issue? I see that there is another question about recompilation avoidance that seems related, but doesn't seem to be quite the same.\n\nTypically, how much computational overhead is there when we take a conventional computation and redo it in a proof assistant?  (Here, I'm not talking about the human effort in doing the re-programming, although I recognize that that is also a major issue.) The answer is probably, \"It depends,\" but I'm wondering if there is any rule of thumb. In this regard, I found the Ph.D. thesis of Muhammad Taimoor Khan, Formal Specification and Verification of Computer Algebra Software, to be very interesting. For certain types of computation in Maple, one can apparently answer this question fairly easily by running Khan's software.  Are there other packages available that allow someone with conventional programming experience but little experience with proof assistants to write \"normal-looking code\" (or something close to it) that carries out a formally verified long computation, and thereby time how much longer it takes than conventional code does?", "output": "Not as general as Andrej's answer but more a data-point for your interest. We've been looking at how to construct formal proofs about neural network-enhanced systems in proof assistants. This runs into exactly the problem you describe: the act of verifying the network can be very expensive (hours/days) which disrupts the interactive component of the proof assistant.\nAs described in Figure 1 in our recent paper, we've taken the external book-keeping approach, i.e. create a cache which stores the result of the verification of the network. The proof assistant then checks the cache instead of reverifying the network each time. The cache maintains consistency by also storing the location and the hash of the network, which can be used to detect when the proof has become invalid (for example due to further training of the network).", "meta": {"post_id": 1161, "input_score": 31, "output_score": 8, "post_title": "What work has been done on computationally intensive proofs?"}}
{"input": "I wrote some code that used a [inhabited \u03b9] assumption, and then used arbitrary \u03b9 in the proof. The mathlib linter then complained\nThe following `inhabited` instances should be `nonempty`. argument 2: [_inst_3 : inhabited \u03b9]\n\nI can change the types as suggested, but what do I do with the arbitrary \u03b9 expression in my proofs?", "output": "What you're seeing here is the behavior of the inhabited_nonempty linter:\nimport tactic\n\nlemma foo {\u03b1} [inhabited \u03b1] : \u2203 x : \u03b1, x = x :=\nbegin\n  exact \u27e8default, rfl\u27e9\nend\n\n#lint only inhabited_nonempty\n/- Checking 1 declarations (plus 1 automatically generated ones) in the current file with 1 linters -/\n\n/- The `inhabited_nonempty` linter reports: -/\n/- USES OF `inhabited` SHOULD BE REPLACED WITH `nonempty`. -/\n\nYou can use the inhabit tactic to solve this problem\nimport tactic\n\nlemma foo {\u03b1} [nonempty \u03b1] : \u2203 x : \u03b1, x = x :=\nbegin\n  inhabit \u03b1,\n  exact \u27e8default, rfl\u27e9\nend\n\n#lint only inhabited_nonempty  -- All linting checks passed!\n\nNote this has the advantage over classical.arbitrary of not introducing the axiom of choice:\n#print axioms foo  -- no axioms\n\nwhile being less cumbersome than the nonempty.elim \u2039nonempty \u03b1\u203a $ \u03bb a, suggested in the other answer.", "meta": {"post_id": 1168, "input_score": 3, "output_score": 10, "post_title": "Like arbitrary, but for nonempty instead of inhabited"}}
{"input": "This document showed that Lean's impredicative universe of strict propositions breaks normalization (of proofs) in a way that canonicity and logical consistency are unaffected, because the counterexample lives in Prop. I wonder how much of trouble it will be to have this counterexample, like both syntactically (in terms of Lean type checking) and semantically (in terms of Lean metatheory)?\nTo me, it seems that you simply need to prevent reduction for terms in Prop and assume them to be equal. This will at least recover the termination of type checking (well, at least refute this particular counterexample of termination of type checking). This can solve the obvious trouble pointed out by the paper.\nPierre-Marie P\u00e9drot said that:\n\nImpredicative SProp breaks SN only when you can eliminate the SProp equality into a non-SProp sort. Otherwise, it's fine.\n\nI'm also unsure about that. I think it is established that equalities in SProp cannot be eliminated into non-SProp (like boolean in SProp has true = false, but if we can eliminate this into non-SProp booleans, we get logical inconsistency), so I assumed this is disallowed in Lean, but still the paper I linked in the beginning claimed that normalization is broken by their counterexample.", "output": "To address a most important point, as suggested by Andrej Bauer, Lean's intended model is one where types are sets.  Mario Carneiro showed in his master's thesis that Lean has such a set-theoretic model (specifically using ZFC plus infinitely many large cardinals).  So that means Lean is consistent (assuming no bugs in its implementation), and a theorem in Lean means what a classical mathematician would think it means (once they interpret types as sets and understand universes).\nThe failure of normalization however means that one can't give a more computational model of Lean, which isn't a large deal since Lean is mostly used as a classical mathematics proof assistant.  Also, it should be pointed out that Lean's reduction (in Lean 3 at least) is painfully slow anyway.  It can barely add numbers with #reduce, so non-termination and slow termination are basically the same in the eyes of the user.  This is discussed in this Zulip thread.\nA related issue which you also address, comes down to elimination outside of Lean's Prop.  Unlike your expectation, Lean does have a limited form of large elimination called (syntactic) subsingleton elimination.\nAs a quick aside, my understanding is that there are two interpretations of definitional equality in Lean.  One is what is actually implemented in Lean.  Mario's thesis has an approximation of that which is decidable, but it breaks transitivity.  Broken defeq transitivity implies broken subject reduction.  The other definitional equality which appears in Mario's thesis is a more ideal one.  It satisfies transitivity and subject reduction, but it is undecidable.\nThe reason for this failure of subject reduction in Lean's implemented type theory, to my understanding, is that Lean has proof irrelevance (similar to Coq's SProp) and a syntactic form of subsingleton elimination as described in this document.  In particular, the latter lets you eliminate \"syntactic subsingletons\" such as true, false, and, =, and acc (which is related to well-founded) to types in Type.  These are called \"syntactic subsingletons\" since they have at most one type constructor.  As Pierre-Marie P\u00e9drot pointed out, this leads to the issues with definitional equality that I mention above.  On Zulip, Mario gave a MWE of how to use proof irrelevance of Prop and the syntactic subsingleton elimination of acc to explicitly break transitivity of defeq, break subject reduction, and fool Lean tactics into producing false terms (but which the kernel still correctly rejects).\nThese issues are relatively small for Lean users.  Again, they don't break consistency or change the set-theoretic model, just make some things more annoying for users.  Lean users rarely notice these issues are there (and likely couldn't point to them at all), but they do occasionally come up as discussed here.  Mario, however likes to point out that it is possible to run into the same issues even in a system without explicitly broken definitional equality.  In particular, it is easy to construct examples where a = b and b = c are fast to compute definitionally, but a definitional proof of a = c is quite slow and will time out.  While not technically a failure of definitional equality, it is the same sort of issue in practice.  One must resort to an alternate proof of a = c in some cases.\n(Also, it should be pointed out that even Coq has some issues with subject reduction.  I think Mario would say the Coq issues are similar in scope to those Lean in that they aren't a large issue, but should be watched out for.  But I think Coq users would counter that they have a plan to fix SR in Coq whereas it is an accepted trait of Lean.  I also have the sense that the SR issues in Coq are more compartmentalized to certain Coq features.)\nNote: I got a lot of my information from this thread on the Lean Zulip.\nThe big question of course now is why does Lean have proof irrelevance and syntactic subsingleton elimination if they knowingly lead to these (small) issues?  And what big benefits does proof irrelevance and syntactic subsingleton elimination provide to Lean users?  I'm trying to figure that out, and then I'll update this answer when I do.", "meta": {"post_id": 1183, "input_score": 16, "output_score": 16, "post_title": "How much of trouble is Lean's failure of normalization, given that logical consistency is not obviously broken?"}}
{"input": "I read this CSTheory SE post, which suggests that it is often not clear what variant of MLTT or CIC is being referred to. But I would like to know the proof-theoretic strengths of the various underlying foundational systems for more 'constructive' proof assistants such as Agda, Coq, Lean. For example, this paper gives the proof-theoretic strength of MLTT[1984]. But Coq is not based on MLTT[1984], but on some (evolving?) variant of CIC. This introduction seems to mix both type theory and Coq, and omits details \"for the sake of simplicity\", so I don't really know what is the whole system...\nCan anyone present a precise table comparing the proof-theoretic strengths of the most common systems underlying Agda, Coq, Lean? In particular, I am interested in what happens when features that might be considered impredicative are included or excluded. So far I have seen two, one being impredicative types (e.g. impredicative Prop) and the other being impredicative inductive/recursive constructions. I am also interested in what happens when LEM is included or excluded, because it seems to me that we are unable to provide non-circular (ontological) justification for having both impredicative Prop and LEM. (Related.)\nFor instance, what is the strength of MLTT[1984]+LEM? Does it exceed that of ZBQC \u2245 MOST? What about CIC and CIC+LEM?", "output": "Here are a few comparisons of dependent type theories with fragments of Peano arithmetic and set theories that I know of:\nPredicative systems (the MLTT family)\nPredicative systems are comparable with small fragments of second-order arithmetic. Their strength is measured by which inductive definitions are allowed:\n\nWhen there is only a type of natural numbers, we get the strength of ATR\u2080, in the sense that both theories prove the same \u03a0\u2070\u2082 statements about arithmetic (proved in the paper you mention). ATR\u2080 is a small fragment of PA2 which plays an important role in reverse mathematics.\nAdding W-types makes the system stronger than \u0394\u00b9\u2082-CA but weaker than \u03a0\u00b9\u2082-CA, which are slightly larger fragments of PA2.\nInduction-recursion is still a bit stronger, but is expected to be bounded above by KPM+ (see here) which is still weaker than \u03a0\u00b9\u2082-CA, I believe. This is more or less the theory of Agda.\n\nThe order can be refined by taking into account the number of predicative universes : MLTT\u2081 < MLTT\u2082 < ... < MLTT\u03c9 < MLTT\u2081 + W < ... MLTT\u03c9 + W < MLTT\u2081 + IR < ...\nA detailed table that also includes ordinal notations and references can be found on the nLab.\nExtensionality axioms such as function extensionality, proposition extensionality, UIP, univalence, equality reflection... do not add any logical strength to the system. The full law of excluded middle, however, is another story: now any type is either inhabited or not, which basically makes Bool an impredicative universe of propositions \u2014 and impredicative systems are much stronger. In fact, MLTT + LEM has the same strength as CIC + LEM.\nImpredicative systems (System F, CC, CIC)\nThe introduction of an impredicative universe adds a tremendous amount of logical strength to a system. We jump from small fragments of second-order arithmetic to systems which are at least as strong as the full PA2.\n\nSystem F has the strength of second-order arithmetic, in the sense that an integer function is provably total in PA2 if and only if it can be defined in system F (standard reference).\nCC, the calculus of constructions (which has an impredicative universe Prop which sits inside the predicative universe Type) has the strength of higher-order arithmetic.\nCC\u03c9, which adds a hierarchy of predicative universes, is stronger than Zermelo set theory (proved here), but weaker than ZF set theory (as witnessed by the normalization proof).\n\nNote that the strength comparison theorems are about definable functions, not provable arithmetic statements as before. Indeed, all of these systems have no inductive types at all, so even the integers need to be encoded with impredicativity. And while you can define very complex functions on these impredicative integers, predicates cannot tell them apart: you can't even prove 0\u22601. If we want to do such things, we need to add inductive types with large elimination, which results in CIC, the calculus of inductive constructions.\n\nIf we restrict large elimination in the impredicative universe to sub-singleton inductive types, then we can lift functions that are defined on impredicative integers to predicative, inductive integers while remaining compatible with classical set-theoretic semantics. This is more or less the theory of Coq.\nIf we allow large elimination for all inductive types in the impredicative universe, then the system becomes incompatible with classical logic.\nFinally, if you add LEM and a variant of the axiom of choice to CIC, then it becomes comparable with ZFC set theory (proved here)", "meta": {"post_id": 1201, "input_score": 21, "output_score": 26, "post_title": "Proof-theoretic comparison table?"}}
{"input": "Off and on I have heard of the jargon \"commuting conversion\" but I don't really know what it means.\nI've heard commuting conversions are problematic but I don't know why.", "output": "Syntactically, commuting conversions are part of the $\\eta$-rules for left-invertible types -- i.e., types which have pattern-matching eliminators. So if $e : A + B$, the $\\eta$-equation for it looks like:\n$$\n\\newcommand{\\c}[1]{\\mathsf{#1}}\nC[e] \\equiv \\c{case}(e, \\c{inl}\\,x \\to C[ \\c{inl}\\,x], \\c{inr}\\,y \\to C[ \\c{inr}\\,y]) \n$$\nThis rule says that if you have a term with an $e : A+B$ occuring in it somewhere (this is the $C[e]$), then it is the same as matching on $e$ and running $C[\\c{inl}\\, x]$ in the left branch and $C[\\c{inr}\\, y]$ in the right branch.\nThe algorithmic difficulty this poses is that this rules is extremely nondeterministic and non-syntax-directed -- if you have a term $t$, you can break it into a context $C$ and expression $e$ such that $C[e] = t$ in potentially many different ways. Moreover, since the context $C$ is duplicated in each branch of the case statement, the context in each can be independently transformed some more into terms which look radically different from one another.\n\nSemantically, commuting conversions arise from the fact that the sum type can be viewed as a coproduct. Since coproducts are a kind of colimit, they satisfy both an existence property, and a uniqueness property.\nThe existence property of coproducts says that for each $X$ and $Y$, we have a type $X + Y$ and injection maps $\\c{inl} : X \\to X + Y$ and $\\c{inr} : Y \\to X + Y$ such that for each $f : X \\to Z$ and $g : Y \\to Z$, we have a map $[f, g] : X + Y \\to Z$ such that $[f,g] \\circ \\c{inl} = f$ and  $[f,g] \\circ \\c{inr} = g$.\nYou can see that these two equations correspond to the $\\beta$-rules for sums:\n$$\n\\c{case}(\\c{inl}\\,z, \\c{inl}\\,x \\to e_1, \\c{inr}\\,y \\to e_2) \\equiv [z/x]e_1\n$$\n$$\n\\c{case}(\\c{inr}\\,z, \\c{inl}\\,x \\to e_1, \\c{inr}\\,y \\to e_2) \\equiv [z/y]e_2\n$$\nThe uniqueness property tells us this map is unique. In particular, uniqueness means that if you have a map $h : X+Y \\to Z$, then $h \\equiv [h \\circ \\c{inl}, h \\circ \\c{inr}]$. (This is easy to see if you draw the coproduct diagram for this.)\nThis is exactly the $\\eta$-law, once you remember that composition of morphisms and substitution of terms are the same thing:\n$$\n\\newcommand{\\c}[1]{\\mathsf{#1}}\nC[e] \\equiv \\c{case}(e, \\c{inl}\\,x \\to C[ \\c{inl}\\,x], \\c{inr}\\,y \\to C[ \\c{inr}\\,y]) \n$$\n\nColimits without the uniqueness property are called \"weak colimits\", and come up fairly often in category theory. But it is still weird that $\\eta$ for projective tuples and lambdas is easy, but that it's so hard for sum types.", "meta": {"post_id": 1202, "input_score": 13, "output_score": 11, "post_title": "What is a commuting conversion and why are they problematic?"}}
{"input": "It is possible (with some cleverness) to prove false from type-in-type.  For instance, Girard proved false in Martin Lof's original system (as described in the introduction of An Intuitionistic Theory of Types).  This answer has a good account of the formal details.\nMost systems allow type-in-type, through a configured opt-in and sometimes even by default (Pebble, Cayenne, some Trellys languages).\nHas anyone ever \"proven\" an incorrect theorem with type-in-type, without intending to show that the system is unsound?", "output": "I did this myself!\n(At least, if you interpret \"incorrect\" as \"probably not true\" rather than \"demonstrably false\".)\nIn the early days of homotopy type theory, we were starting to use multiple universes in ways that, I believe, had rarely been done before in type theory.  Since Coq didn't support universe polymorphism yet, the only way to formalize some results was using type-in-type.  In particular, I used it when formalizing some of the initial work on modalities that eventually became the RSS paper.\nOne of the results I proved, at that time, was that an (idempotent monadic) modality is left exact if and only if the universe of modal types is modal.  This was a really striking application of, and justification for, the notion of left exactness.  In particular, it implies that the modal types for a left exact modality form a model of type theory on their own, which is useful when constructing higher-topos models of type theory since any topos is a left exact localization of a presheaf topos.\nUnfortunately, when we later went back to write up and formalize these results correctly using universe polymorphism, I realized that the proof was invalid.  When universe levels are tracked, what I had proven was that the type of modal types in one universe $U$ was null with respect to all $U$-small modally-connected types; but since $U$ is not $U$-small, that doesn't imply $U$ is modal \u2014 unless we assume the modality is generated by a $U$-small family, i.e. that it is $U$-accessible.  (In this form, the result is Theorem 3.11 in RSS.)\nI don't have a counterexample to the statement in the non-accessible case, but I doubt it is true.  In fact, for a non-accessible modality there isn't necessarily a canonical way to extend a modality on one universe to a modality on larger universes, so if we have a lex modality on $U$-small types, it's not even obvious what it means to say that $U$ is modal.\nEver since this experience, I've been unconvinced by any argument along the lines of \"type-in-type is okay as long as you don't do anything stupid\".  I'm willing to grant that what I did was stupid; the point is that it's possible to do stupid things by accident.", "meta": {"post_id": 1219, "input_score": 31, "output_score": 43, "post_title": "Has anyone ever accidentally \"proven\" a false theorem with type-in-type?"}}
{"input": "Is impredicativity useful for program extraction in Coq? For example is there some kind of realizability argument that depends on impredicativity?\nOf course it doesn't seem to be necessary for program extraction, as Agda manages to extract programs to Haskell, but I suspect there might be some kind of theoretical tradeoff.", "output": "I have occasionally thought about this question. My inconclusive conclusion is that impredicativity hinders program extraction. Let me try to give an argument in the context of realizability. I am going to crank up the technical level of the discussion, because the concepts involved are, well, a bit technical (but they also embody some pretty concrete intuitions).\nAssume we're working in a realizability topos over a pca $\\mathbb{A}$. Now, that's a pretty big and complicated category, which among other things contains $\\mathsf{Set}$ as a full subcategory: given any set $X$, we get an object $\\nabla X = (X, {\\Vdash_{\\nabla X}})$ in the topos whose realizability relation is trivial: $r \\Vdash_{\\nabla X} x$ holds for all $r \\in \\mathbb{A}$ and all $x \\in X$. From a computational point of view the realizers of $\\nabla X$ are maximally uninformative because every realizer $r$ realizes every element of $X$.\nSo if the realizability topos can contain such computationally trivial objects, perhaps we should try to define precisely what it means for an object to be computationally trivial in general.\nHere is a bad attempt: say that an object $S$ of the topos is computationally trivial if every morphism $S \\to \\mathsf{Bool}$ is constant, where $\\mathsf{Bool} = 1 + 1$ is the object of Booleans. The idea is right, namely $S$ should be considered computationally trivial if we can't extract any interesting bits of information from it, but the execution is wrong, because sometimes we can extract less than one bit. (For instance, we can't decide whether any given machine halts, but we can semi-decide that it halts.)\nGiven any disjoint non-empty subsets $T_0, T_1 \\subseteq \\mathbb{A}$ define the test object $[T_0, T_1]$ to be the set $\\{0,1\\}$ with the realizability relation\n$$r \\Vdash_{[T_0, T_1]} b \\iff (r \\in T_1 \\land b = 1) \\lor (r \\in T_0 \\land b = 0).\n$$\nIn words, $1$ is realized by the elements of $T_1$ and $0$ by the elements of $T_0$. Examples:\n\nDecidable test: $T_0 = \\{\\mathtt{false}\\}$ and $T_1 = \\{\\mathtt{true}\\}$ yields the booleans,\nSemi-decidable test: $T_0 = \\{\\ulcorner M \\urcorner \\mid \\text{machine $M$ does not halt on input $\\ulcorner M \\urcorner$}\\}$ and $T_1 = \\{\\ulcorner M \\urcorner \\mid \\text{machine $M$ halts on input $\\ulcorner M \\urcorner$}\\}$.\nWe can make much weaker notions of test than decidability and semi-decidability. Write $\\mathsf{PA} \\models \\phi$ if the sentence $\\phi$ in the language of Peano arithmetic is true, and define $$T_0 = \\{ \\ulcorner \\phi \\urcorner \\mid \\mathsf{PA} \\models \\phi\\}$$ and $$T_1 = \\{ \\ulcorner \\phi \\urcorner \\mid \\mathsf{PA} \\models \\lnot\\phi \\}.$$ This sort of test is very weak. It is not about deciding arithmetical sentences, but rather expresing conditions with arithmetical formulas. Indeed, suppose $r \\Vdash_{[T_0, T_1]} b$. Then $r$ encodes some arithmetical sentence $\\phi$ such that $b = 1 \\Leftrightarrow \\mathsf{PA} \\models \\phi$ \u2013 so telling which bit $r$ encodes is as hard as computing arithmetical truth.\n\nA morphism $f : S \\to [T_0, T_1]$ can be thought of as a two-valued test.\n\nDefinition: An object $S$ of the realizability topos is computationally trivial if for all test objects $[T_0, T_1]$ all morphisms $S \\to [T_0, T_1]$ are constant.\n\nWhat the definition says is that one cannot extract any non-trivial information from $S$, for any notion of two-valued test.\n\nProposition: For any set $X$, the object $\\nabla X$ is computationally trivial.\n\nHere is the punchline: impredicativity is intimately tied to completeness, bu completeness prevents extraction of any useful information.\n\nTheorem: A complete lattice is computationally trivial.\n\nProof. We already know that $\\nabla \\{0,1\\}$ is computationally trivial. Now consider any complete lattice $L$, a test object $[T_0, T_1]$ and a morphism $f : L \\to [T_0, T_1]$. Given $x \\in L$, define $g : \\nabla \\{0,1\\} \\to L$ by\n$$g(p) = \\sup \\{ y \\in L \\mid y = x \\land p = 1 \\}$$\nThe map $f \\circ g : \\nabla \\{0,1\\} \\to [T_0, T_1]$ is constant, therefore\n$$f(\\bot) = f(g(0)) = f(g(1)) = f(x),$$\nwhich shows that $f$ is constant. $\\Box$\nP.S. A comment for the connoisseurs: all the above says is that all morphisms from a complete lattice to a modest set are constant.", "meta": {"post_id": 1221, "input_score": 11, "output_score": 14, "post_title": "What is the role of impredicativity in program extraction?"}}
{"input": "Every community can have inside jokes among it, and April Fool\u2019s jokes are a common variety, but proof assistants are particularly susceptible to April Fool\u2019s jokes. They have strong guarantees of correctness so one could claim both that these claims are wrong or that one found irrefutable proof of something crazy.  More so, they are very labor-intensive, making one possibly overly excited for claimed magical tools which speed up the field.\nWhat are some good April Fool\u2019s day jokes about proof assistants?\n\nFor those whose might vote to close this, let me at least say there is precedence for this question.  Also this question is not itself a April Fool\u2019s joke. :)\n\nOne answer per joke please.  Keep it to the subject of this website.  Provide a link if possible.  I assume this will be made community wiki.", "output": "Brunhilde/Rhein 2015, as announced on the Isabelle mailing list was a fully working German translation of Isabelle/HOL, including keywords. It also came with a homepage, example documents and suggestive screenshots such as this one:", "meta": {"post_id": 1229, "input_score": 6, "output_score": 10, "post_title": "What are good examples of April fools jokes in the proving assistant community?"}}
{"input": "When defining an inductive type, there is a famous \"positivity\" restriction on the constructor types. For example, an inductive type $\\mathsf D$ has constructor $\\mathsf c : F(\\mathsf D) \\to \\mathsf D$, where $F(\\mathsf D)$ is the type of the constructor argument, and it can contain $\\mathsf D$ itself. Then the positivity restriction basically states that the proof assistant needs to see that $F$ is a functor $\\mathsf{Type} \\to \\mathsf{Type}$(1). This is crucial because otherwise the induction principle wouldn't make sense: $$\\forall (P : \\mathsf D \\to \\mathsf{Prop}).\\left(\\forall (\\vec x : F(\\mathsf D)). \\mathtt{fmap}_{F} P \\, (\\vec x) \\to P(\\mathsf c\\, \\vec x)\\right) \\to \\forall x. P(x)$$ where $\\mathtt{fmap}_F$ is the map on the morphisms associated with the functor $F$. The proof checker, based on the consideration above, implements some syntactic criteria to ensure functorality, which become known as positivity. On the other hand, the \"strictly positive\" criterion, as I view it, is added to avoid size issues similar to that of set theory.\nThis has led me to wonder if such a criterion can be relaxed on propositions. In languages having definitionally irrelevant (or related weaker notions of) propositions, the propositions can be roughly viewed as a subsingleton, i.e. a subset of the singleton. This avoids all the size issues. Also, the functorality condition degenerates because of the irrelevance. So perhaps for inductive propositions, we can allow $F : \\mathsf{Prop} \\to \\mathsf{Prop}$ to simply be monotonic. So if the proof checker can deduce (from some syntactic criteria, or by a user supplied proof) that $F(p) \\to F(q)$ whenever $p \\to q$, then it should allow this inductive definition. In proof assistants embracing classical logic (at least in $\\mathsf{Prop}$), this can be much more powerful. The proof checker might just run a small SAT solver to see if the definition is monotonic.\nThe question is: does this leads to anything interesting (including possible paradoxes when combined with other features in the type theory)? Pointers to existing literature are also welcome.\n(1) : Let's ignore universe levels for now. $\\mathsf{Type, Prop}$ are just suggestive notations.", "output": "Impredicative strict Prop supports initial F-algebras for all F : Prop \u2192 Prop functors. In Coq:\nRequire Import Coq.Unicode.Utf8.\n\nRecord Functor : Type := mkFunctor\n  {ob : SProp \u2192 SProp; hom : \u2200 (A B : SProp), (A \u2192 B) \u2192 ob A \u2192 ob B}.\n\nRecord Alg (F : Functor) : Type := mkAlg\n  {carrier : SProp; phi : F.(ob) carrier \u2192 carrier}.\n\nArguments mkAlg {F}.\nArguments carrier {F}.\nArguments phi {F}.\n\nDefinition InitAlg (F : Functor) : Alg F :=\n  mkAlg (\u2200 (P : SProp), (F.(ob) P \u2192 P) \u2192 P)\n        (\u03bb fx P c, c (F.(hom) _ _ (\u03bb g, g P c) fx)).\n\nDefinition InitAlgInit (F : Functor)(A : Alg F) (x : (InitAlg F).(carrier)) : A.(carrier) :=\n  x (A.(carrier)) (A.(phi)).\n\nAs you mentioned, all equations can be omitted here because of the irrelevance, and there's no need to consider \"dependent\" elimination. For ordinary Prop, we would need an axiom of irrelevance to do the same. Impredicativity is generally required, else \u2200 (P : SProp), (F.(ob) P \u2192 P) \u2192 P) doesn't fit in SProp.\nI couldn't come up with examples of interesting applications of non-strictly positive SProp-s, maybe someone else here can.", "meta": {"post_id": 1239, "input_score": 10, "output_score": 10, "post_title": "Replacing (strict) positivity with monotonicity on propositions"}}
{"input": "Consider the simple indexed inductive type\nInductive Single : nat -> Set :=\n| single_O : Single O\n| single_S {n} : Single n -> Single (S n).\n\nIntuitively, I thought that Single n has a unique value for each n : nat.\nI started by trying to prove that forall s : Single O, s = single_O.\nHowever, the usual tactics inversion, destruct, and induction did not work:\nLemma single_O_unique (s : Single O) : s = single_O.\n    inversion s.  (* No effect *)\n    Fail destruct s.\n    Fail induction s.\n\nThe error messages were:\nAbstracting over the terms \"n\" and \"s\" leads to a term\nfun (n0 : nat) (s0 : Single n0) => s0 = single_O\nwhich is ill-typed.\nReason is: Illegal application: \nThe term \"@eq\" of type \"forall A : Type, A -> A -> Prop\"\ncannot be applied to the terms\n \"Single n0\" : \"Set\"\n \"s0\" : \"Single n0\"\n \"single_O\" : \"Single 0\"\nThe 3rd term has type \"Single 0\" which should be coercible to \n\"Single n0\".\n\nSo I resorted to a manual match expression:\n    refine match s with\n    | single_O => _\n    | single_S _ => _\n    end.\n\nResulting in the following proof context:\ns: Single 0\n\n(1/2)\nsingle_O = single_O\n\n(2/2)\nIDProp\n\nwhich was puzzling, but easy to prove:\n    - reflexivity.\n    - exact idProp.\nQed.\n\nQuestions:\n\nWhy was inversion unable to recognize that s could only be single_O and substitute accordingly?\n\nWhy did the refine tactic produce the subgoal IDProp?\n\nIs there a way to get inversion or destruct to work in this case?  Or, what would a better way to prove s = single_O?\n\n\nFull example:\nInductive Single : nat -> Set :=\n| single_O : Single O\n| single_S {n} : Single n -> Single (S n).\n\nLemma single_O_unique (s : Single O) : s = single_O.\n    inversion s.  (* No effect *)\n    Fail destruct s.\n    Fail induction s.\n    refine match s with\n    | single_O => _\n    | single_S _ => _\n    end.\n    - reflexivity.\n    - exact idProp.\nQed.", "output": "Or, what would a better way to prove s = single_O?\n\nI would define a function that, given a nat n, computes the canonical proof Single n.\nFixpoint Canonical (n : nat) : Single n := match n with\n  | O => single_O\n  | S n => single_S (Canonical n)\nend.\n\nYou can then easily prove that any Single n proof is equal to the canonical one by induction. Here the abstraction won't fail because the equality is already generic over n.\nLemma single_canonical (n : nat) (s : Single n) : s = Canonical n.\nProof.\ninduction s.\n - reflexivity.\n - simpl; f_equal; assumption.\nQed.\n\nYour original lemma is then a direct corollary.\nLemma single_O_unique (s : Single O) : s = single_O.\napply single_canonical with (n := O).\nQed.", "meta": {"post_id": 1246, "input_score": 11, "output_score": 8, "post_title": "Proving uniqueness of an instance of an indexed inductive type"}}
{"input": "I just started with Lean and with this nice SE.\nIn the official web book/tutorial, when explaining definitions https://leanprover.github.io/theorem_proving_in_lean/index.html they ask to complete this (partial) definition of curry (in paragraph 2.4):\ndef curry (\u03b1 \u03b2 \u03b3 : Type*) (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := sorry\n\nI tried\ndef curry (\u03b1 \u03b2 \u03b3 : Type*) (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := \u03bb  x y, f (x,y) \n\nand it seems to be accepted by the system.\nIs this definition correct? If yes, how do I use it?\nI expected something like\ncurry x y f    or      curry f x y\nshould work, but they don't.\nActually the given (partial) definition is of type \u03b1 \u2192 \u03b2 \u2192 \u03b3 so it expects just 2 arguments of type \u03b1 and \u03b2 respectively, but no function f.\nThank you for your help", "output": "If Lean checks your code you know it is correct.  (Well, that and the fact there is only one curry function up to functional equivalence.)\nNow as for how to use curry you just defined, notice the type signature says you need to put in the following things in order:\n\n\u03b1 : Type*\n\u03b2 : Type*\n\u03b3 : Type*\nf : \u03b1 \u00d7 \u03b2 \u2192 \u03b3\nx : \u03b1\ny : \u03b2\n\nSo you have to use it as, e.g.\ncurry \u2115 bool \u2124 f n b\n\n\nNote, it is cumbersome to have to write the types since you can usually get them from the types of f, n, and b in this case.  So usually (and the tutorial will get to this soon), you write this function using curly braces for the arguments which are implicit:\ndef curry' {\u03b1 \u03b2 \u03b3 : Type*} (f : \u03b1 \u00d7 \u03b2 \u2192 \u03b3) : \u03b1 \u2192 \u03b2 \u2192 \u03b3 := \u03bb  x y, f (x,y)\n\nIn this case, you can write this as\ncurry' f x y\n\nor in rare cases when you need to supply the types:\n@curry' \u03b1 \u03b2 \u03b3 f x y", "meta": {"post_id": 1249, "input_score": 9, "output_score": 9, "post_title": "How to define curry in Lean"}}
{"input": "How are the Coq math-comp account and repositories related?\n\nDetails\nOne of my side goals is to try to keep the tags on this site meaningful and useful.\nToday I ran into this question:\nHow to prove has_esp when using mathcomp.analysis?\nI do not use Coq so take that in mind.\nWhen I saw\n\nPage of mathcomp.analysis: https://github.com/math-comp/analysis\n\nI thought it was odd as the tag coq-mathcomp was used, but then a specific repository was given. In looking at at the mathcomp account (https://github.com/math-comp), which is different from the matchcomp repository (https://github.com/math-comp/math-comp) I did see a separate repository for analysis (https://github.com/math-comp/analysis).\nI know that sometimes that with GitHub accounts multiple repositories can be loaded into a tool by various means with a package management system being common.\nI also downloaded the code as noted on the web page (https://github.com/math-comp/math-comp/releases) and searched for an analysis directory and found none thinking that analysis would be included in the download on the web page.\nSo it seems that some useful information is glossed over on the web page.\nMy current thought to my question above is\n\nmath-comp/math-comp - What most users mean when the mention Coq and math-comp\nmath-comp/analysis - Analysis code not installed with math-comp/math-comp\nmath-comp/algebra-tatics - Another repository not installed with math-comp/math-comp\nmath-comp/math-comp.github.io - The website https://math-comp.github.io/\n\n...\nNeedless to say for those that use it often, it is not confusing, but to some one new it is as confusing as the different Lean accounts. (ref)\nThe primary goal of this question is to have an answer that makes it clear to some one not familiar with Coq math-comp account and repositories to understand what they need to install, be it one of the repositories or a combination of them.\nUsing that answer, it should be clear enough to know how to add tags related to math-comp as they are needed. That does not need to be addressed in the answer, but it will be used as a criteria for accepting an answer.\n\nNote: For the question How to prove has_esp when using mathcomp.analysis?, I created, added and edited the tag coq-mathcomp-analysis", "output": "As Meven wrote, the Mathematical Components repositories originate from the work of Georges Gonthier and his team proving first the Four Color Theorem, then the Odd Order Theorem.\nIn Coq people do not install \"repositories\", they install \"packages\" (most often through the opam package manager, but this can also be through the Nix package manager or through a distribution like Debian). The MathComp library contained in the main repository (math-comp/math-comp) is already divided into multiple packages:\n\ncoq-mathcomp-algebra\ncoq-mathcomp-character\ncoq-mathcomp-field\ncoq-mathcomp-fingroup\ncoq-mathcomp-ssreflect\n\nBut there are many more packages that have the coq-mathcomp- prefix and are also available to users. These packages are developed in separate repositories (most of them under the math-comp organization, but now the fourcolor repository has migrated to the coq-community organization).\nRoughly speaking, the criteria for a package to be inside or outside the main repository is supposed to be its quality and maturity, but also its reusability (that's why odd-order and fourcolor are outside).\nAll of these packages and the ones that depend on them form what one could call the \"MathComp sub-ecosystem of the Coq ecosystem\".", "meta": {"post_id": 1276, "input_score": 8, "output_score": 13, "post_title": "Explanation of Coq math-comp repositories"}}
{"input": "In the init/logic.lean file of the Lean 3 standard library, nonempty and inhabited are defined. It seems like these two classes are very similar - you can instantiate either of them given an element of the type. In what situations do you use one over the other?", "output": "The difference between nonempty and inhabited is that nonempty A : Prop but inhabited A : Sort (max 1 u) (if A : Sort u). This makes inhabited A isomorphic as a type to A, while nonempty A is the propositional truncation of A, equivalent to \u2203 x : A, true. To extract a value from inhabited A is inhabited.default, but extracting a value from a proof of nonempty A is the axiom of choice and is noncomputable.\nGenerally, you should use nonempty A if you only need the \"mere fact\" that A is not empty, while inhabited A is used if you need to access a specific default value for totalizing a function or so. Mathlib has a linter to ensure that you don't use inhabited in place of nonempty for proving theorems unless you need it in the statement of the theorem.", "meta": {"post_id": 1286, "input_score": 13, "output_score": 15, "post_title": "Lean \"nonempty\" vs \"inhabited\""}}
{"input": "I have this function:\nDefinition bexp x y := bexp_r x y [true].\n\nAnd I have this goal:\nvalue (bexp [] y) = 0 ^ value y\n\nI want to replace the call to bexp with the underlying bexp_r.\nI tried to use the simpl tactic but it did not replace it.", "output": "The tactic you are looking for is probably unfold. If you write unfold bexp it will replace it with its body.\nIn certain cases, unfolding the definition will unfold too much and if you want more control over it you can prove unfolding lemmas instead and rewrite with them.", "meta": {"post_id": 1297, "input_score": 6, "output_score": 10, "post_title": "How to replace a function by its body"}}
{"input": "The docs on lambdas in Agda provide two forms of lambda: a curly brace based version, and the where syntax. But while writing some programs, I stumbled across a third version: one pattern, no braces, no where. I thought it would behave the same as a single pattern and braces, but here's a counter example to that:\nnatid : \u2115 \u2192 \u2115\nnatid = \u03bb x \u2192 x\n\ng0 : ( \u03bb x \u2192 natid x ) \u2261 ( \u03bb x \u2192 natid x )\ng0 = refl\n\ng1 : ( \u03bb { x \u2192 natid x } ) \u2261 ( \u03bb x \u2192 natid x )\ng1 = refl\n\ng2 : ( \u03bb x \u2192 natid x ) \u2261 ( \u03bb { x \u2192 natid x } )\ng2 = refl\n\ng3 : _\u2261_ {_} {_ \u2192 _} ( \u03bb { x \u2192 natid x } ) ( \u03bb { x \u2192 natid x } )\ng3 = refl\n\n-- g4 : ( \u03bb { x \u2192 natid x } ) \u2261 ( \u03bb { x \u2192 natid x } )\n-- this fails with error message:\n     _a_91 : Agda.Primitive.Level  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,28-29 ]\n     _A_92 : Set _a_91  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,28-29 ]\n     _94 : _A_92  [ at /Users/olekgierczak/repos/plfa/src/plfa/part1/Connectives.lagda.md:278,8-25 ]\n\n     \u2014\u2014\u2014\u2014 Errors \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n     Unsolved constraints\n\nSo my question is, what does a lambda with no braces around a single pattern mean, and why does the type inference of Agda treat g1/g2/g4 differently.", "output": "The key thing about \"pattern\" lambda is that it's a fake: each such thing elaborates as the local invocation of a new (i.e., generative) top-level helper function. The type declaration for that helper function must be somehow synthesized, and it is indeed important to allow it to be a dependent type. It looks to me as if allowing the extra degrees of freedom on both sides of the equation results in an underconstrained inference problem.\nIn response to the question in the comment, the g3 example gives _ -> _ as the type at which equality is demanded, which constrains the type of the function to be non-dependent. That seems to be enough information to ensure a most general solution.\nMeanwhile, in reference to the examples which do check out, note that g0 requires only syntactic identity to go through; the other three work only thanks to eta-expansion and laziness. You should really think of each pattern lambda as some generated foo, which has a definition given by what's in the braces. Neither foo nor \\ x -> natid x reduces. Fortunately, as they have function type, Agda will generate a fresh variable standing for a hypothetical input and apply both functions to it, at which point they do both compute. But foo computes only because it is lazy in its input.\nSo here's the classic gotcha:\nyay : (\\ f -> _\u2261_ {_}{Bool -> Bool} f f) (\\ { false -> true ; true -> false })\nyay = refl\n\nchecks, but\nnay : _\u2261_ {_}{Bool -> Bool}\n        (\\ { false -> true ; true -> false })\n        (\\ { false -> true ; true -> false })\nnay = {!refl!}\n\ndoes not.\nHow so? In yay only one auxiliary definition is generated, and it is seen to be the same as itself by name. In nay, two auxiliary definitions are generated, differing only in name, but eta-expansion is not enough to make the names compute away.", "meta": {"post_id": 1305, "input_score": 8, "output_score": 8, "post_title": "Curly Braces and Lambdas in Agda"}}
{"input": "Theorem search\n  {P : nat -> Prop} (dec : forall n, {P n} + {~P n})\n: ~~(exists n, P n) -> {n | P n}.\nAdmitted.\n\nI don't think this is provable in Coq without additional axioms, and it is provable when assuming LEM : forall A : Prop, A \\/ ~A. However, I don't think it proves LEM. What are the weakest axioms that prove search? I'm not very familiar with all the versions of the axiom of choice, but I guess this is probably equivalent to one.", "output": "This is an axiom in itself, known as Markov's principle. A more compact but equivalent way to state this axiom is\nAxiom MP : forall (f : nat -> bool), ~~ (exists n, f n = true) -> exists n, f n = true.\n\nThe original phrasing of the question hardwires a bit of choice that is provable in CIC, known as constructive definite description.\nLemma description : forall (f : nat -> bool), (exists n, f n = true) -> {n | f n = true}.\n\nThis theorem is a consequence of singleton elimination.\nMarkov principle is not provable in CIC, but it enjoys a special status amongst semi-classical principles. As already observed, it is a consequence of classical logic in Prop. As such it does not endanger the computational properties of extraction. Even better, you can actually natively extend CIC with it without breaking strong normalization and canonicity. See for instance this other question for more details.", "meta": {"post_id": 1332, "input_score": 9, "output_score": 11, "post_title": "What axioms do I need to search the naturals?"}}
{"input": "I have used Coq's bidirectionality hints (placement of & in a call to Arguments) to some effect, mostly by trial and error. Despite familiarity with bidirectional type checking, I can't connect the explanation given in the documentation with what I actually observe. So, precisely what change does & make to the checking of an application? To start, I understand the default bidirectional typing rule for applications to be, more or less (read clockwise, with \u2208 denoting type synthesis and \u2264 denoting subtyping (including unification)):\nf \u2208 \u2200 (x1 : A1) ... (xn : An), B\nM1 \u2208 A1'    ...    Mn \u2208 An'\nA1' \u2264 A1    ...    An\u2032 \u2264 An\n----------------------------------\nf M1 ... Mn \u2208 B[M1/x1, ..., Mn/xn]\n\nSome questions I'd like to be answered as corollaries of the explanation:\n\nIs Arguments foo & x y. equivalent to Arguments foo x y.?\nIs Arguments foo x y &. equivalent to Arguments foo x y.?\nIs Arguments foo {x} & y. equivalent to Arguments foo & {x} y.?\nWhat happens for partial applications (of Curried functions)?\nIs the Arguments f ... & ... . mechanism reasonably complete for bidirectional typing strategies you may want/expect?", "output": "I'm familiar with bidirectionality hints in Coq but with superficial knowledge of bidirectional typing, so some details may be off.\nAs our running example consider this binary function with a bidirectionality hint in the middle:\nParameter f : forall (x : A), B x -> C x\nArguments f x & y.\n\nBidirectionality hints affect type checking of an application.\nNormally, function application is associated with a synthesis (\u2191) rule, which turns into checking (\u2193) via the standard subsumption/unification rule (denoted \u2291, but in Coq it's more like an =, except that coercions make it asymmetric again):\nf : forall (x : A), B x -> C x\na \u2193 A     b \u2193 B a\n----------------- app-syn\n  f a b \u2191 C a\n\ne \u2191 T'     T' \u2291 T\n----------------- syn-check\n      e \u2193 T\n\nThey combine into the following rule for checking an application f a b:\nf : forall (x : A), B -> C x\na \u2193 A     b \u2193 B a    C a \u2291 T\n---------------------------- (1)\n         f a b \u2193 T\n\nNotably, the unification with the required type T happens at the end.\nA bidirectionality hint moves that unification earlier, to the &:\nf : forall (x : A), & B x -> C x      (* \u2190 f with a bidir. hint after 1 argument *)\na \u2193 A     C a \u2291 T     b \u2193 B a\n----------------------------- (2)\n         f a b \u2193 T\n\nOf course, rules (1) and (2) are equivalent when read purely declaratively. The difference becomes observable when those rules are interpreted in a stateful context. In Coq, we are actually type checking terms containing metavariables (holes), which get instantiated by unification, possibly creating redexes, hence the order in which premises are evaluated matters.\nIn this example, if we type check f ?a b, containing a hole ?a, then the standard type checking algorithm (rule 1) will try to check b \u2193 B ?a first, which may get stuck on that hole. If we take the bidirectionality hint into account (rule 2), we first unify C ?a \u2291 T, which may hopefully instantiate ?a so that subsequently checking b \u2193 B ?a succeeds.\n\nAs a concrete example, consider the following definitions. So is a predicate on Booleans which is inhabited by I when the Boolean is true, and uninhabited when the Boolean is false. From a proof of So b, So_eq extracts a proof that b = true. We use So as a means of proof automation when using auto_fin \u2013 we can cast a concrete natural number m into a bounded natural number of type Fin.t n whenever m <? n computes to true.\nRequire Fin.\nRequire Import PeanoNat.\n\nDefinition So (b : bool) : Prop := if b then True else False.\n\nDefinition So_eq {b : bool} : So b -> b = true :=\n  match b with\n  | true => fun _ => eq_refl\n  | false => False_rec _\n  end.\n\nDefinition auto_fin {n} m (prf : So (m <? n)) : Fin.t n :=\n  Fin.of_nat_lt (proj1 (Nat.ltb_lt _ _) (So_eq prf)).\n\nWe want to apply auto_fin as follows, checking it against a finite type with concrete bound (5 in this case). However, from the error message we can see that this fails because the implicit argument n (which we can see should eventually be 5) has not been instantiated by the time we check the prf argument I.\nFail Check auto_fin 3 I : Fin.t 5.\n\n(* The term \"I\" has type \"True\" while it is expected to have type \"So (3 <? ?n)\" *)\n\nTo fix this, we want to make sure that n will propagate from the type we're checking against to the type of prf before we check prf. A bidirectionality hint can provide this behaviour:\nArguments auto_fin {n} m & prf.\n\nThe old and new type checking rules for auto_fin are, respectively, as in the following preformatted block. With the instantiations n = ?n, m = 3, prf = I, T = Fin.t 5, we see that the new rule is able to solve for ?n by unifying Fin.t ?n with Fin.t 5, and then proceed to reduce So (3 <? 5) to True, against which I checks. The key feature is that solving n by unification against the result type allows for reduction in the later argument prf.\nOld:\nn \u2193 nat    m \u2193 nat    prf \u2193 So (m <? n)    Fin.t n \u2291 T\n------------------------------------------------------\n                @auto_fin n m prf \u2193 T\n\nNew:\nn \u2193 nat    m \u2193 nat    Fin.t n \u2291 T    prf \u2193 So (m <? n)\n------------------------------------------------------\n                @auto_fin n m prf \u2193 T\n\n\n\n\nIs Arguments foo & x y. equivalent to Arguments foo x y.?\nIs Arguments foo x y &. equivalent to Arguments foo x y.?\n\n\nAs explained above, bidirectionality hints move a unification step from the end to the middle of an application. So (1) no, (2) mostly yes. Only \"mostly\" because with dependent types the \"arity\" of a function might depend on the arguments, so in full generality there is no way to specify a bidirectionality hint (essentially a number of arguments) that is equivalent to no hint.\n\n\nIs Arguments foo {x} & y. equivalent to Arguments foo & {x} y.?\n\n\nType checking happens after implicit arguments have been elaborated into metavariables (foo y becomes @foo ?x y). Furthermore you can always ignore implicit arguments with the @foo syntax. So these hints differ.\n\n\nWhat happens for partial applications (of Curried functions)?\n\n\nSince the effect of a bidirectionality hint is to insert a unification after some number of arguments, this works smoothly with currying. If a function is applied to fewer arguments than its hint, then it will be checked as if there was no hint.\n\n\nIs the Arguments f ... & ... . mechanism reasonably complete for bidirectional typing strategies you may want/expect?", "meta": {"post_id": 1343, "input_score": 15, "output_score": 8, "post_title": "How do Coq's bidirectionality hints (`&`) affect type checking?"}}
{"input": "Question\nIs CiC stronger than CoC, in terms of proof strength?\nContext\nTo illustrate the kind of confusion I am in, and what I'd like to learn from the answer, here is part of my inner monologue:\nIf I have understood correctly, CiC is CoC plus inductive definitions. An inductive definition places one or more types into the context, plus some constants and a recursor function. So is it just syntactic sugar to formulate assumptions or is it kind of an axiom, that recursors exist for every possible inductive definition? Can you do inductive definitions within a proof and if so, how would this look like in coq or lean?\nExample: Let FTT be a rephrasing of the Feit\u2013Thompson theorem, that does't refer to the integers or any other infinite structure, e.g. \"Every (Tarski-)finite non abelian simple group has a non trivial involution\". Thanks to Gonthier et alt. FTT is clearly provable in coq/CiC. I assume the proof uses the integers. Is FTT\nstill provable if we did not define the integers before? If yes, can we create the integers on the fly when we need them in the proof? Or is there a procedure to eliminate the use of the integers from the proof?", "output": "Meven's answer explains that without inductive types, you cannot hope to recover a natural number object with proper induction principles. You can interpret this as saying that you lose some mathematical expressiveness.\nHowever, in the Calculus of Constructions (CC) you can still define the Church integers\n$N := \\forall (X : \\mathrm{Prop}) , X \\to (X \\to X) \\to X$,\nand even though they don't come with an induction principle, you can still define exactly the same functions as terms $N \\to N$ as in CC + Inductives. So, in a way, you do not get any proof-theoretic strength out of inductives in an impredicative system.\nNote that it is not true at all for predicative systems, such as MLTT, where inductive types are the main source of proof-theoretical strength.\n\nLet's make this claim more precise: by CC, I mean the original system of Coquand, which can be expressed as a PTS with two sorts $\\mathrm{Prop}$ and $\\mathrm{Type}$, the first being impredicative.\nFor simplicity, I will only consider one inductive type, the integers. I use CC+Nat to mean CC extended with an inductive type $\\mathbb{N} : \\mathrm{Prop}$ with large elimination (and adequate computation rules):\n$$\n\\frac{P : \\mathbb{N} \\to \\mathrm{Prop} \\quad t_0 : P\\ 0 \\quad P_S : \\Pi\\ n. P\\ n\\to P\\ (S\\ n) \\quad n : \\mathbb{N}}{\\mathrm{natrec}(P, t_0, t_S, n) : P\\ n}\n$$\n$$\n\\frac{K : \\mathrm{Type} \\quad P_0 : K \\quad P_S : K\\to K \\quad n : \\mathbb{N}}{\\mathrm{Natrec}(K, P_0, P_S, n) : K}\n$$\nOne can show that any term of type $N \\to N$ (where $N$ still means the Church integers, not $\\mathbb{N}$!) in CC+Nat corresponds to an integer function using the normalization theorem. I will first explain that any such function is provably recursive in higher-order Peano arithmetic without choice (PA\u03c9), and then I will explain that any function that is provably recursive in PA\u03c9 can be expressed as a term in CC. Thus, CC+Nat and CC can define exactly the same functions on the Church integers.\n\nFrom CC+Nat to PA\u03c9\nThis direction requires analyzing a normalization proof (this one doesn't treat large elimination, but we can follow Werner on this). Given any typing derivation in CC+Nat, our goal is to prove that the term is normalizing within PA\u03c9 (we cannot do it uniformly for all derivations, as the proof strength of PA\u03c9 is not sufficient).\nFirst, we can show through simple syntactic consideration that in CC+Nat, terms can be stratified into kinds (inhabitants of $\\mathrm{Type}$), predicates (inhabitants of a kind), and objects (inhabitants of a predicate of kind $\\mathrm{Prop}$), and that nothing complex happens at the level of kinds \u2014 they can't be abstracted over, and they can be described by the following syntax:\n$$\nK ::= \\mathrm{Prop}\\ |\\ \\Pi (\\alpha : K) . K\\ |\\ \\Pi (x : T) . K\\qquad \\text{where $T$ has type Prop}\n$$\nThe standard reducibility proof assigns a set-theoretic interpretation to each kind:\n$$\n\\begin{split}\n\\mathcal{V}(\\mathrm{Prop}) & = SAT \\quad \\text{The set of saturated sets of terms} \\\\\n\\mathcal{V}(\\Pi (\\alpha : K_1) . K_2) & = \\mathcal{V}(K_1) \\to \\mathcal{V}(K_2)\\\\\n\\mathcal{V}(\\Pi (\\alpha : T) . K) & = \\mathcal{V}(K_2)\n\\end{split}\n$$\nUnfortunately, we cannot reproduce this definition in PA\u03c9, because it \"lives in kind \u03c9+1\".\nBut we are only interested in proving normalization for a specific derivation, and only finitely many kinds appear in that derivation \u2014 so we can restrict ourselves to a subtheory with bounded kinds, in which case the interpretation is definable in PAn for some n.\nThen one proceeds to assign an element $[P]$ of $\\mathcal{V}(K)$ to every predicate $P$ of kind $K$ by induction on the syntax, in the usual fashion (see this summary for instance. Integers and predicates defined by recursion are treated in Werner's thesis).\nFinally, one shows by induction on the typing derivations that if $\\vdash t : A$, then $t \\in [P]$ and thus $t$ is normalizing.\nWe have that if $\\vdash f : N \\to N$ in CC+Nat, then PA\u03c9 proves that for any Church integer $\\overline{n}$, $f\\ \\overline{n}$ is normalizing (since it does not involve more kinds than $f$). After some simple considerations on normal forms, we can show that it normalizes to an integer $\\overline{m}$, and thus we showed that the function coded by $f$ is recursive.\n\nFrom PA\u03c9 to CC\nGirard's representation theorem shows that any function that is provably recursive in PA\u03c9 can be expressed as a term in System F\u03c9. The idea is to first use a double negation translation and \"Friedman's trick\" to reduce the problem to higher-order Heyting arithmetic HA\u03c9, and then translate HA\u03c9 into F\u03c9 by erasing integers.\nFinally, there is an obvious embedding of System F\u03c9 in CC.\n\nExtensions\nThis proves that CC+Nat and CC can define the same functions on the Church integers. What about stronger systems?\nWerner actually treats a full scheme of inductives with large elimination for small constructors (also known as \"Impredicative-Set\"). I would expect that the same technique carries through for the full scheme, but inductives are somewhat messy.\nWhat about a universe hierarchy? We could hope that the full CIC (CC with a universe hierarchy and inductives) has the same proof-theoretic strength as CC\u03c9 (CC with a universe hierarchy). However, I don't know any normalization proof for CIC \u2014 we cannot do the stratification by kind anymore, so it is not clear whether a similar technique works.", "meta": {"post_id": 1360, "input_score": 13, "output_score": 10, "post_title": "Calculus of (inductive) Constructions: Do inductive definitions increase proof strength?"}}
{"input": "I've been playing with Lean, trying to prove the next lemma:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\n\nHere is the proof I've currently written:\nbegin\n  intro h,\n  cases classical.em C; cases classical.em A; cases classical.em B; try {right, assumption};\n  {right, apply h, intro hA, contradiction},\n  {sorry},{sorry},{sorry}\nend\n\nFirst {} seems okay; here is the Tactic state right before the contradiction:\n1 goal\nABC: Prop\nh: (A \u2192 B) \u2192 C\nh_1: \u00acC\nh_2: \u00acA\nh_3: \u00acB\nhA: A\n\u22a2 B\n\nSo, I have that h_2: \u00acA and hA: A, which should lead to the contradiction. Applying contradiction in Coq thus leads to completing the goal.\nBut here in Lean something strange occurs: in Tactic state window I get the notification goals accomplished , whileas in Messages I get contradiction tactic failed with the next state:\nstate:\nA B C : Prop,\nh : (A \u2192 B) \u2192 C,\nh_1 : \u00acC,\nh_2 : A,\nh_3 : \u00acB,\nhA : A\n\u22a2 B\n\nSo it seems like h_2 somehow has been changed, and this might lead to that message, although the goal is completed. Probably I've used contradiction tactic in the wrong way, but I don't see that for now. Can anyone help me to figure things out and to get rid of the message?\nThanks in advance!", "output": "The issue is the ; at the end of the long line. This is causing the {right, ...} block to be applied to all goals, meaning that the {sorry} blocks never have a chance to run, and the error you are getting at contradiction is in the second invocation on the block on the second goal. Change the ; to , to fix the issue:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nbegin\n  intro h,\n  cases classical.em C;\n    cases classical.em A;\n    cases classical.em B;\n    try {right, assumption}, -- <- note the comma\n  {sorry},\n  {sorry},\n  {right, apply h, intro hA, contradiction},\n  {right, apply h, intro hA, contradiction},\nend\n\n(Actually, as you can see the provided proof only works on goals 3 and 4; it is the first two that fail.)\nAlternatively, you can keep the ; but use try to apply the block anywhere it works:\nlemma l1_cl (A B C : Prop) : ((A \u2192  B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nbegin\n  intro h,\n  cases classical.em C;\n    cases classical.em A;\n    cases classical.em B;\n    try {right, assumption}; -- <- note the semicolon\n    try {right, apply h, intro hA, contradiction},\n  {sorry},{sorry},\nend\n\n\nOr if you are feeling lazy (and are using mathlib)\nimport tactic.tauto\nlemma l1_cl (A B C : Prop) : ((A \u2192 B) \u2192 C) \u2192 ((A \u2227 \u00ac B) \u2228 C) :=\nby tauto!", "meta": {"post_id": 1366, "input_score": 7, "output_score": 11, "post_title": "In Lean, contradiction tactic failed but actually goal accomplished"}}
{"input": "Most theorem provers simply axiomize inductive types (or equivalently W types) in the abstract which is fine.\nBut I'm curious about explicit constructions of inductive types within the theory.\nI suppose I'm interested in both inductive types and \"weak inductive types\" (weak initial algebras) such as impredicative encodings. I think the term I've most found in my searches has been \"weak natural numbers object.\" Inductive types can be better but weak inductive types are cool too.\nI know you can use impredicative universes as in System F to encode inductive types. Or you can just accept a universe bump. And apparently if you internalize a small amount of parametricity you can construct appropriate induction principles.\nBut I'm pretty sure I've heard you can construct inductive types other ways. I think maybe you need to assume some classical principles? I've read some stuff on transfinite induction I still don't really get. I think once you have a base inductive type of ordinals you can construct other inductive types in those terms?\nI have a hunch you can abuse impredicative proof irrelevant propositions and natural numbers to construct inductive types but I don't really have anything solid here.\nAlso for some reason I think it's easier to construct free monads the hard way instead of inductive types? I'm not sure this is an important issue.\nI don't think W types and polynomial endofunctors directly solve the problem. They provide some clarifying language for how to talk about inductive types but they're not quite an explicit construction.\nThere's a paper \"Induction Is Not Derivable in Second Order Dependent Type Theory\" I don't understand yet but I think all this means is you need to assume more axioms than second order dependent type theory?", "output": "In the HOLish settings, these types (starting with the natural numbers) are indeed constructed from first principles; they're certainly not axiomatised.  Harrison had an early (1995) paper on how to do this, and the technology has developed from there.\nHarrison's construction doesn't use ordinals, but encodes its types as trees underneath and then uses an inductive relation and the HOL type definition principle to prune away values that are not desired.\nThe more capable and more recent technology in Isabelle/HOL (\u201cbounded natural functors\u201d) does have some fancy cardinality reasoning behind it.  There's a nice 2012 LICS paper on this tech by Traytel et al.", "meta": {"post_id": 1376, "input_score": 9, "output_score": 10, "post_title": "Construction of inductive types \"the hard way\""}}
{"input": "A neutral/normal term in the lambda calculus is typically defined\ndata nf = Lam of nf | Neu of ne\ndata ne = Var of int | App of ne * nf\n\nNow the question is what to do about constructors and eliminators such as pairs, projections, natural numbers, or the recursor.\n\nWe could view them all neutral terms because they are just variables pair, fst, snd, zero, succ, rec that happen to have some semantic meaning. For example:\n\ndata ne = Var of int | App of ne * nf \n| Pair of nf * nf | Fst of nf | Snd of nf\n| Zero | Succ of nf | Rec of nf * nf * nf\n\n\nBut this question considers an alternative, namely to view constructors (pairs, natural numbers) as normal terms and eliminators (projections, recursor) as neutral terms, which also seems to make sense considering Lam is the constructor for function types and App the eliminator. For example:\n\ndata nf = Lam of nf | Neu of ne\n| Pair of nf * nf \n| Zero | Succ of nf\ndata ne = Var of int | App of ne * nf\n| Fst of ne | Snd of ne \n| Rec of nf * nf * ne\n\n(Edit) It seems in the first approach, terms are only normal up to $\\lambda$-reductions, whereas in the second, terms are in fully normal form.\nIs there an advantage to one approach over the other?", "output": "A neutral term is a variable, or an eliminator stuck on a neutral term. Basically a stack of eliminators applied to a variable.\nThese terms are called neutral, and distinguished from other normal forms, because they have not reduced to a constructor, but they cannot reduce further (because the eliminators are stuck on a variable).\nWith this in mind, your second definition of the type of normals and neutrals is the correct one. I would say this approach has the advantage of actually representing normal forms. I'm not sure why you would want a language with pairs where fst (pair zero zero) does not reduce.\nI would also caution that constructors like pair and zero, and eliminators like fst and rec really are not \"just variables\", anymore than lam is just a variable. Of course they are \"variables\" in the sense that they are strings of characters, but as you say, they have semantic meaning distinct from variables. When I used \"variable\" earlier in this answer, I meant a thing with the semantics of a variable, that can be the subject of substitution.", "meta": {"post_id": 1397, "input_score": 7, "output_score": 9, "post_title": "What is a neutral term?"}}
{"input": "The chess computer which beat the human world champion in 1997 had a huge database of openings inbuilt into it. However my understanding of Deep Mind's alpha zero is that it is capable of generating these openings by itself.\nPerhaps a theorem-proving analogy would be the following. The research groups trying to develop systems which do mathematics automatically -- is it true that right now they would have to teach such a system a proof of the infinitude of primes first (as \"training data\" or whatever)? And we're not yet at the stage where the systems can figure out a proof of this 2000+ year old result by itself? Or am I out of date?", "output": "I doubt you are out of date.  I think given a reasonable interpretation of this question, this is well beyond current (2022) systems.\nNonetheless, I will also argue your question needs some clarification as to what \"by itself means\".  Rhetorically, if GPT-f or the Isabelle Hammer could solve the infinitude of primes proof, would this count?\nGPT-f (in Lean or Metamath) is a trained model based on human proofs.  Assuming the training data didn't include the proof of infinitude of the primes, but still included much of Lean's (or Metamath's) library, would that count?  There are probably other proofs about the prime numbers and other theorems about sets being infinite.  Also, GPT-f takes advantage of Lean's tactics which also take a lot of math knowledge to create.  Is all this too much knowledge to be able to count for it being able to come up with the proof \"by itself\"?\nReducing the knowledge slightly are systems found in the papers Reinforcement Learning of Theorem Proving and Learning to Reason in Theories Without Imitation  These systems don't use any proof data.  They only use theorem statements, and definitions, and (sometimes) built-in tactics.  Would this count?\nSecond, the Isabelle Hammer is a mostly hand engineered tool (with a bit of machine learning), but a very powerful one.  Nonetheless, a lot of it's engineering is based on our understanding of first order logic and common mathematical objects like the integers.  Also, it again can use existing definitions and previous theorems in the library.  Does this count as \"by itself\"?\nAnd just to be clear, I don't know for sure if either GPT-f or the Isabelle Hammer can do this.  I doubt it, but one would have to check.  Nonetheless, my point was more of a thought experiment, since I think even if they could, it would wouldn't count.\nNow one could theoretically design a system which starts from \"nothing\" and tries to prove theorems.  Here \"nothing\" could mean a few things:\n\nIt could start from some axiom system like Lean's axioms, as well as just the definitions needed to state the infinitude of primes.  And it has the infinitude of primes as a goal.  It can't use human-written tactics.  It has to just use the base logic.\nSame as (1) but now it is not given any definitions or goals.  It just has to come up with interesting math on its own.\nSame as (2), but now we don't even have a logical framework.  It has to develop mathematics on its own.\n\nI am confident we can't do any of those in 2022.  And I question whether we can do (2) or (3) ever without somehow cheating and imparting mathematical knowledge.  Imagine for example how humans learned math throughout history.  I'm not an expert on this, but I imagine many real world concerns came into play:  Division of property for inheritance (both geometric land plots and discrete objects and animals), engineering, understanding physics, money and accounting, and so on.  Even the idea of primes might have been linked to our understanding that some things are made of multiple other things and then it is natural to ask how many basic things are there.\nNow, we could train an agent in a realistic multi-agent simulation environment with physics, an economy, geometry, etc with the hope that it learns math, but to code that up we are putting in our math knowledge.  Alternatively, we could feed a model with a lot of real world data (scraped from say the Internet), but again I would worry that this data already contains a lot of math knowledge.\nMy personal thought on this particular challenge is that the infinitude of primes should be replaced with a harder problem, and eventually we should work up to an AI which can develop unmistakably new mathematics or solve difficult unsolved problems.  Then there is no trouble throwing in data, throwing in existing math knowledge, giving the model goals, and throwing in real world data.  Then we don't have to worry as much about if we are cheating.  We just have to worry about building the future AI mathematician.\n\nUpdate: I want to update my answer with why I think my version (1) of the problem posed (which is the easy case) is much more difficult than chess, go, or Atari games, but also has a flavor of being possible with technologies on the horizon.\nRecall, in version (1) the learning agent knows the axiomatic rules of a theorem prover and knows the goal (prove that there are infinitely many primes) including all the necessary definitions leading up to that goal.  However, it doesn't have access to other theorems of mathematics, other definitions, or advanced tactics. At first, this feels just like the scenario of Alpha Zero where we start with the rules of chess and a goal of winning.  But note there are a few key differences, and in my opinion, one of the most key differences is that two-player games like Alpha Zero make it easy to develop a curriculum of progressively harder problems.  The agent plays itself, and to make progress it only needs to learn just enough to beat itself.  Combine that with a good learning tool (neural networks), a good search algorithm (Monte Carlo tree search), and enough computation, it can advance from random play to grand master.  But theorem proving doesn't have this.  Theorem proving is a one player game, also known as a puzzle.  However, even in many puzzles it is possible to develop good curriculums.  To solve Sokoban, many agents just start solving small random levels, progressively getting better on harder levels.  Similarly, for the Rubik's cube.  Moreover, the Rubik's cube has a nice trick.  You can start from a solved cube, shuffle it up, and preform every step of that shuffling in reverse to have a solution.  In this way it is possible to get lots of training data at many levels of difficultly (distance from the solution).\nTheorem proving seems different.  Sure if one already has a vast library of theorems statements (as in much of the previous work on AI and theorem proving), then it is possible to use those theorems as a curriculum for reinforcement learning.  The model will first solve the easiest problems by random chance, learn from those problems, and then be able to solve harder problems.  We can even improve this, as many papers do, by learning from partially solved proofs.  The real question however is if any of this is still possible without a bank of starting problems to get any traction?\nThere is however still a path forward.  The agent could just randomly explore the space of proofs, combining axioms and rules until it proves something.  Then it could learn from those proofs.  As it get's better, it can find harder and harder proofs.  But unlike the Rubik's cube, it is hard to find a random proof and harder still to find an \"interesting one\".  Indeed, now we are getting into hard problems in reinforcement learning such as learning to explore an open world with few or no explicit goals.  In such exploration, teh agent must be driven by \"curiosity\", but what makes an experience \"novel\" in that it should be remembered and learned from in the future, or sought out?  And in mathematics, what makes a mathematical fact interesting?\nNow one might see the benefit of a more holistic approach.  Maybe a theorem is \"interesting\" if it is useful to prove other theorems?  Maybe agents shouldn't just prove theorems, but also conjecture them?   Maybe agents shouldn't just focus on exploring the space of theorems, but also focus on other mathematical concepts like computation?  (Recall, a lot of math has been developed just to perform computations practically.) And soon we may want to be venturing outside the world of pure math?  (And this is why I'm not sure if Euclid's theorem is a good test problem or not, since it's simplicity requires avoiding real world knowledge for fear we might be tainting our model with facts about the prime numbers and arithmetic.)\nBut even if we stay inside the world of pure math, we still probably need our agent to develop its own definitions.  For example, Euclid's proof uses the product of all the first n primes, which is a special case of an iterated function.  What on Earth could make an algorithm find and explore such a concept?  And why is that concept any more \"interesting\" or \"natural\" or \"useful\" for the task at hand than any other definition in the infinite space of all definitions?  If the agent just starts enumerating definitions it will get all sorts of minutia like f(x) = x+(1+x)+1.  We need a way to pair this down to the most \"useful\", \"interesting\", \"fruitful\" ideas.\nOne work which has shown a lot of promise in this direction is DreamCoder, which isn't for proving theorems, but is for writing code.  Not only does it learn to write code with no human code examples, but it does so by building its own library of functions.  It is very careful to only keep the best most useful functions in the library to avoid an overload of choice.  And in doing so it learned to derive the map, fold, and unfold operators from just pure simply typed lambda calculus.  This is pretty remarkable!  However, even in this case DreamCoder was only able to do this since it was given a carefully designed curriculum of problems (given as input-output pairs where the agent was supposed to find a computer function matching that pair).  (In fairness to DreamCoder it used a really small curriculum, but it was a curriculum nonetheless.)\nSo the real question is, can we develop a good notion of \"interesting\", \"useful\", \"fruitful\" mathematics allowing us to start from \"nothing\" and make headway in mathematics by building our own curriculum of mathematics getting us closer and closer to Euclid's theorem?  And if we can do it, is it worth the effort?", "meta": {"post_id": 1403, "input_score": 10, "output_score": 10, "post_title": "auto-generating the proof of infinitude of primes"}}
{"input": "In Lean, some definitions must be marked as noncomputable, for example if they depend on the law of the excluded middle or other nonconstructive choice principles.  Usually, the reason for noncomputability is obvious.  Occasionally, however, Lean will tell me that something must be marked as noncomputable even though it seems perfectly constructive to me.  The example that I am facing at the moment involves long and complicated definitions spread over a number of files, so I cannot give a self-contained explanation here.  Are there good general methods for diagnosing this kind of issue?", "output": "The purpose of the noncomputability checker is to (try to) determine whether or not the VM compiler will succeed in making executable bytecode, which can be evaluated more efficiently (by #eval for instance). One thing to keep in mind is that \"computability\" is not a property that's verified by the Lean kernel, so if you're not planning on evaluating your definitions, the easiest thing to do is to just put noncomputable theory at the top of your modules :-). Lean will still compile definitions that pass the noncomputability checker when in this mode -- it just won't complain at you if you neglect to annotate noncomputable definitions with noncomputable.\nThat said, there are a few tricks to get things to be computable. Overall, what the noncomputability checker is trying to do is determine whether there is a noncomputable function being used within the computationally relevant part of the definition. The following are computationally irrelevant: anything that is Prop-valued (that includes set), anything that is Type-valued, proofs, and the parameters/indices that are the first arguments to a recursor or a constructor. (Note that a definition can depend on the law of the excluded middle and still be computable -- the way it usually would be noncomputable is if you're depending on the classical decidable instance for em in the computationally relevant part of the definition.)\nSometimes there are functions that use noncomputable arguments in a computationally irrelevant way. You can try using the @[inline] attribute for this since the noncomputability checker will inline definitions with this attribute while checking.\nSometimes it's that you're accidentally using a theorem/lemma instead of a def. Only def's are compiled, so technically theorems are noncomputable (in the next release of Lean 3 there will be a more user-friendly error message for this).\nSometimes it's that you're using a noncomputable instance somewhere. If you can figure out what it is, you can write a computable instance for the given types. The @[inline] trick can also be useful here if those instances aren't used in a computationally relevant way. (This shows up in mathlib for some constructors for homomorphisms, though while these constructors are computable, they're not really usefully computable.)", "meta": {"post_id": 1407, "input_score": 5, "output_score": 9, "post_title": "Lean: dubious noncomputability"}}
{"input": "Some time ago I asked this question on Stack Overflow but got no answer:\nhttps://stackoverflow.com/questions/60521384/tracing-tactics-in-isabelle\nSection 9.4 The Classical Reasoner of the Isar Reference Manual writes:\n\nThe tactics can be traced, and their components can be called\ndirectly; in this manner, any proof can be viewed interactively.\n\nI have found sections in this manual about tracing the simplifier and higher order unification.\nHow can I trace the classical reasoner?", "output": "I recall there is no end-user method to trace the classical reasoner, but there is a diagnosis configuration.\nYou should add a line setup \"Config.put_global Blast.trace true\" and further uses of blast will output the a (rather verbose) trace-like report.\nI believe that you can combine this with a judicious use of declare [[blast_depth_limit=n]] with an appropriate non-negative n to  dissect the output.", "meta": {"post_id": 1410, "input_score": 7, "output_score": 8, "post_title": "Tracing the classical reasoner in Isabelle"}}
{"input": "Coq\u2019s built-in termination checker accepts some rather intricate recursion patterns with functional values in data types, as shown by this example\nInductive Z_inf_branch_tree : Set :=\n  Z_inf_leaf : Z_inf_branch_tree\n| Z_inf_node : Z\u2192(nat\u2192Z_inf_branch_tree)\u2192Z_inf_branch_tree.\n\nFixpoint n_sum_all_values (n:nat)(t:Z_inf_branch_tree){struct t} : Z :=\n(match t with\n| Z_inf_leaf \u21d2 0\n| Z_inf_node v f \u21d2 v + sum_f n (fun x:nat \u21d2 n_sum_all_values n (f x)) end ).\n\n(taken from from Chapter 7.3.5.2 here https://www.labri.fr/perso/casteran/CoqArt/coqartF.pdf).\nI am looking for a formal description of the termination checker at work here, in order to reimplement it in a different system? Is it written down somewhere accessible?", "output": "Lennard G\u00e4her has implemented Coq's guard checker in Coq on top of MetaCoq as part of a lecture project I advised here:\nhttps://github.com/lgaeher/metacoq/blob/guarded/guarded/guardchecker.v\nIt has lots of comments and examples is the most accessible description I know of, the other two existing descriptions I am aware of being the comments in the OCaml code of Coq's implementation and this slide set by Bruno Barras, both of which served as sources for Lennard's code.\nLennard's code turns Coq's guardedness checker off to avoid making some functions structurally recursive, and thus there are no proofs about it. In principle it should however be possible to turn everything into \"proper\" Coq functions with some work and then prove the properties postulated in MetaCoq here.", "meta": {"post_id": 1428, "input_score": 6, "output_score": 8, "post_title": "Formal description of Coq\u2019s termination checker"}}
{"input": "I am a high school senior going into college and I am applying to a scholarship in which I must write an essay about a potential future technology that would dramatically impact humans. I immediately thought of proof assistants, which already exist, but I'm sure there are some limitations that can be overcome. I don't really know much about proof assistants, so I would like to learn more. I feel as though they might help to prove significant theorems that we currently have no answer to (if not already) that can benefit humanity massively.\nIf you could respond or link me to a place where I could learn more about the current state of proof assistants, I would greatly appreciate it. Here are some important questions I have:\n\nWhat types of theorems can proof assistants solve, and what types can they not at the present moment?\nHow can they be improved?\nWhat avoidable and unavoidable limitations are there?\nIs there anything else I should know about the current and potential future states of proof assistants?", "output": "These are broad questions and I doubt you will get a complete answer in a single stack exchange answer.  But since you seem ernest and interested, let me at least try to help clarify some of your questions.\n\nWhat types of theorems can proof assistants solve, and what types can they not at the present moment?\n\nFirst, to be clear, the primary purpose of proof assistants isn't to solve new theorems in mathematics automatically.  It is to help a human write proofs in an air-tight form.  Think of them as a programing language but instead of writing code, you write a proof---and instead of compiling and running the code, the computer checks that the proof is correct.\nThe three primary use cases of proof assistants are:\n\nTo prove the correctness of computer code and hardware.  Unlike mathematical proofs, these proofs are very tedious and a small missing case could mean the difference between correct code and a costly bug.  Proof assistants help to do this tedious work and not miss edge cases.\nTo build a library of formal mathematics.  By formalizing all of mathematics we can begin to make mathematics more accessible to computers and unify all of mathematics into a few curated digital libraries of mathematics.\nTo explore new types of logical systems.  New logics can be unintuitive and formal systems can help work with them.  Conversely, new foundational systems may be better suited for formalizing mathematics on a computer.\n\nA related technology called automated theorem provers are used to prove theorems automatically.  SAT solvers, SMT solvers, and resolution theorem provers are some examples.  They are often used for industrial applications, but they also have been used to solve certain combinatorial theorems.\nHaving said all this, there is a continuum between proof assistants and automated theorem provers.  More proof assistants have some levels of automation (some using full powered automated theorem provers) which fill in \"obvious proof steps\". Further a lot of current work in AI theorem proving uses data from proof assistant libraries to train AI models.\nNow, if I misunderstood your question and you meant what types of theorems can a proof assistant be used for formalize, here is the answer: In principle, all correct theorems should be formalizable since they follow from the axioms of mathematics.  In practice, some types of mathematics are easier than others.  Obviously, theorems like Fermat's Last Theorem which both depend on a lot of other kinds of mathematics and are large proofs in themselves, would be a lot of work to formalize.  It is better to build a library of mathematics slowly adding in more and more until you have the theorems and tools you need to prove modern theorems.\nAlso, some subjects like calculus and statistics are import so we teach them to college freshman, but their foundations are actually quite complicated making them difficult to formalize.\n\nHow can they be improved?\n\nUsability is a big concern.  Proof assistants are still hard to work with.  Formalizing is slow and it is a specialized skill.  Adding the state of art in program language design and interactive development environment tools would help.\nMathematics in a proof assistant is also very different looking than informal mathematics.  Bridging this gap would go a long way to speeding up formal proofs.\nBetter automation might help.  Then the user could have the computer fill in the obvious and not-so-obvious steps.  A dream would even have a computer system which could auto-formalize mathematics from say an informal LaTeX proof to a formal proof.  This is incredible challenging, but an exciting research goal.\nProof repair is an interesting topic.  Proofs break if you change the theorem you are proving.  This is especially common when proving the correctness of a piece of computer code which is subject to change.  But in many cases, it should be straight forward to repair the proof, and the computer in principle should be able to do it for you.\nThere are many proof assistants, but they are incompatible with each other.  A proof in one proof assistant isn't usable by any other proof assistants.  Better interchangeability would go a long way.\nThere is also the problem that formalization is not well rewarded in academics or industry.\n\nWhat avoidable and unavoidable limitations are there?\n\nI think I mentioned many avoidable limitations.\nAs far as unavoidable ones, baring human-level artificial intelligence, it is likely going to continue to be a somewhat tedious human process which gets progressively easier with better tools.\n\nIs there anything else I should know about the current and potential future states of proof assistants?\n\nProbably.  But at the same time, just get started playing with proof assistants if they interest you.  They are video-game level addictive!  I recommend the natural number game as a fun starting point.  After doing that, pick a proof assistant that interests you, get to know the community, and start formalizing stuff!", "meta": {"post_id": 1456, "input_score": 7, "output_score": 13, "post_title": "What is the current state of proof assistants?"}}
{"input": "What is a weak function space and what does it have to do with higher order abstract syntax?\nI mean I know what a weak function space is. It's that thing you use for HOAS in Lambda Prolog or toolkits like Beluga!\nBut what are the technical details here?\nIn category theory functions are abstracted away to exponential objects of closed Cartesian categories (not the same as a Cartesian closed category, STLC with product types is Cartesian closed, STLC without product types is closed Cartesian) and residuals/internal homs of closed monoidal categories.\nI have a hunch weak function spaces don't really work like exponentials at all.", "output": "In category theory the adjective \u201cweak\u201d is used when the uniqueness parts of a universal property is removed.\nFor example, a weak product of $A$ and $B$ is an object $P$ with morphisms $p_1 : P \\to A$ to $p_2 : P \\to B$, such that, for all $f : C \\to A$ and $g : C \\to B$ there exists (not necessarily unique!) $h : C \\to P$ satisfying $p_1 \\circ h = f$ and $p_2 \\circ h = g$.\nWe may apply the same idea to exponentials. A weak exponential of $A$ and $B$ is an object $E$ with a morphism $e : E \\times A \\to B$ such that, for all $f : C \\times A \\to B$ there exists (not necessarily unique!) $h : C \\to E$ such that $e \\circ (h \\times \\mathrm{id}_A) = h$. This is just the usual universal property of exponentials, with the uniqueness part removed.\nAn example of a category which has weak exponentials but no exponentials is the category of topological spaces and continuous maps. In more syntactic settings, such as logical frameworks, removing the uniqueness part from the definition of exponentials corresponds to removing the $\\eta$-rule. Consequently, given a term $z : C, x : A \\vdash f : B$, there may be many terms $z : C \\vdash h : A \\to B$ satisfying $$z : C, x : A \\vdash f \\equiv h\\, x : B.$$\nTo see that the $\\eta$-rule gives uniqueness, given $h_1$ and $h_2$ satisfying the above equation, observe that $$h_1 \\equiv (\\lambda x \\,. h_1 \\, x) \\equiv (\\lambda x \\,.f) \\equiv (\\lambda x \\,. h_2 \\, x) \\equiv h_2.$$\nConversely, from uniqueness we get $\\eta$-rule as follows: given any $z : C \\vdash h : A \\to B$, observe that\n$$z : C, x : A \\vdash h\\,x \\equiv h\\,x : B$$\nand by $\\beta$-reduction\n$$z : C, x : A \\vdash h\\,x \\equiv (\\lambda y \\,. h\\,y) \\, x : B$$\ntherefore by uniqueness $z : C \\vdash h \\equiv (\\lambda y \\,. h\\,y) : A \\to B$.\nAs an exercise you can verify that weakness of products correspond to absence of the $\\eta$-rule $(\\pi_1\\,t, \\pi_2\\,t) \\equiv t : A \\times B$.\nLet us also try to answer the question \u201cwhat does it have to do with HOAS?\u201c Recall that in HOAS we use the meta-level function space to model binding in the object-level terms. Does the meta-level $\\eta$-rule transfer to the object-level $\\eta$-rule? For that we would need something like (I am using $\\mathtt{typeface}$ to denote object-level syntactic constructs):\n$$\\mathtt{lambda}(\\lambda x \\,. \\mathtt{app}(h,x)) \\equiv h,$$\nwhich is not the case because \u201cobject-level application $\\mathtt{app}$ is not meta-level application\u201c. So in HOAS the object-level $\\eta$-rule is not available, hence \u201cweak\u201d exponentials.\nIncidentally, the meta-level $\\beta$-reduction fails to transfer to object-level $\\beta$-reduction also, because the object-level $\\beta$-reduction is something like\n$$\\mathtt{app}(\\mathtt{lambda}(t), u) \\equiv t \\, u,$$\nand once again the equation does not hold. Of course, one could consider HOAS with object-level equations, in which case both the $\\beta$-reduction and the $\\eta$-rule could be imposed.", "meta": {"post_id": 1491, "input_score": 5, "output_score": 8, "post_title": "What is a weak function space and what does it have to do with HOAS?"}}
{"input": "I am a little aware of various attempts to axiomize set theory within a theorem prover. Is there a standard kind of encoding of sets? An organic model to interpret set theories into? I would like to be sure my axioms have reasonable interpretations.\nSomething like this page on pure sets on nlab seems close but also seems a bit hairy.\nIt feels like these sort of encodings would be easiest with some hairy uses of functional or propositional extensionality which is mildly inconvenient.\nYou also have classical or choice principle issues. Something like CZF feels like it would be easiest to model in type theory (but hairier to axiomize.)\nFrom what I grasp of the pure set page you want something like\nInductive pregraph :=\n| sup X (f: X -> pregraph).\n\nFixpoint monic p :=\n  let 'sup X f := p in\n  f x == f y -> x = y\n  /\\ forall x, monic (f x).\n\nExisting Class monic.\n\nDefinition graph := { p: graph | monic p }.\n\nDefinition member P '(sup X ps) :=\n exists x. ps x == P.\n\nBut you need to define equivalence of pregraphs or assume some flavor of extensionality. Then you need to carve out well-founded sets. I feel like there ought to be an easier way.\nMaybe it would be nicer to first model proper classes and then model pure sets as classes which are members of other classes?", "output": "Depends on which type theory :-)\nZF set theory has been formalised in Isabelle as Isabelle/ZF by directly assuming the ZF axioms (and optionally AC). For those who prefer higher-order logic (also known as simple type theory), it's been done many times.\nDetails here and an application here. See also an impressive application of Isabelle/ZF.", "meta": {"post_id": 1525, "input_score": 4, "output_score": 10, "post_title": "Is there a \"standard\" encoding or model of material set theory in type theory?"}}
{"input": "My understanding of Coq is that Prop : Type_1, Set : Type_1, and then Type_1 : Type_2, Type_2 : Type_3, etc.\nSo, at the bottom level, the world splits into two universes: Prop (impredicative) and Set (predicative, like all the other Type_i universes). My question is: why not just have Prop : Type_0 : Type_1 : Type_2 : ..., with Prop being impredicative as it is today, and all the other levels would be predicative, also as is the case today? Wouldn't that be simpler? Would cumulatively still work?", "output": "As it often happens with Coq, the answer is historical reasons.\nIn the original version dating back from 1984, Coq was based on the Calculus of Constructions, a barebone dependent type theory. In particular, it did not feature inductive types. Instead, following the PTS tradition, it had impredicative universes.\nThe introduction of inductive types did not change this state of affair, and for a long time Coq had two impredicative universes, Prop and Set. The difference between both lied in the fact that the former was erased by extraction, but not the latter. This phase separation had been around for a long time already.\nWhen dealing with several impredicative universes, one has to be extremely careful because proofs of False lurk around the corner. In particular, Prop : Set when both are impredicative is enough be a variant of Girard's system U\u207b  and thus inconsistent. The Coq developers of yore were well aware of this issue and relied on an alternative hierarchy, so that Prop : Type and Set : Type.\nNow, at some point it was decided to make Set predicative by default, for other somewhat related reasons. Indeed, impredicative proof-relevant universes are very much inconsistent with many slightly classical principles like excluded middle in Type or some forms of choice. In particular and ironically, with an impredicative Set, Coq has no set-theoretical model (as in Polymorphism is not set-theoretic). Nowadays, Set is thus predicative except if the user opts in impredicativity with a specific flag.\nImpredicative set is essentially not used as of today, and really not tested so it has fallen to bitrot. It is extremely likely that it is not usable anymore, and there is an evergreen discussion about its removal. Yet, since the flag still exists Coq needs to at least pretend to avoid inconsistencies when the user sets it, and therefore one cannot have Prop : Set.\nIf the flag ever gets removed, maybe we can consider adding this rule, but then there might be weird backwards incompatibilities. Therefore, it is not even clear we will ever perform this change even in a situation where it is perfectly sound.", "meta": {"post_id": 1551, "input_score": 16, "output_score": 21, "post_title": "Why not have `Prop : Set` in Coq?"}}
{"input": "What are all the differences between these keywords that allow for defining top level variables?\nWhat I have noticed so far is that theorems can't be anonymous \u2014 you can write example : A := B but not theorem : A := B.", "output": "An outline of the differences in Lean 4, which you mentioned is the version you are interested in:\n\nA def is the primary way to define a named function / value in Lean. Unless its result type is Prop or it is marked noncomputable, a def will be compiled into Lean's intermediate representation (IR) in preparation for execution by Lean's interpreter and/or for further compilation into C code.\n\ndef add (x y : Nat) : Nat := x + y -- a simple computation\ndef one_plus_one_eq_two : 1 + 1 = 2 := rfl -- theorems can be defs too!\n\n\nA theorem is essentially a noncomputable def. This means that values defined by a theorem cannot be used in ordinary computation as they will not (and maybe cannot) be compiled down into executable code. Note that Lean erases types (eg., T : Type u, Prop : Type), propositions (e.g., 1 = 1 : Prop) and proofs (e.g., rfl : 1 = 1) -- all of which are technically noncomputable -- before attempting compilation, which is why they are still safe to use in computable code. Such values are termed irrelevant by the Lean compiler IR.\n\ntheorem foo : Nat := 5\ndef addFoo (x : Nat) := x + foo \n/-\nerror: \nfailed to compile definition, consider marking it as 'noncomputable' \nbecause it depends on 'foo', and it does not have executable code\n-/\n#eval foo -- same error\ntheorem addFoo' (x : Nat) : Nat := x + foo -- ok\n\n\nAn example creates an anonymous def that is type checked for correctness and then immediately discarded by Lean. Thus, an example has no lasting effect on the environment and cannot be referred to by later declarations. Note that while Lean will end up discarding the code it generates from an example, it does still, by default, generate code. Thus, you must mark an example noncomputable if it uses a noncomputable definition for a (i.e., to define a value that is not erased).\n\nexample : 1 + 1 = 2 := rfl\nexample (x y : Nat) : Nat := x + y\nexample (x : Nat) : Nat := x + foo -- same error as above\nnoncomputable example (x : Nat) : Nat := x + foo -- ok\n\nNote that there are other keywords for defining functions / values (e.g., axiom, opaque, abbrev, etc.) which I have not discussed here as you did not ask about them. Furthermore, given your choice of keywords to compare, it seemed best to focus on the named/anonymous, computable/noncomputable distinctions that separate def, theorem, and example.", "meta": {"post_id": 1575, "input_score": 8, "output_score": 8, "post_title": "What are the differences between theorem, example, def, etc?"}}
{"input": "I noticed that in Lean, the localization of rings (which is unique up to isomorphism) is defined as a predicate is_localization.\nI am not an expert in Lean, and I'm not aware of much of the discussion in the Lean community. But this has led me to wonder: Since such an approach has many benefits, why don't we use that everywhere? Is it possible to define and use is_nat, is_tensor_product, etc? If not, what are the downsides that prevent this from happening?\nOne thing that I can think of is that Lean has built in support for inductive types, so defining natural numbers or cartesian products in this way wouldn't be worth it, since all the support (pattern matching etc.) will be lost.\nAs a tangential question, how well does this approach scale with other proof assistants such as Coq and Agda?", "output": "The question is about declarative versus computational formalization.\nIn the declarative style we characterize a \u201cthing\u201d by stating conditions,\nquite often universal properties, that determine it up to a unique isomorphism. Any \u201cthing\u201d satisfying the stated conditions is acceptable.\nIn the computational style we just construct the \u201cthing\u201d concretely, and use features provided by the proof assistant to compute with it directly.\nA typical example is the structure of natural numbers. In the declarative style we define \u201cpointed successor algebra\u201d and say that $\\mathbb{N}$ is an initial pointed successor algebra. In the computational style we define $\\mathbb{N}$ as an inductive type.\nEach approach has benefits and drawbacks:\n\nThe declarative style is more flexible, but doing everything in this style complicates formalization. For example, one can never refer to \u201cthe natural numbers $\\mathbb{N}$\u201d but must always say \u201cand given an initial pointed successor algebra $N$ ...\" Now imagine you had to do this every time you wanted to mention a cartesian product, a function space, a unit type, etc.\n\nThe computational style allows us to compute more easily (although the declarative one does as well with some extra work), but it locks in specific design choices. You defined natural numbers in unary, but now you'd like to compute in binary? Tough luck, you'll have to get the proof assistant developers to hack the system and provide some magic.\n\n\nPeople tend to use the computational style, but there are situations which demand a different approach because the \u201cunique up to unique isomorphism\u201c cannot be realized as an exact equality. For example, suppose we want to define \u201cthe $n$-th power of $A$\u201d as a type? That's easily done:\nopen import Data.Nat\nopen import Data.Product\nopen import Data.Unit\n\nmodule Cow where\n\n  power : Set \u2192 \u2115 \u2192 Set\n  power A zero = \u22a4\n  power A (suc n) = A \u00d7 power A n\n\nBut what if we want $A^{n + m} = A^n \\times A^m$ to hold as an equality? This might be doable, but is at the very least going to require a lot of trickery and fragile design. And then someone is going to walk in and ask about $A^0 = 1$, and everything will crumble.\nThe Lean library defines is_localization for a similar reason. Because the \u201cequality\u201d $R[1/f][1/g] = R[1/f g]$ (or some such) cannot be achieved using any reasonable concrete construction of $R[1/f]$, we switch to declarative style as we have to deal with isomorphisms anyhow.\nIn my opinion, we are not facing just a formalization engineering problem, but rather a genuinely mathematical one. Many branches of mathematics sweep the problem under the rug by saying things like \u201cwe identify $A^{n + m}$ with $A^n \\times A^m$\u201c \u2013\u00a0but homotopy theorists and higher-category theorists thrive on it. It remains to be seen how their insights can be transformed into formalization techniques. A very good attempt is homotopy type theory and univalent mathematics, which however requires one to pay a high price \u2013\u00a0a new way of thinking \u2013 and is therefore not very popular.", "meta": {"post_id": 1629, "input_score": 6, "output_score": 8, "post_title": "Defining essentially unique objects with typeclasses"}}
{"input": "I understand that we can use W-types to encode natural numbers and a wide variety of other inductive types in intensional MLTT. Can we encode W-types using only natural numbers within type theory, potentially giving up some of the judgmental equalities, but keeping the propositional equality the same? I always thought it was the case based on this quote from nLab W-type page:\n\"In a topos with a natural numbers object (NNO), W-types for any polynomial endofunctor can be constructed as certain sets of well-founded trees; thus every topos with a NNO is a \u03a0W-pretopos. This applies in particular in the topos Set (unless one is a predicativist, in which case Set is not a topos and W-types in it must be postulated explicitly).\"\nBut I am unsure if this is a correct interpretation of the categorical concepts involved. I think the culprit here is the requirement of a topos with NNO, not just a locally cartesian closed category.\nIs this the case? Can we construct W-types from natural numbers?\nEdit: I have found a construction of W-types from natural numbers in presence of propositional resizing in Strictly Positive Types in\nHomotopy Type Theory, so I think I should specify that I want to keep the theory as predicative as possible and hopefully free of uses of K or univalence.\nEdit 2: To specify what I mean by intensional MLTT, it is the rules for $\\Pi$, $\\Sigma$, $\\mathbb{N}$, Id, 0, 1, 2, plus an infinite predicative hierarchy of universes. $U_0 : U_1$, $U_1 : U_2$ etc.", "output": "The answer seems to be no. According to Anton Setzer's PhD thesis: Proof theoretical strength of Martin-L\u00f6f Type Theory with W-type and one universe:\n\nAczel has shown in [Acz77] that Martin-L\u00f6f\u2019s type theory with one universe but no W-type has strength $| \\widehat{ID}_1 | = \\phi_{\\epsilon_0}0$.\n\nHowever:\n\nThe strength we prove, is in fact far bigger [than that shown by Palmgren], it is slightly bigger than the strength of $| KP_i|$. Until recently (work of Rathjen, [Rat90], [Rat91a], [Rat92],\nsee also [Buc93], [Sch91a], [Sch91b], [Sch92b]), $KP_i$ has been essentially the strongest theory, for which proof theoretical analysis could be carried out. For the author, $|KP_i|$ is an ordinal which seems to be an ordinal of significance similar to that of $\\Gamma_0$\n\nFurthermore:\n\nPrecisely we calculate the proof theoretical strength of intensional Russel, extensional, Tarski, and extensional Russel-version of Martin-L\u00f6f\u2019s type theory with one Universe and the W-type, $ML^i_1W_R$ , $ML^e_1W_T$ and $ML^e_1 W_R$, namely $$|ML^i_1W_R| =|ML^e_1W_T|=|ML^e_1 W_R|= |\\bigcup_{n\\in{\\mathbb N}}KPi_n^+| = \\psi_{\\Omega_1}\\Omega_{I+\\omega}.$$\n\nThis sorts out the proof theoretical strength for MLTT with a single universe.\nNow, according to Michael Rathjen's paper The strength of Martin-L\u00f6f type theory with a superuniverse (which is at least stronger than MLTT with an infinite hierarchy of universes):\n\nthe proof-theoretic ordinal of $MLS$ is $\\Phi_{\\Gamma_0}0$\n\nSo according to the wikipedia page on ordinal analysis, as pointed out by Dan Doel, which seems to suggest that\n$$\\phi_{\\epsilon_0}0 < \\Gamma_0 < \\Phi_{\\Gamma_0}0 < \\psi_{\\Omega_1}\\Omega_{I+\\omega}$$\nSo it seems that $|ML_\\omega| < |ML_1W|$ thus it cannot be possible to encode W-types in $ML_\\omega$.", "meta": {"post_id": 1674, "input_score": 7, "output_score": 12, "post_title": "Can you build W-types out of natural numbers predicatively?"}}
{"input": "all,\nI have a project in Lean which turns out to involve some features which might better be called programming. So, for that part of the project, I was thinking I would treat Lean like it is Python in a few ways. For this I was hoping to have dynamic data types. Could anyone help me by writing out the following examples, translating from pseudocode to Lean? The more explicit the better.\nThanks very much!\nINT: Dynamic natural number type\n\nINT a = 1\na = a + 1\na = 3\n\nSTRING: a dynamic string type\n\nSTRING s = \"a\"\ns = \"b\" %Now s is set to \"b\"\n\nLIST[X]: a dynamic list of type X\n\nl : LIST[STRING] = [\"a\",\"b\", \"c\"]\nl[0] = \"k\" %resets \"a\" to \"k\". The new list is [\"k\", \"b\", \"c\"]\nl.delete(1) %deletes \"b\". The new list is [\"k\", \"c\"]\nl.add(\"b\", 2) %new list is [\"k\", \"c\", \"b\"]\n\nSTRUCTURE: a dynamic class-ish type\n\nCan introduce any number of pieces of associated types like strings, lists of strings, etc.\nCan be initiated by specifying associated types\nSome associated types are default\nAccessing and modifying associated types\nDemanding proofs of various properties during initiation\n\nSTRUCTURE whose constituents have a product\n\nS : STRUCTURE\ns1 : S\ns2 : S\na : s1\nb : s2\n#check (a, b)\n\ns1 x s2\n\na : s1 x s2\n#check pi1(a)\n\ns1\n\n#check pi2(a)\n\ns2\nThanks very much for any help!", "output": "First there is no reason to put quotes around \"programming\".  Lean 4 is a fully capable programming language with all the needed features.  But it is also a (very pure) functional programming language and your question conveys some confusion both of how functional languages work and even of how Python works.\nHere is a crash course focusing on your questions.\nMutability\nIn Python, many types (e.g. int, str, tuple) are immutable, meaning you can never change an object of that type.  In Python, when you do\nx = \"Hello World!\"\nx = x.replace(\"Hello\", \"Goodbye\")\n\nyou are not changing the string \"Hello World!\", but instead creating a new string in the second line and pointing that variable x to the new string.  If the previous string is no longer referenced (as in this example), then the Python garbage collector cleans it up.  (C and Rust are different in this regard, but we don't need to worry about that.)  Lean is very similar to Python in this regard.\ndef goodbye_world : String :=\n  let x := \"Hello World!\" \n  let x := x.replace \"Hello\" \"Goodbye\" -- a new string with the same variable name as before\n  x                                    -- the value at the end of a function is what is returned\n\n#eval goodbye_world  -- Goodbye World!\n\nLists and persistent data structures\nIn Python some types are mutable (changable) like list\nx = [0, 1, 2]\ny = x        # x and y point to the same list\nx.append(3)  # change the list\nx[0] = 4     # change the list\nprint(y)     # [4, 0, 1, 2]\n\nSince in pure functional programming all types are immutable, one has to think differently.  Consider using the immutable tuple in Python instead of list:\nx = (0, 1, 2)\ny = x             # x and y point to the same tuple, but not for long\nx = x + (3,)      # x is a new tuple (0, 1, 2, 3)\nx = (0,) + x[1:]  # x is a new tuple (4, 1, 2, 3)\nprint(y)          # y never changed: (0, 1, 2)\n\nThis isn't really a problem for simple cases, but if the tuple gets too big it can lead to a lot of copying of data.\nBecause immutable structures are all there are in pure functional programming, functional programming tends to adopt persistent data structures.  These are immutable data structures where you can change a little part without having to make a copy of the whole thing.  Instead of having a list be a single block of memory, it is made up of a series of pointers (basically a linked list).  So if you change one part of the list you don't have to copy a giant block of data.  Here is a hacky version in Python, but the Lean version is cleaner:\nx = (1, (2, (3, None)))  # linked list\nhead, tail = x\ny = (4, tail)  # (4, (2, (3, None)))\n\nI've changed the first element of the linked list, but I've done so without copying the whole object.  x[1] and y[1] point to the same object in memory.  Lean's List type is built this way:\ndef example_lists : List Nat \u00d7 List Nat :=\n  let x := [0, 1, 2]\n  let y := 4 :: x\n  (x, y)   \n\n#eval example_lists  -- ([0, 1, 2], [4, 0, 1, 2])\n\nNow there is a further complexity with List in Lean.  Usually in Lean you want your functions to handle all possible cases.  So one can't just pop the head of a list since the list might be empty.  Lean4 has many ways to deal with this:\ndef l := [1, 2, 3]\ndef e := ([] : List Nat)\n\n#eval l.head (by simp)  -- 1  (Must supply a proof that l \u2260 [].)\n#eval e.head _   -- No proof for _ would make this compile.\n#eval l.head?    -- some 1  (Answer in an Option.)\n#eval e.head?    -- none    (Answer in an Option.)\n#eval l.head!    -- 1\n#eval e.head!    -- 0  (Returns default value for Nat type in #eval.  \n                 --     If run inside `main : IO unit` will give runtime error.)\n#eval l.headD 0  -- 1\n#eval e.headD 0  -- 0  (We supplied a default value.)\n\nList.head and List.head? are the most principled approaches.  List.head! is fine for simple one-off applications but your program may crash if you make a mistake.  List.headD in my opinion is more likely to lead to subtle bugs than be helpful.\nFor more list operations, see the Lean 4 source code.\nSo we can do:\ndef example_list : List Nat :=\n  let y := [0, 1, 2]\n  let y := 4 :: y.tail! -- add 4 to the front of y\n  let y := y ++ y  -- concatenate y with itself\n  y\n\n#eval example_list -- [4, 1, 2, 4, 1, 2]\n\nNote, List is meant to be used as a stack and you will get better performance operating on the left of a List than the right side.\nStructures\nLike Lists, Lean (and most other functional languages) make it easy to work with structures in a persistent way.  Often you want to change just one element of a structure.  To do that you need to make a copy of the structure and change that element.  It would be a pain to have to re-enter all the previous information again each time.  In Lean, you just have to specify which fields are changing.\nstructure Pair where\n  x : Nat\n  y : Nat\n\ndef origin : Pair := {x := 0, y := 0}\n\n#reduce origin                -- { x := 0, y := 0 }\n#reduce {origin with x := 1}  -- { x := 1, y := 0 }\n#reduce {origin with y := 1}  -- { x := 0, y := 1 }\n\nPolymorphism\nSince Lean is statically typed it has to know the type of every variable at compile time.  Nonetheless (like almost all statically typed functional programming languages) a type in Lean can depend on another type.  For example, List depends on the type of the elements in the list.  Since List (and every other type in Lean) is immutable, your can't change it, but you can make a new list with the same variable which has a different type.  This is a common use case for List.map:\ndef my_list_of_strings : List String :=\n  let x : List Nat := [0, 1, 2, 3] -- List Nat\n  let x : List Int := x.map (fun y : Nat => -y)\n  let x : List String := x.map (fun y : Int => y.repr)\n  x\n\n#eval my_list_of_strings  -- [\"0\", \"-1\", \"-2\", \"-3\"]\n\nStructures also allow polymorphism in two ways shown here:\nstructure LabeledPoints (CarrierType : Type) where\n  LabelType : Type\n  label : CarrierType -> LabelType\n\ndef my_labeled_nat : LabeledPoints Nat := { \n  LabelType := String,\n  label := fun x => x.repr\n}\n\n#check my_labeled_nat -- LabeledPoints Nat\n#eval my_labeled_nat.label 0 -- \"0\"\n\nWhether to put the type inside (e.g. LabelType) or outside (e.g. CarrierType) is largely dependent on context and use case.  (See the discussion on bundled structures.)\nType Classes\nIn a polymorphic setting everything you do must work for all possible types.  So I can make the following theorem.\ntheorem LabeledPoints.has_label\n  (T : Type) \n  (labeled_points : LabeledPoints T) \n  (t : T): \\exists l : labeled_points.LabelType, labeled_points.label = l :=\n\nand because I put it in the LabeledPoints namespace, I can use it with \"dot notation\" as follows:\n#check my_labeled_nat.has_label 0  -- Prop\n\nBut I can't construct this next function since not all types T have a string representation (like Nat did in the example above).\ndef mk_repr_labeled_points (T : Type) : LabeledPoints T := { \n  LabelType := String,\n  label := fun x => x.repr  -- this won't work since not all types T have T.repr\n}\n\nInstead, to do this, one needs type classes.  Type classes are a way that Lean (and other functional programming languages) let you define functions, structures, and other types which only depend on a certain class of types instead of all possible types.  In this next example, the type class parameter [Repr T] expresses that T must have a representative string function (which can be accessed with reprStr).\ndef mk_repr_labeled_points (T : Type) [Repr T]: LabeledPoints T := { \n  LabelType := String,\n  label := reprStr\n}\n\n#check mk_repr_labeled_points True  -- fails to typecheck\n#check mk_repr_labeled_points Int   -- LabeledPoints Int\n#eval (mk_repr_labeled_points Int).label (-1 : Int) -- \"-1\"\n\nLoops\nThe examples so far are kind of silly.  What you often want to do is keep changing a variable in a loop.  In functional programming loops are often done with recursion and you should learn it.  Here are two ways to accomplish the task of adding the squares of every number in a list:\nThis is the classic recursive approach.  (I could also make this tail recursive, but that is too advanced right now for this answer.)\ndef sum_of_squares : List Nat -> Nat\n| []     => 0\n| x :: t => x * x + sum_of_squares t\n\n#eval sum_of_squares [0, 1, 2, 3]  -- 14\n\nIterating over a List is so common, there is a function foldl just for this which makes it a one-liner.\n#eval [0, 1, 2, 3].foldl (fun x partial_sum => x * x + partial_sum) 0\n\nAdvanced Lean feature: do notation\nIf you want to do real programming, you probably want to print to STDOUT and read data from a file or from STDIN.  To do this, you have to use the IO monad.  I don't have room to go into the details, but when you are in a do block of the IO monad it looks even more like procedural programming (e.g. Python).  You can do sum_of_squares like this:\ndef sum_of_squares_io (l : List Nat) : IO Nat := do\n  let mut r := 0\n  for x in l do\n    r := r + x * x\n  return r\n\nBut note that your result will be wrapped in the IO monad.  See The Lean 4 Manual and Functional Programming in Lean for more information.\nAdvanced Lean feature: |>. notation\nInstead of let ... let ... let ... often you just want to chain your functions together into a one-liner.\n#eval \"Hello World!\".replace \"Hello\" \"Goodbye\"\n\nbut it isn't easy to add in another .replace here since .replace has arguments .  For that, Lean added the |>. notation (along with |>, and <|.)\n#eval \"Hello World!\" |>.replace \"Hello\" \"Goodbye\" |>.replace \"!\" \"?\"\n-- Goodbye World?\n\nAdvanced Lean feature: In-place changes\n(I could be a bit mistaken on what I write in this section.)  Unlike most other purely functional languages, Lean has support for non-persistent data structures without breaking the pure functional programming paradigm. For example, if you want to work with an Array or a HashMap, you can.  It works like this:  If you create an object at the same time that you dereference another object of the same Type, the Lean compiler will, instead of allocating new memory, just reuse the old memory.  For objects like Array, instead of making a copy of the array, it will just update the array in-place.  But it can be a bit tricky to get the syntax right (and unlike Rust, the Lean compiler won't help you keep track of references to be sure your only have one pointer to an array).\ndef example_array : Array Nat :=\n  let a := Array.mk [0, 1, 2, 3, 4]\n  let b := a.set! 3 10  -- makes a *copy* of a (since a is still referenced)\n                        -- and modifies element in position 3\n  let b := b.set! 4 11  -- modifies b *in-place*\n\n#eval example_array -- [0, 1, 2, 10, 11]\n\nFinal thoughts\nLearning functional programming is not a one-day activity.  It requires a new way of thinking about coding.  Lean 4 is a wonderful language with great features and syntax choices, but it is still very different from procedural languages like Python.  Expect a learning curve.  For learning more I would recommend:\n\nFunctional Programming in Lean (in progress)\nThe Lean 4 Manual (in progress)\nTheorem Proving in Lean 4 (The most comprehensive Lean 4 manual so far.  While it focuses on theorem proving, the sections on induction and recursion, and on structures would be especially applicable to programming.)\nA good tutorial or course for another functional language, like Haskell or Scala", "meta": {"post_id": 1701, "input_score": -1, "output_score": 10, "post_title": "A project in Lean which involves \"programming\""}}
{"input": "I guess my question can be reduced to implementing this function:\ndef abEq (A B : Type) (a: A) (ab : A = B): B := sorry\n\nI am new to Lean 4 and started learning from the official tutorial, currently I am at the chapter 7 (Inductive types).\nI already know that substituting things in propositions given a certain equality is done using Eq.subst, and Eq.subst ab a is among the things I tried. My problem is that the type assertion a: A is not a proposition, and I have no idea how to proceed.\n\nWhy I am trying to do this\nPerhaps I am suffering from an XY problem, so I thought including the following might help.\nIn Lean, I am trying to define what I would define in set theory like this:\nA signature is a pair $(Op, ar)$, where\n\n$Op$ is an arbitrary set of operations, and\n$ar\\colon Op \\to Nat$ a function giving their arities.\n\nLet $Sig_{Op}$ be the set of all signatures with $Op$ as the set of operations, then\n\\begin{align}\narity\\colon Sig_{Op} &\\to Op \\to Nat \\\\\n(Op, ar) &\\mapsto op \\mapsto ar(op)\n\\end{align}\nreturns the arity of the operator $op$ in the signature $(Op, ar)$.\nI came up with this:\ndef signature := (Op: Type) \u00d7 (Op \u2192 Nat)\n\ndef sigOp (Op: Type) := { s: signature // s.fst = Op }\n\ndef arity {Op: Type} (op: Op) (s: sigOp Op): Nat := ...\n\nbut I am stuck implementing the function arity. s.val.snd op does not work, because s.val.snd expects an argument of type s.val.fst, while op has type Op. The value s carries a proof s.property: s.val.first = Op, but I don't know how to apply it. Is my problem in that I just don't know how to implement arity, or are my Lean definitions themselves wrong and I need to change them? If so, how?", "output": "Indeed, you are experiencing the XY problem. You are not taking the advantage of dependent types, which you can do as shown below.\nRegarding your problem X: one just has to perform a transport across equality, but in a universe:\ndef abEq (A B : Type) (a : A) (ab : A = B): B :=\n  @eq.rec Type A (\u03bb (X : Type), X) a B ab \n\nNamely, suppose we have:\n\na type U\nelement u : U\na type family P : U \u2192 Type\nan element x : P u\nan element v : U\nequality e : u = v\n\nThen eq.rec U u P x v e will produce an element of P v. We say that we transported x : P u to P v along e. If we set U := Type, u := A, P := \u03bb X, X, x := a, v := B and e := ab then we get your example.\nNow let's get onto problem Y.\nI am including a fairly elaborate example of how to go about formalizing universal algebra because the classical treatment of universal algebra is not the right way to formalize the topic, but I see people falling into the trap repeatedly. In particular, one should not use natural numbers as arities \u2013 that creates a host of unecessary complications.\n-- A signature has a type of operation symbols, and each symbol\n-- has an arity. Improtantly, the arity is a type, not a number!\n-- That is, if we want a symbol to have arity 3 we do not use\n-- the number 3 but instead (any) type with 3 elements.\nstructure signature :=\n  (symbol : Type) -- the type of operation symbols\n  (arity : symbol \u2192 Type) -- the arities of symbols\n\nnamespace arity\n  -- we define arities for constants, unary and binary symbols,\n  -- as these are most common.\n\n  inductive const : Type\n\n  inductive unary : Type\n    | u1 : unary -- the index of the only argument\n\n  open unary\n\n  inductive binary : Type\n    | b1 : binary -- the index of the first argument\n    | b2 : binary -- the index of the second argument\n\n  open binary\nend arity\n\n-- the type of algebras for a given signature,\n-- note that each symbol op is interpreted as a map which\n-- takes a function (args : S.arity op \u2192 carrier) and returns\n-- an element of the carrier. If the type S.arity op has\n-- n elements, then args is (equivalent to) an n-tuple,\n-- as it should be.\nstructure algebra (S : signature) :=\n  (carrier : Type) -- the underlying carrier\n  (act : \u03a0 (op : S.symbol), (S.arity op \u2192 carrier) \u2192 carrier)\n\n-- Example: the free algebra generated by a signature S and a set X of generators\n\n-- the carrier of the free algebra is the type of trees whose leaves are\n-- the generators and the nodes are operation symbols\ninductive tree (Leaf : Type) (Node : Type) (arity : Node \u2192 Type) : Type\n| leaf : Leaf \u2192 tree -- the injection of generators into the tree algebra\n| node : \u03a0 (t : Node), (arity t \u2192 tree) \u2192 tree\n\n-- the free algebra on S generated by X\ndef free (S : signature) (X : Type) : algebra S :=\n  { carrier := tree X S.symbol S.arity,\n    act := tree.node }\n\n-- the type of homomorphisms between two S-algebras\nstructure hom {S : signature} (X : algebra S) (Y : algebra S) :=\n  (map : X.carrier \u2192 Y.carrier)\n  (is_hom : \u03a0 (op : S.symbol) (args : S.arity op \u2192 X.carrier),\n              map (X.act op args) = Y.act op (map \u2218 args) )\n\n-- composition of homomorphisms is a homomorphism\ndef hom_compose {S : signature} {X Y Z : algebra S}\n                (g : hom Y Z) (f : hom X Y) : hom X Z :=\n  { map := g.map \u2218 f.map,\n    is_hom := begin intros op args, simp [g.is_hom, f.is_hom] end\n  }\n\n-- example: the signature of a group\n\n-- the type of symbols of a group\ninductive group_symbol : Type\n  | uni : group_symbol -- unit\n  | inv : group_symbol -- inverse\n  | mul : group_symbol -- multiplication\n\nopen group_symbol\n\ndef group : signature :=\n  { symbol := group_symbol,\n    arity := \u03bb op, match op with\n                   | uni := arity.const\n                   | inv := arity.unary\n                   | mul := arity.binary\n                  end }\n\n-- the group Z/3Z\n\n-- the carrier of Z/3Z\ninductive Z3_carrier : Type\n  | z0 : Z3_carrier\n  | z1 : Z3_carrier\n  | z2 : Z3_carrier\n\nopen Z3_carrier\n\n-- the inverse operation in Z/3Z\ndef Z3_inv : Z3_carrier \u2192 Z3_carrier\n  | z0 := z0\n  | z1 := z2\n  | z2 := z1\n\n-- addition in Z/3Z\ndef Z3_add : Z3_carrier \u2192 Z3_carrier \u2192 Z3_carrier\n  | z0 z0 := z0\n  | z0 z1 := z1\n  | z0 z2 := z2\n  | z1 z0 := z1\n  | z1 z1 := z2\n  | z1 z3 := z0\n  | z2 z0 := z2\n  | z2 z1 := z0\n  | z2 z2 := z1\n\n-- the group Z/3Z\ndef Z3 : algebra group :=\n  { carrier := Z3_carrier,\n    act := \u03bb op,\n             match op with\n             | uni := \u03bb args, z0\n             | inv := \u03bb args, Z3_inv (args arity.unary.u1)\n             | mul := \u03bb args, Z3_add (args arity.binary.b1) (args arity.binary.b2)\n             end\n  }", "meta": {"post_id": 1740, "input_score": 11, "output_score": 9, "post_title": "Given `A = B`, how to prove `a: A` also has type B in Lean 4"}}
{"input": "I saw:\n\nsubsingleton elimination from lean-forward, which, I so far understood as \"eliminate a type in Prop to a type in whatever universe that we know has at most one constructor with arguments either in Prop or also subsingletons\".\nfunction comprehension from XTT, $\\S 8.2.1$, which corresponds to the statement that (paraphrased) if \u2200 (a : A), \u2203 (b : B), R a b \u2227 b unique, then \u2203 f : A -> B, it holds that \u2200 (x : A), R x (f x). If A lives in Type then this is trivial, but when A : Prop then this becomes more interesting: we intend to erase propositions, so we cannot have their computational content relevant, so only when we know b unique do we say \u2200 x, f x = b.\n\nI wonder are these concepts the same thing? Is there any slight difference I didn't notice? They're both about eliminating Prop into some uniquely inhabited Type.", "output": "Indeed, these two notions are about what we allow to eliminate from Prop to Type, and they both roughly follow the idea that \"using propositions to build relevant content is fine as long as there is but one way to use it\". But they are quite different.\nSubsingleton elimination (in the way this term is usually used) is a syntactic/structural criterion on inductive types, that tells you for which such inductive types pattern-matching/eliminators is allowed with a return type in Type. This criteria is designed so that there is at most one canonical form of this type, so that there is definitionally at most one inhabitant in the empty context (if canonicity holds).\nFunction comprehension, on the other hard, is a semantic criterion, which allows to construct a relevant function out of irrelevant content, provided one can show internally that there is but one possible such function. The paradigmatic example of this idea is what the HoTT Book calls principle of unique choice (section 3.9, where they also mention what you call function comprehension as corollary).\nThe important difference, as highlighted, is that the first notion is syntactic, while the second is semantic. This means that subsingleton elimination is necessarily more restrictive than unique choice/function comprehension, but it has the advantage that it can be checked mechanically, and that in practice it is nice to avoid the whole going-through-unique-choice yoga when you want to eliminate a Prop inductive into Type. Moreover, as pointed out by the people working on strict propositions (see my answer to a previous question), the subsingleton criterion might need to be refined/amended depending on the exact kind of propositions you want.", "meta": {"post_id": 1741, "input_score": 5, "output_score": 9, "post_title": "Is 'subsingleton elimination' the same concept as 'function comprehension'?"}}
{"input": "How can we define in a proof assistant (eg., Coq) a notion of a 2-player game, where each player is a program that has access to the opponent's source code?\nBackground: In Open Source Game Theory, we think about bots that reason about each other (prove theorems), knowing the other's source code. I would like implement such bots in a proof assistant, if possible. (https://arxiv.org/pdf/2208.07006.pdf, Open Problem 4)\nWhat I have in mind would looks like this:\nInductive outcome := Cooperate | Defect.\n(*  Dupoc is short for Defect Unless Proof of Cooperation   *)\nDefinition Dupoc (n:N) (bot2 : N -> bot -> outcome) : N -> bot -> outcome :=\n    'try for n steps to find a proof that bot2 n self = Cooperate, if found':\n        return Cooperate\n    else\n        return Defect.\n\nThe point is that we want to run the agents together with each other as input:\nDefinition game n1 bot1 n2 bot2 := (bot1 n1 bot2, bo2 n2 bo1).\n\nOf course, the type bot does not exist as I have written it. Is there a way to actually make this happen?\nThe other problem is that the bot's function is trying to find a proof and returning a bool that reflects the success. Is this possible in some proof assistant? The proof assistant would have to have access to its own inner workings somehow, which seems hard.\nI think I could solve the problem by having a Coq implementation in Coq.\nThen I could have, like in the paper:\nInductive CoqTokens := ...\nDefinition BotExpression := ... (* A valid expression for a Bot *)\nClass Bot := {\n    play : BotExpression -> outcome,\n    source : BotExpression (* This should be the source code for 'play' in our Coq-in-Coq implementation *)\n}.\nDefinition game bot1 bot2 := (bot1.play bot2.source, bot2.play bot1.source).\n\nBut this seems very much work if it is possible. The Coq implementation in Coq still needs to contain everything about proofing. And I am not sure if it actually solves the problem.", "output": "As you found out, the usual technique to solve this kind of difficulty is to introduce a notion of \"code\" (what you called BotExpression), which expresses the behaviour of your bot. Once you are happy with your codes, you can in a second time define a \"decoding\" function, something which takes in a code and spits out the expression, but this time as an actual Coq function. Taking the notations from your question, this means that you should be able to define a function that takes in source and outputs play. Note that to do this you do not have to reify all the syntax of Coq. Indeed, the actions available for your bots can probably be defined with a set of primitives smaller than full-fledged Coq. This lets you avoid the problem of using some form of self-description of the meta-language (which are very complicated beasts).\nSimilarly, you might also want to have such an encoding for proofs, again because your bots need to manipulate \"concrete\" data rather than meta-proofs in the ambient theory. Regarding formalization of logic, and also to give you ideas on the general code/decoding setting, you might want to give a look at eg. this library formalizing first-order logic.\nFinally, there is a second issue you might stumble upon, either in the definition of your decoding function, or that of a play, which is related to the definition of functions in Coq. Indeed, as Coq lets you only define total/terminating functions, quite a lot of things with a \"self-recursive\" flavour are not definable as functions, as they would lead to non-terminating behaviour. In such a case, a standard solution is to replace the type A -> B by the type A -> B -> Prop, ie. replace an actual function by its graph. This avoids termination issues, and also determinism: since you do not encode a game as a function, you can have multiple choices to move on. Moreover, if you define your relation as an inductive one, you can reason by induction on it, which is probably the kind of reasoning principles you will be after.", "meta": {"post_id": 1779, "input_score": 10, "output_score": 9, "post_title": "How can we formalize a game where each player is a program having access to opponent's code?"}}
{"input": "In Zermelo-Fraenkel axiomatics, one does not need to use the axiom of choice to resolve unique existential quantifiers, the axiom of replacement is enough for this.\nBut it seems that Type theory does not allow that: if you have a family of singletons, there is no way to get the family of its members.\nIf it is true, that can be proven by constructing a model of Type theory in which this does not hold.\nI suspect that \u201chomotopic\u201d models can be useful there \u2014 there are families of contractible spaces which have no section, but I have no clue about how to do it for real.", "output": "Let $\\exists! z \\in C . P(z)$ be shorthand for $\\exists z \\in C . (P(z) \\land \\forall w \\in C . P(w) \\to z = w)$.\nYour question is related to the axiom of unique choice (AUC): given a relation $R : A \\to B \\to \\mathsf{Prop}$ satisfying\n$$\\forall x \\in A . \\exists! y \\in B . R(x, y), \\tag{1}$$\nis there a map $f : A \\to B$ such that $\\forall x \\in A . R(x, f x)$? Indeed, $R$ can be thought of as a singleton in $B$ parameterized by $A$, and $f$ is the unique element of $B$ (still parameterized by $A$).\nA relation satisfying (1) is called a functional relation.\nAnother way to state AUC is this: the graph of $f : A \\to B$ is the relation $\\Gamma_f : A \\to B \\to \\mathsf{Prop}$ defined by\n$$\\Gamma_f \\, a\\, b \\mathrel{{:}{=}} (f a = b).$$\nThen AUC states that every functional relation is the graph of a function.\nWhether AUC holds depends on how precisely we set up foundations.\nIn toposes AUC holds, so in particular it holds in ZFC and other set theories.\nIn Martin-L\u00f6f type theory AUC holds if we write (1) as\n$$\\textstyle\n\\prod_{x : A}\n(\\sum_{y : B}\nR(x,y) \\times \\prod_{z : B} R(x,z) \\to \\mathsf{Id}(y,z))\n$$\nThat's because we used $\\Sigma$ in place of $\\exists$, so we can extract the witness $y$. (Of course, for the same reason MLTT validates the full axiom of choice.)\nIn homotopy type theory AUC holds. Note that\n$$\n\\textstyle\n\\exists z \\in C . (P(z) \\land \\forall w \\in C . P(w) \\to z = w)\n$$\nis defined to be\n$$\n\\textstyle\n\\|\\sum_{z : C} (P(z) \\times \\prod_{w : C} P(w) \\to z = w)\n\\|\n$$\nBecause the type inside the truncation is a proposition, we may apply Lemma 3.9.1 of HoTT book to extract $z : C$.\nIn the Calculus of Inductive Constructions, i.e., the type theory of Coq, AUC does not hold when we use $\\mathsf{Prop}$. This is so because we cannot eliminate from the proposition $\\exists! y \\in B . R(x, y)$ to $B$, unless $B$ itself is a proposition.\nSo the question really is: which formalism are you using?", "meta": {"post_id": 1783, "input_score": 5, "output_score": 8, "post_title": "Does one need a type-theoretical axiom of choice for singletons?"}}
{"input": "This is somewhat conceptual beginner's question about proof assistants.\nI've been re-reading the famous Seven Trees in One / Objects of categories as complex numbers. The gist: The type $T$ of binary trees admits an explict, combinatorial ismorphism $T \\cong T^7$.\nOne way to see this is that in every commutative semiring, the equation $x = x^2+1$ implies $x = x^7$ (straightforward equational reasoning). We can then construct what the second paper calls the Burnside rig, the commutative semiring of isomorphism classes of types. Because we have an isomorphism $T \\cong T^2+1$, the equivalence class $[T]$ in this semiring satisfies $[T] = [T]^7$, hence $T \\cong T^7$.\nThis suggests to me a nice modular way to construct the seven-trees-in-one isomorphism\n\nFormalize the Burnside rig\nuse an existing semiring tactic to solve the equational reasoning part\nextract the isomorphism\n\nFor the sake of this question, I will take it as a given that the appropriate semiring tactic exists and can solve (x = x^2 + 1) -> (x = x^7). (I know ring tactics exists, but semiring is slightly more subtle).\nMy question is thus about the remaining aspects of the formalization. How would we go about this problem in different proof assistants (Coq, Agda, Lean)?\n\nLean has quotients. Can we form the Burnside rig as is? Will we still be able to compute the isomorphism?\nWould Coq require that its ring tactic does not use vanilla = but setoids?\nAm I correct to think that under univalence, universes are commutative semirings on the nose? Does this simplify the proof, say in Cubical Agda? I've enjoyed Computing with Semirings and Weak Rig Groupoids but can it be used here in practical terms?", "output": "The main subtlety is that it doesn't seem easy to automate the semigroup equational reasoning required by Seven Trees in One, but if we put that process aside (like, accepting to do it by hand, which could still look decent if just associativity and commutativity were handled automatically), I would expect the rest to go as you would expect. I don't know about Lean's quotient, but your given approaches for Coq (generalizing up to proof-relevant equivalence relations) and Agda (using univalence) seem reasonable.\nHere's an implementation in Agda, though it does not extract a completely working bijection: one direction generates many applications of transp that Agda doesn't want to reduce for some reason. This might also affect the other direction, but I haven't found an example in my limited number of tests. If you tell it what the right normal term is, it is accepted (thanks to Dan Doel for pointing this out!); that slightly defeats the point of the exercise, hence it's \"not completely working\".\nA neat trick is that you can define the quotient \u2115[X]/(X\u2261X\u00b2+1) directly as a higher inductive type and prove the equation X^7=X in that concrete structure once and for all instead of parameterizing the proof by an abstract semiring. Initiality (aka. \"fold\") then transports that equation to other semirings with an element satisfying x = x^2+1.\n-- 1. Prove X\u2077\u2261X in the polynomial quotient semiring \u2115[X]/(X\u2261X\u00b2+1)\n-- 2. Define a map toType : \u2115[X]/(X\u2261X\u00b2+1) \u2192 Type,\n--    using the semiring structure of Type and the fact that\n--    the equation T\u2261T\u00b2+1 holds for the type of binary trees T,\n--    thanks to univalence.\n-- 3. We deduce T\u2077 \u2261 T, by T\u2077 \u2261 toType X\u2077 \u2261 toType X \u2261 T\n-- 4. ???\n-- 5. Profit\n\n{-# OPTIONS --cubical #-}\nmodule S where\n\nopen import Cubical.Core.Everything\nopen import Cubical.Foundations.Isomorphism\nopen import Cubical.Foundations.Univalence\nopen import Cubical.Foundations.Structure\nopen import Cubical.Foundations.Prelude\nopen import Cubical.Foundations.Function\nopen import Cubical.Foundations.Transport\nopen import Cubical.Foundations.HLevels\nopen import Cubical.Data.Sigma\nopen import Cubical.Data.Sum\nopen import Cubical.Data.Unit\nopen import Cubical.Data.Nat using (\u2115; zero; suc)\n\npattern 3+_ n = suc (suc (suc n))\npattern 2+_ n = suc (suc n)\npattern 1+_ n = suc n\n\ninfixl 6 _+_\n\n-- * Part 1: X \u2261 X\u00b2+1 \u2192 X\u2077 \u2261 X\n--\n-- (X\u2077\u2261X is the name of the final theorem.)\n-- The proof follows Seven Trees in One (p 6) https://arxiv.org/pdf/math/9405205.pdf\n--\n-- X^7 = X^7 + X^4 + X = X\n--     by lemma X^(1+r) = X^(1+r) + X^(k+3) + X^k\n--     used twice, with r=6,k=1 and with r=0,k=4.\n--     (here we only give ad hoc proofs for those two instances)\n\n-- The semiring \u2115[X]/(X\u2261X\u00b2+1)\n-- This actually only requires a minimal set of axioms (weaker than semirings)\n-- to prove X\u2077\u2261X.\ndata \u2115[X]/tree : Type where\n  _+_ : \u2115[X]/tree \u2192 \u2115[X]/tree \u2192 \u2115[X]/tree\n  X^ : \u2115 \u2192 \u2115[X]/tree\n  +-comm : {a b : \u2115[X]/tree} \u2192 a + b \u2261 b + a\n  +-assoc : {a b c : \u2115[X]/tree} \u2192 a + b + c \u2261 a + (b + c)\n  tree : \u2200 n \u2192 X^ (1+ n) \u2261 X^ (2+ n) + X^ n\n\ntree\u02b3 : \u2200 {a} n \u2192 a + X^ (suc n) \u2261 a + X^ (suc (suc n)) + X^ n\ntree\u02b3 {a} n =\n  a + X^ (suc n) \u2261[ i ]\u27e8 a + tree n i \u27e9\n  a + (X^ (suc (suc n)) + X^ n) \u2261\u27e8 sym +-assoc \u27e9\n  a + X^ (suc (suc n)) + X^ n \u220e\n\n-- X^ (n + i) + ... + X^ n (left associated)\nX^[_+_] : \u2115 \u2192 \u2115 \u2192 \u2115[X]/tree\nX^[ n + suc i ] = X^[ suc n + i ] + X^ n\nX^[ n + zero ] = X^ n\n\nX\u00b2\u2261X\u00b2+X\u00b3+1 : \u2200 {a} n \u2192 a + X^ (2+ n) \u2261 a + X^ (2+ n) + X^ (3+ n) + X^ n\nX\u00b2\u2261X\u00b2+X\u00b3+1 {a} n =\n  a + X^ (suc (suc n)) \u2261\u27e8 tree\u02b3 (suc n) \u27e9\n  a + X^ (suc (suc (suc n))) + X^ (suc n) \u2261\u27e8 tree\u02b3 n \u27e9\n  a + X^ (suc (suc (suc n))) + X^ (suc (suc n)) + X^ n \u2261\u27e8 cong (_+ X^ n) +-assoc \u27e9\n  a + (X^ (suc (suc (suc n))) + X^ (suc (suc n))) + X^ n \u2261\u27e8 cong (\u03bb x \u2192 a + x + X^ n) +-comm \u27e9\n  a + (X^ (suc (suc n)) + X^ (suc (suc (suc n)))) + X^ n \u2261\u27e8 cong (_+ X^ n) (sym +-assoc) \u27e9\n  a + X^ (suc (suc n)) + X^ (suc (suc (suc n))) + X^ n \u220e\n\nX\u2077\u2261X\u2077+X\u2074+X : X^ 7 \u2261 X^ 7 + X^ 4 + X^ 1\nX\u2077\u2261X\u2077+X\u2074+X =\n  X^ 7 \u2261\u27e8 expand-X\u2077 \u27e9\n  X^[ 5 + 3 ] + X^ 3 \u2261\u27e8 X\u00b2\u2261X\u00b2+X\u00b3+1 1 \u27e9\n  X^[ 5 + 3 ] + X^ 3 + X^ 4 + X^ 1 \u2261[ i ]\u27e8 expand-X\u2077 (~ i) + X^ 4 + X^ 1 \u27e9\n  X^ 7 + X^ 4 + X^ 1 \u220e\n  where\n    expand-X\u2077 : X^ 7 \u2261 X^[ 5 + 3 ] + X^ 3\n    expand-X\u2077 =\n                    X^ 7 \u2261\u27e8 tree 6 \u27e9\n      X^ 8        + X^ 6 \u2261\u27e8 tree\u02b3 5 \u27e9\n      X^[ 7 + 1 ] + X^ 5 \u2261\u27e8 tree\u02b3 4 \u27e9\n      X^[ 6 + 2 ] + X^ 4 \u2261\u27e8 tree\u02b3 3 \u27e9\n      X^[ 5 + 3 ] + X^ 3 \u220e\n\ntree\u02e1 : \u2200 {a} n \u2192 X^ (suc n) + a \u2261 X^ (suc (suc n)) + (X^ n + a)\ntree\u02e1 {a} n =\n  X^ (suc n) + a \u2261[ i ]\u27e8 tree n i + a \u27e9\n  X^ (suc (suc n)) + X^ n + a \u2261\u27e8 +-assoc \u27e9\n  X^ (suc (suc n)) + (X^ n + a) \u220e\n\nX\u00b3+1+X\u2261X : \u2200 {a} n \u2192 X^ (3+ n) + X^ n + (X^ (1+ n) + a) \u2261 X^ (1+ n) + a\nX\u00b3+1+X\u2261X {a} n =\n  X^ (3+ n) + X^ n + (X^ (1+ n) + a) \u2261\u27e8 cong (_+ (X^ (1+ n) + a)) +-comm \u27e9\n  X^ n + X^ (3+ n) + (X^ (1+ n) + a) \u2261\u27e8 sym +-assoc \u27e9\n  X^ n + X^ (3+ n) + X^ (1+ n) + a \u2261\u27e8 cong (_+ a) (sym (tree\u02b3 (1+ n))) \u27e9\n  X^ n + X^ (2+ n) + a \u2261\u27e8 cong (_+ a) +-comm \u27e9\n  X^ (2+ n) + X^ n + a \u2261\u27e8 cong (_+ a) (sym (tree n)) \u27e9\n  X^ (1+ n) + a \u220e\n\nX\u2077+X\u2074+X\u2261X : X^ 7 + X^ 4 + X^ 1 \u2261 X^ 1\nX\u2077+X\u2074+X\u2261X =\n  X^ 7 + X^ 4 +  X^ 1      \u2261[ i ]\u27e8 X^ 7 + X^ 4 + expand-X i \u27e9\n  X^ 7 + X^ 4 + (X^ 5 + _) \u2261\u27e8 X\u00b3+1+X\u2261X 4 \u27e9\n                 X^ 5 + _  \u2261\u27e8 sym expand-X \u27e9\n                 X^ 1 \u220e\n  where\n    expand-X : X^ 1 \u2261 X^ 5 + _\n    expand-X =\n      X^ 1     \u2261\u27e8 tree 0 \u27e9\n      X^ 2 + _ \u2261\u27e8 tree\u02e1 1 \u27e9\n      X^ 3 + _ \u2261\u27e8 tree\u02e1 2 \u27e9\n      X^ 4 + _ \u2261\u27e8 tree\u02e1 3 \u27e9\n      X^ 5 + (X^ 3 + (X^ 2 + (X^ 1 + X^ 0))) \u220e\n\nX\u2077\u2261X : X^ 7 \u2261 X^ 1\nX\u2077\u2261X =\n  X^ 7               \u2261\u27e8 X\u2077\u2261X\u2077+X\u2074+X \u27e9\n  X^ 7 + X^ 4 + X^ 1 \u2261\u27e8 X\u2077+X\u2074+X\u2261X \u27e9\n  X^ 1 \u220e\n\n-- * Part 2: Tree as a solution of the equation X \u2261 X\u00b2+1\n--\n-- (toType is the name of the final theorem.)\n-- (univalence is used via isoToPath)\n\ndata Tree : Type where\n  Leaf : Tree\n  Node : Tree \u2192 Tree \u2192 Tree\n\n\u228e-comm : \u2200 {A B : Type} \u2192 A \u228e B \u2261 B \u228e A\n\u228e-comm = isoToPath \u228e-swap-Iso\n\n\u228e-assoc : \u2200 {A B C : Type} \u2192 (A \u228e B) \u228e C \u2261 A \u228e (B \u228e C)\n\u228e-assoc = isoToPath \u228e-assoc-Iso\n\nopen Iso\n\nIso-Tree-tree : (A : Type) \u2192 Iso (Tree \u00d7 A) ((Tree \u00d7 Tree \u00d7 A) \u228e A)\nIso-Tree-tree A .fun (Node t\u2081 t\u2082 , a) = inl (t\u2081 , t\u2082 , a)\nIso-Tree-tree A .fun (Leaf , a) = inr a\nIso-Tree-tree A .inv (inl (t\u2081 , t\u2082 , a)) = Node t\u2081 t\u2082 , a\nIso-Tree-tree A .inv (inr a) = Leaf , a\nIso-Tree-tree A .rightInv (inl (t\u2081 , t\u2082 , a)) = refl\nIso-Tree-tree A .rightInv (inr a) = refl\nIso-Tree-tree A .leftInv (Node t\u2081 t\u2082 , a) = refl\nIso-Tree-tree A .leftInv (Leaf , a) = refl\n\nTree-tree : (A : Type) \u2192 (Tree \u00d7 A) \u2261 ((Tree \u00d7 Tree \u00d7 A) \u228e A)\nTree-tree A = isoToPath (Iso-Tree-tree A)\n\nTree^ : (n : \u2115) \u2192 Type\nTree^ 0 = Unit\nTree^ (1+ n) = Tree \u00d7 Tree^ n\n\n-- This is essentially a way to define a semiring using a HIT,\n-- although this particular formulation only covers the substructure\n-- generated by Tree.\n-- Under univalence (which is used by isoToPath), Type is a semiring.\ntoType : \u2115[X]/tree \u2192 Type\ntoType (a + b) = toType a \u228e toType b\ntoType (X^ n) = Tree^ n\ntoType (+-comm {a} {b} i) = \u228e-comm {toType a} {toType b} i\ntoType (+-assoc {a} {b} {c} i) = \u228e-assoc {toType a} {toType b} {toType c} i\ntoType (tree n i) = Tree-tree (toType (X^ n)) i\n\n-- * Part 3: X\u2077\u2261X \u21d2 toType X\u2077 \u2261 toType X \u21d2 Tree\u2077 \u2261 Tree \u21d2 Iso Tree\u2077 Tree\n\nIso-Tree\u2077-Tree : Iso (Tree^ 7) Tree\nIso-Tree\u2077-Tree =\n  Tree^ 7  Iso\u27e8 pathToIso (\u03bb i \u2192 toType (X\u2077\u2261X i)) \u27e9\n  Tree \u00d7 Unit  Iso\u27e8 rUnit\u00d7Iso \u27e9\n  Tree \u220eIso\n\n-- * Examples\n\nf = Iso-Tree\u2077-Tree\n\n_ : fun f (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt) \u2261 Leaf\n_ = refl\n\n_ : fun f (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Node Leaf Leaf , tt)\n  \u2261 Node Leaf Leaf\n_ = refl\n\n_ : fun f (Node Leaf Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n  \u2261 Node (Node  (Node (Node (Node (Node (Node Leaf Leaf) Leaf) Leaf) Leaf) Leaf)  Leaf) Leaf\n_ = refl\n\n_ : fun f (Node Leaf Leaf , Node Leaf Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n  \u2261 Node (Node (Node (Node (Node (Node (Node Leaf Leaf) (Node Leaf Leaf)) Leaf) Leaf) Leaf)  Leaf) Leaf\n_ = refl\n\n_ : inv f Leaf \u2261 (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , tt)\n_ = refl -- When you ask Agda to normalize this goal, it gets stuck on transp, lots of transp. But refl still typechecks.\n\n_ : inv f (Node Leaf Leaf) \u2261 (Leaf , Leaf , Leaf , Leaf , Leaf , Leaf , Node Leaf Leaf , tt)\n_ = refl\n\n\nCode on Gist: https://gist.github.com/Lysxia/24bc08e7c95aa5594d51e70b9d3842eb", "meta": {"post_id": 1814, "input_score": 11, "output_score": 8, "post_title": "Seven Trees in One, or How to formalize the Semiring of Types?"}}
{"input": "I was messing around with toy lemmas in Idris and came up with this silly proof.\n%default total\n\ndata Dumb : Nat -> Type where\n  Way : Dumb Z\n  NoWay : Dumb (S k) -> Dumb k\n\nnotDumb : Dumb (S k) -> Void\nnotDumb (NoWay d) = notDumb d\n\nAnd in Agda...\ndata Dumb : \u2115 -> Set where\n  way : Dumb 0\n  no_way : {k : \u2115} -> Dumb (suc k) -> Dumb k \n\nnot_dumb : {k : \u2115} -> Dumb (suc k) -> \u22a5\nnot_dumb (no_way d) = not_dumb d\n\nPretty straightforward. But then when I tried it in Coq, it became a bit tricky. The only way I could figure out how to do it was with dependent induction.\nRequire Import Coq.Program.Equality.\n\nInductive dumb: nat -> Prop :=\n  | way : dumb 0\n  | noway (k: nat): dumb (S k) -> dumb k.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros. intro.\n  dependent induction H.\n  apply (IHdumb (S k)).\n  auto.\nQed.\n\nAnd the proof term is pretty complicated.\nnot_dumb = \nfun k : nat =>\n(fun H : dumb (S k) =>\n (fun (gen_x : nat) (H0 : dumb gen_x) =>\n  (fun H1 : dumb gen_x =>\n   dumb_ind\n     (fun gen_x0 : nat =>\n      forall k0 : nat, gen_x0 = S k0 -> block (block False))\n     (fun (k0 : nat) (H2 : 0 = S k0) =>\n      False_ind (block (block False))\n        (let H3 : False :=\n           eq_ind 0\n             (fun e : nat =>\n              match e with\n              | 0 => True\n              | S _ => False\n              end) I (S k0) H2 in\n         False_ind False H3))\n     (fun (k0 : nat) (H2 : dumb (S k0))\n        (IHdumb : forall k1 : nat, S k0 = S k1 -> block (block False))\n        (k1 : nat) (H3 : k0 = S k1) =>\n      solution_left nat\n        (fun x : nat =>\n         dumb (S x) ->\n         (forall k2 : nat, S x = S k2 -> block (block False)) ->\n         block (block False)) (S k1)\n        (fun (_ : dumb (S (S k1)))\n           (IHdumb0 : forall k2 : nat,\n                      S (S k1) = S k2 -> block (block False)) =>\n         IHdumb0 (S k1) eq_refl : block False) k0 H3 H2 IHdumb) gen_x\n     H1) H0 k) (S k) H eq_refl)\n:\n~ dumb (S k)\n     : forall k : nat, ~ dumb (S k)\n\nSo the questions are:\n\nWhy are these proofs so different? Is there something fundamental about languages like Idris and Agda which make this sort of proof simpler? Is there something about Coq that makes this kind of proof hard? My sense is it has something to do with pattern matching, but it would be great if someone who knows more about pattern matching in Idris or Agda could verify this.\n\nWhat's going on in this Coq proof? Is it ultimately doing the same thing as what is being done in the Idris/Agda case? Is it just that Coq's totality checker is doing less work?\n\nIs there actually a very simple Coq proof of this? And the entire issue is moot?\n\n\nThanks, anything helps.\n\nFollow up. Thanks for the nice responses. They made me realize you can do this directly with the induction principle, albeit not using tactics.\nDefinition is_zero (n : nat) : Prop :=\n  match n with\n  | 0 => True\n  | (S _) => False\n  end.\n\nLemma dumb_is_zero : forall k, dumb (S k) -> is_zero (S k) -> is_zero k.\nProof.\n  intros.\n  destruct k.\n  - exact I.\n  - assumption.\nQed.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros k d.\n  exact (dumb_ind is_zero I dumb_is_zero (S k) d).\nQed.\n\nI'm still curious though, is there any way of generating the proof using tactics?\n\nI guess ultimately the point is that I really want to prove\nLemma not_dumb: forall k, dumb (S k) -> is_zero (S k).\nProof.\n  intros k d.\n  induction d.\n  - exact I.\n  - inversion IHd.\nQed.\n\nThanks again.", "output": "There is a coq proof which doesn't require dependent induction:\nLemma dumb0 : forall k, dumb k -> k = 0.\nProof.\n  intros k dumb.\n  induction dumb.\n  - reflexivity.\n  - discriminate IHdumb.\nQed.\n\nLemma not_dumb: forall k, ~dumb (S k).\nProof.\n  intros k dumb.\n  discriminate (dumb0 (S k) dumb).\nQed.\n\nAnd a way to write the proof as a fixpoint, which looks very similar to the Idris and Agda examples:\nLemma false_succ_eq_zero : forall {P : Prop} {k : nat}, S k = 0 -> P.\nProof.\n  discriminate.\nQed.\n\nFixpoint dumb0' {k : nat} (d : dumb k) : k = 0 :=\n  match d with\n  | way => eq_refl\n  | noway k' d' => false_succ_eq_zero (dumb0' d')\n  end.\n\nDefinition not_dumb' {k : nat} (d : dumb (S k)) : False :=\n  false_succ_eq_zero (dumb0' d).\n\nNotice how dumb0' has the same recursion scheme as notDumb and not_dumb in Idris and Agda.\n\nHowever, there's a lot more boilerplate in the Coq version. And the latter proof is hiding some of the complexity in the false_succ_eq_zero term:\nfalse_succ_eq_zero = \nfun (P : Prop) (k : nat) (H : S k = 0) =>\nlet H0 : False := eq_ind (S k) (fun e : nat => match e with\n                                               | 0 => False\n                                               | S _ => True\n                                               end) I 0 H in\nFalse_ind P H0\n\nCoq proofs are \"mechanized\": they make heavy use of generalized tactics, which produce the ugly code that you see in the Print statements, but enable proof automation (ex: auto randomly tries a predefined set of tactics, lia solves arbitrary linear relations, the Ltac language lets you define custom tactics). Idris and Agda instead rely on more traditional pattern matching, thus their definitions are more \"readable\" (and there is no separate \"proof buffer\" in the interactive theorem-proving process) and their dependent type unification is stronger (see: Agda's cubical types, which supposedly automate a lot of \"intuitive\" things you must explicitly do with Coq like rewrite equalities), but they don't have the kind of powerful automation that Coq does.", "meta": {"post_id": 1848, "input_score": 7, "output_score": 8, "post_title": "Why are these two proofs so different?"}}
{"input": "I am considering whether I should learn the ACL2 proof assistant. However, before diving into that, I wanted to know if there are significant limitations in terms of proving math. The first thing I noticed is that it is said to be first-order-logic (e.g. here).\nI am not sure if being FOL is a significant limitation in terms of proving math. From my very limited understanding, a concrete example of using higher order logic is to assert in inductions that an arbitrary property $P$ holds for all values (e.g. the ability to quantify over properties or functions). But from initial reading, it seems that ACL2 can define various kinds of induction rules:\n\nHowever, unlike Nqthm, ACL2 provides a means by which the user can\nelaborate the rules under which function applications suggest\ninduction schemes. Such rules are called :induction rules. The\ndefinitional principle automatically creates an :induction rule, named\n(:induction fn), for each admitted recursive function, fn.\n...\nIt is possible for the user to create additional :induction rules by\nusing the :induction rule class in defthm.\n\nMy questions are:\nAre there proofs using inductions that can be proved in HOL provers (such as Isabelle) but not in ACL2?\nMore generally, are there other types of mathematical proofs that can be done in HOL provers but not in ACL?", "output": "ACL2 supports well-founded induction up to, but not including $\\epsilon_0$. So you cannot prove consistency of (first order) arithmetic, whereas in higher order logic you can (type :doc ordinals in the REPL for documentation).\nMore practically, however, you cannot express infinite sets in ACL2. More precisely, you can define a predicate (ie. even x holds iff x is an even number) which holds on infinitely many objects, and think of this as an infinite set, however you cannot quantify over infinite sets. This restriction to quantification over finite sets is the real limitation here, more so than being a first-order logic (recall ZFC is a first order logic).\nThere are advantages to this approach, however, as ACL2 supports highly automated inductive proofs.", "meta": {"post_id": 1867, "input_score": 5, "output_score": 8, "post_title": "Is ACL2 first-order-logic and is this a limitation for proving math?"}}
{"input": "I am wondering whether a rule like the following is consistent with decidable conversion and type-checking for dependent type theory:\n$$ \\frac{f\\, g : (x:\\mathsf{bool}) \\to C~x\\quad f~\\mathsf{tt} \\equiv g~\\mathsf{tt} \\quad f~\\mathsf{ff} \\equiv g~\\mathsf{ff}}{f\\equiv g} $$\nThat is, if two functions with domain $\\mathsf{bool}$ agree definitionally on $\\mathsf{tt}$ and $\\mathsf{ff}$, then they are convertible.  An analogous rule for functions on general inductive types like $\\mathbb{N}$ is certainly bad, but it seems not too ridiculous to me to wonder whether the special case of $\\mathsf{bool}$ (and other non-recursive inductive types, like sum-types and enumerated types) might be possible.\nOne thing that makes me somewhat hopeful is that this rule would make $(x:\\mathsf{bool}) \\to C~x$ behave very much like $C~\\mathsf{tt} \\times C~\\mathsf{ff}$, and indeed be definitionally isomorphic to it if the latter also has an $\\eta$-rule.  Since such a rule for products is possible, I thought maybe it could also be possible for the function-type.  And implementation-wise, I could imagine a bidirectional conversion-checking algorithm noticing when the type is a function with domain $\\mathsf{bool}$ and deciding to apply this rule instead of the ordinary $\\eta$-rule for functions.", "output": "If we have $\\eta$ for functions and also your pointwise conversion rule, that implies the full $\\eta$ rule for the (finite) domain type. When checking conversion of arbitrary $t,u$, we can abstract over an arbitrary $\\mathsf{bool}$ subterm $v$ to get an equivalent conversion problem\n$$(\\lambda~(x:\\mathsf{bool}).~ t')~v\\equiv(\\lambda~(x:\\mathsf{bool}).~ u')~v$$\nFor instance\n$$\\mathsf{if}~b~\\mathsf{then}~t~\\mathsf{else}~t \\equiv (\\lambda~x.~\\mathsf{if}~x~\\mathsf{then}~t~\\mathsf{else}~t)~b \\equiv (\\lambda~x.t)~b \\equiv t$$\nThe full $\\eta$-rules for finite sums are rather complicated and expensive to check. There's a reference for STLC with finite sums and products. However, I do not know any reference for deciding finite sum $\\eta$ in richer systems.\nI expect that $\\eta$ for non-empty finite sums remains decidable even in the fully dependent type theories. $\\eta$ for the empty type is decidable only when consistency of typing contexts is decidable.\nMore specifically for our current case, let's assume that $\\bot \\to A$ is definitionally irrelevant. Then, if we have $p : \\bot$ and $t,~u : A$, then\n$$t \\equiv (\\lambda~x.~t)~p \\equiv (\\lambda~x.~u)~p \\equiv u$$\nTherefore, if $\\bot$ is provable then all terms are convertible. This is undecidable for the usual $\\bot$ in type theories, but it might be decidable  in other settings. For example, in ABCFHL cubical type theory all judgments hold when the empty cofibration is implied by the context. This is still decidable because cofibrations are decidable. This is implemented in cooltt.\nAlternatively, we can drop the function $\\eta$ rule. Then, we recover the finite product types as $(x : A) \\to C~x$ for finite $A$, and everything is decidable. I don't think this configuration is useful for anything though.", "meta": {"post_id": 1885, "input_score": 9, "output_score": 9, "post_title": "Strong eta-rules for functions on sum types"}}
{"input": "I'm implementing a version of cubical type theory where the well-definedness of pattern matching functions is implied by:\n\nthe well-typedness of the clauses (type check)\nthe coverage of the patterns (coverage check)\nthe fact that recursive calls are structurally recursive (termination check)\nthe overlapping clauses are confluent (IApplyConfluence) -- see https://amelia.how/quick/boundaries-in-cubical-agda.html\n\nMy question: in what order does it make the best sense? Obviously type check should come first, but what about the others?\n\nSince IApplyConfluence requires reduction of function bodies (for the conversion checks), we may potentially need to reduce the function being type-checked. If there is a non-termination problem, then we are in trouble (the type checker may loop). However, it makes me worried because how am I about to reduce a function that we do not yet know if it's well-defined?\nTermination check is, in my impression, the last phase of the type checking, because there are techniques developed to do some clever reduction to make more definitions type-checked. It seems that we should definitely termination-check those who have no problem on reduction. However, if the clauses are not even confluent, we may reduce the wrong thing!\nIn case of mutually recursive functions, it's worse: the termination check of the two functions happens after both of their type checking, which means\n\nif termination comes last, then both of their confluence check will happen when the other is neither known to be structurally recursive nor confluent.\nif confluence comes last, then both of their termination check will happen when the other is neither known to be structurally recursive nor confluent.", "output": "As you have noticed there is no \"natural\" way to prove confluence and termination: to prove confluence from local confluence you need termination, but proving termination often depends on typing, which becomes a lot more tricky without confluence.\nIn our paper on rewrite rules (The Taming of the Rew: a Type Theory with Computational Assumptions) we took one possible approach, which is to prove (global) confluence directly, i.e. without relying on termination. The basic idea is to rely on the triangle property of parallel reduction, which is a well-known trick that was first used to prove confluence of untyped lambda calculus. The main challenges here are (1) to define a proper notion of parallel reduction for your language and (2) to prove that the triangle property holds.\nAlternatively, you can look into first proving termination without assuming well-typedness of your program, or at least without assuming that subject reduction holds. There are many possible directions to explore here. One example that bears mentioning is SizeChangeTool: A Termination Checker for Rewriting Dependent Types, which requires local confluence but not global confluence.", "meta": {"post_id": 1890, "input_score": 8, "output_score": 10, "post_title": "Termination and confluence -- which goes first?"}}
{"input": "A question was recently asked on the Coq-club mailing list on Coq rejecting a nastily nested inductive type. We encountered a similar difficulty while trying to port code from Agda to Coq: Agda accepts the following\nrecord Foo (x : Set -> Set) :  Set where\n\ndata Bar : Set -> Set where\n  bar : Bar (Foo Bar)\n\nwhile Coq rejects\nRecord Foo (x : Type -> Type) : Type := {}.\n\nFail Inductive Bar : Type -> Type :=\n    | bar : Bar (Foo Bar).\n(*Non strictly positive occurrence of \"Bar\" in \"Bar (Foo Bar)\".*)\n\nIt seems like Coq does not accept nested inductive types in indices, while Agda does.\nSo I tried to look up what kind of complex induction pattern (induction-induction, induction-recursion, nestedness\u2026) Agda exactly allows.\nI found the manual page on mutual recursion, which gives some examples, but is rather vague\u2026 Is there any more precise reference on what Agda allows and forbids? And, ideally, some hint as to why the allowed things are ok/have a semantic?", "output": "I try to write about what's supported in Agda and the semantics that we have (or don't have) for it. Disclaimers:\n\nI'm not an expert on the relevant parts of the Agda source code, I mostly write here as an Agda user, so I might make mistakes. Corrections are welcome.\nI don't write about coinduction. It adds many new weird and interesting feature interactions, but I'm not as nearly as experienced with coinduction as with induction in Agda.\nI don't write about erasure, irrelevance or the flat modality. I've never used these.\nI don't write much about cubical features. They don't yet work well together with many inductive features, e.g. indexed inductive types break canonicity in the 2.6.3 version of cubical Agda. The most general rigorous specification of cubical (higher) inductive types so far is in Evan Cavallo's thesis, and that only covers plain indexed HITs (no IR, no induction-induction, no nesting).\n\nI also don't write much about dependent pattern matching. It is a powerful and complicated feature, but, perhaps surprisingly, I do not think that it's a source of weirdness and exotic features. The --without-K option precisely captures the UIP and non-UIP behaviors, and everything works generally as expected.\n1. What's in Agda\n\nIt's a fun exercise to try to summarize this in one phrase. I'd say that Agda supports\n\nSet/Prop-sorted non-interleaved structural inductive-inductive-recursive types, nested with simple first-order polarities\n\n(Disregarding the cubical mode).\nI'll expand on what the above means, but first let's do an overview of inductive signatures. In a nutshell, an Agda inductive signature is a list where each entry is one of four possible things:\n\nA declaration of a type constructor (sort). Each declaration has a telescope of parameters, a telescope of indices and a return sort which is either Set i or Prop i. In --without-K mode, the return universe size must be large enough to accommodate the index telescope.\nA declaration of a definition.\nThe specification of all constructors of a previously declared sort.\nThe body of a previously declared definition.\n\nAll signature entries are well-typed modulo all previously declared symbols and the definitional equalities stemming from definition bodies. Moreover, a signature is valid only if\n\nAll inductive types are strictly positive. This depends on polarities and positivity checking.\nAll definitions are total. This depends on termination checking.\nAll inductive types return in a universe which is large enough to accommodate all fields of all constructors of the type.\n\nSet/Prop-sorted\nWe can freely mix Set and Prop sorts, where Prop contains definitionally irrelevant types. This has not been described in any publication. I think that this is a fairly tame feature compared to some of\nthe others, in the sense that I don't see any issue with computational adequacy (e.g. canonicity), and it shouldn't be difficult to handle in semantics.\nNon-interleaved\nAgda mandates that all constructors of a sort are specified together. The following can't be expressed:\nA  : Set\nB  : A \u2192 Set\nC  : (a : A) \u2192 B a \u2192 Set\na1 : A\nb  : B a1\na2 : C a1 b \u2192 A\n\nThe interleaved mutual keyword also doesn't allow this; it's merely syntactic sugar which desugars to forward-declared signatures. The problem here is that each constructor depends on the previously declared ones, so they can't be reordered. However, Szumi Xie discovered that we can convert this (and every analogous example) to a non-interleaved version by pushing all sorts inside a Tarski-style \"universe\":\ndata U  : Set\ndata El : U \u2192 Set\n\ndata U where\n  A : U\n  B : El A \u2192 U\n  C : (a : El A) \u2192 El (B a) \u2192 U\n\ndata El where\n  a1 : El A\n  b  : El (B a1)\n  a2 : El (C a1 b) \u2192 El A\n\nI note though that while initial algebras can be derived this way, de-interleaving changes the presented theory itself (notions of algebras are different).\nAlso, this technique does not (fully) work with mixed Set/Prop induction, because we have to choose a single universe for El.\nStructural\nAgda functions are defined using structural recursion, instead of eliminators. In simple cases, the two approaches coincide, but in Agda's generality, they don't. We will see examples later.\nAgda uses lexicographic recursion, which implies that multiple function arguments are considered simultaneously in termination checking. Contrast this to Coq where there's at most one structurally decreasing argument. When we specify constructors of a sort, we extend the structural ordering\nrelation on types by positing that every constructor is larger than its fields.\n(Side comment: predicativity affects structural ordering, because an impredicative function is not necessarily larger than its result values. This is relevant in Coq but not in Agda).\nInductive-inductive-recursive\nClearly, Agda includes the inductive-inductive and inductive-recursive types from the literature, but it includes more than that; see examples later.\nNested with simple first-order polarities\nAgda supports nested induction. The classic example:\ndata Tree (A : Set) : Set where\n  node : A \u2192 List (Tree A) \u2192 Tree a\n\nHere we refer to a pre-existing List type former. But we can use any type former in scope in a nested way:\ndata Bush (A : Set) : Set where\n  nil  : Bush A\n  cons : A \u2192 Bush (Bush A) \u2192 Bush A\n\nThe most general specification of nested induction that I know about is by Johan & Polonsky. But this only covers parameterized finitary algebraic data types, a tiny subset of Agda. The positivity complications are sidestepped by only having finitary types (no function types are allowed in signatures). In ibid. the following is accepted:\ndata Fix f = Fix (f (Fix f))\n\nThe reason is that the type language only allows strictly positive instantiations for f. In Agda, this is not accepted. Instead there's positivity checking and a system of polarities. Like termination checking, positivity checking can look into all definitions and types in a signature.\nAny function-typed symbol can have a polarity for each argument. Polarities are constant, positive, strictly positive, negative and invariant. Function arguments with arbitrary types can have polarity. Polarities are simply typed and first-order, in the sense that they are concrete and and non-dependent for each argument. For example, it is not expressible that the polarity of a parameterized type depends on parameter values. The following fails to check:\ndata A (b : Bool)(X : Set) : Set where\n  a : if b then X else (X \u2192 X) \u2192 A b X\n\ndata B : Set where\n  b : A true B \u2192 B\n\nIt works though when A is a plain definition, because while Agda only considers parameter polarities for data, it does unfold definitions:\nA : Bool \u2192 Set \u2192 Set\nA b X = if b then X else (X \u2192 X)\n\ndata B : Set where\n  b : A true B \u2192 B\n\nWhat about the strength of nested induction compared to non-nested induction? We don't know much about this in the generality of Agda, and from what I've heard, not even in the generality of Coq. We do know that nested induction allows construction of some types which are otherwise not constructible. For example, infinitary quotient inductive types are not constructible from inductive types and quotients, but if we can use nested quotients, they actually are.\n2. Exotic features\n\nLet's look at some notable concrete features which fall out from the Agda implementation.\nRecursion-recursion\nThis feature has no semantics in the literature. The name \"recursive-recursive\" was coined by Fredrik Forsberg, as a characterization of inductive-inductive eliminators, but the weird part here is that recursion-recursion works in Agda wholly separately of induction-induction. Take this ordinary mutual inductive definition:\ndata Con : Set\ndata Ty  : Set\n\ndata Con where\n  \u2219 : Con\n  _,_ : Con \u2192 Ty \u2192 Con\n\ndata Ty where\n  Pi  : Con \u2192 Ty \u2192 Ty \u2192 Ty\n  U   : Con \u2192 Ty\n  El  : Con \u2192 Ty\n\nThis is intended to be \"raw syntax\" for some toy type theory. Now, we want to specify well-formedness for contexts and types. We want to specify that U \u0393 is a type in \u0393, and El \u0393 is a type in (\u0393 , U). We define well-formedness predicates by recursion:\nopen import Data.Product\nopen import Relation.Binary.PropositionalEquality\nopen import Data.Unit\n\nWfCon : Con \u2192 Set\nWfTy  : \u2200 \u0393 \u2192 WfCon \u0393 \u2192 Ty \u2192 Set\n\nWfCon \u2219       = \u22a4\nWfCon (\u0393 , A) = \u03a3 (WfCon \u0393) \u03bb \u0393w \u2192 WfTy \u0393 \u0393w A\n\nWfTy \u0393 \u0393w (Pi \u0393' A B) = \u0393 \u2261 \u0393' \u00d7 \u03a3 (WfTy \u0393 \u0393w A) \u03bb Aw \u2192 WfTy (\u0393 , A) (\u0393w , Aw) B\nWfTy \u0393 \u0393w (U \u0393')      = \u0393 \u2261 \u0393'\nWfTy \u0393 \u0393w (El \u0393')     = \u0393 \u2261 (\u0393' , U \u0393')\n\nAgda accepts this, but it cannot be defined directly using the elimination principle of Con and Ty! The problem is that the type of WfTy refers to WfCon, but no such dependency is possible in the eliminator. Notions of \"elimination\" and \"structural recursion\" diverge here.\nNotably, much of the infamously tedious boilerplate in the initiality constructions is for recovering recursive-recursive definitions from the elimination principles of untyped syntax.\nThis is related to the construction of inductive-inductive types: we represent inductive-inductive types using mutually inductive \"raw terms\" together with well-formedness predicates. The bulk of the work is to show\nthat recursion-recursion is derivable for the ordinary mutual inductive terms.\n\nWe know that finitary inductive-inductive types are constructible from UIP, funext and plain induction.\nWe reasonably conjecture that infinitary inductive-inductive types are constructible from UIP, funext, SProp, propext, quotients and WISC.\n\nBut we know pretty much nothing --without-K. Except, if we throw recursion-recursion in the mix, it looks like all inductive-indutive types are constructible! This is unpublished and rather fuzzy, because we don't even have a specification of recursion-recursion, but I'm fairly confident in it. I conjecture that neither recursion-recursion nor induction-induction are derivable --without-K, in the absence of some other unknown features.\nDouble induction-recursion\nThe definition of universes using induction-recursion is well-known. But what about universes of setoids, where every represented type is equipped with an equivalence relation? This is required when we want to model an observational type theory. There are several ways to do it in Agda; here's a minimal incomplete example.\nopen import Data.Nat\nopen import Data.Unit\nopen import Data.Empty\nopen import Relation.Binary.PropositionalEquality\nopen import Data.Product\n\ndata U : Set\nU~     : U \u2192 U \u2192 Set\nreflU  : \u2200 {A} \u2192 U~ A A\nEl     : U \u2192 Set\nEl~    : \u2200 {A A'} \u2192 U~ A A' \u2192 El A \u2192 El A' \u2192 Set\n\nresp~ : \u2200 {A} \u2192 (El A \u2192 U) \u2192 Set\nresp~ B = \u2200 {a a'} \u2192 El~ reflU a a' \u2192 U~ (B a) (B a')\n\ndata U where\n  \u2115' : U\n  \u03a0' : (A : U)(B : El A \u2192 U) \u2192 resp~ B \u2192 U\n\nU~ \u2115'         \u2115'           = \u22a4\nU~ \u2115'         (\u03a0' _ _ _)   = \u22a5\nU~ (\u03a0' _ _ _) \u2115'           = \u22a5\nU~ (\u03a0' A B _) (\u03a0' A' B' _) = \u03a3 (U~ A A') \u03bb A~ \u2192 \u2200 {a a'} \u2192 El~ A~ a a' \u2192 U~ (B a) (B' a')\n\nreflU {\u2115'}        = tt\nreflU {\u03a0' A B Br} = reflU , \u03bb a~ \u2192 Br a~\n\nEl \u2115'         = \u2115\nEl (\u03a0' A B _) = (a : El A) \u2192 El (B a)\n\nEl~ {\u2115'}       {\u2115'}         A~        x y = x \u2261 y\nEl~ {\u03a0' A B _} {\u03a0' A' B' _} (A~ , B~) f g = \u2200 {a a'} a~ \u2192 El~ (B~ a~) (f a) (g a')\n\nHere U itself is equipped with a relation and each El type is also equipped with a heterogeneous equivalence. The resp~ field in the type of \u03a0' requires that the codomain type respects the chosen equivalences.\nThis definition, however, is not covered by any specification of induction-recursion in the literature. In classic IR, an inductive sort can be fibered over an external type. Here, U~ maps from two copies of U to Set; hence the name \"double induction-recursion\". More generally, Agda supports recursive functions mapping out from any telescope of inductive sorts.\nIn this specific example, the double IR can be avoided by defining U~ inductively and El~ homogeneously, but a) that doesn't compute as strictly as we'd like to have in observational TT b) in the complete formalization of the setoid universe we'll need double IR anyway in some other places.\nIn the recent OTT paper, Pujet & Tabareau use similar IR for the semantics of the universe. They work in constructive set theory and don't give a particular justification for this IR. I don't know enough about set theory to tell if this IR is indeed trivially valid there. However, I think that double IR is very likely to make sense and it is the most convenient way to formalize setoid universes. The alternative is rather gnarly: Altenkirch et al. defines the same setoid universe without IR, using complicated induction-induction.\nEndo-induction-recursion\nIn classic IR, a recursive function has to map into an external type, i. e.\nsome type which is not declared as a sort in the signature that is being defined. In Agda, we can map wherever we want. Example:\ndata A : Set\ndata B : A \u2192 Set\nf      : (a : A) \u2192 B a\ndata A where ...\ndata B where ...\n\nThis is again not described in the literature. But the semantics looks a bit easier in this case, as far as I see. I expect that endo-IR can be derived from quotient induction and UIP. The following is due to Ambrus Kaposi.\nFirst, we define a quotient inductive type which includes f as an ordinary constructor and the definition of f as a bunch of quotient equations. Then, we use elimination to define f again as a function instead, which is always possible if the original f specification is properly structurally recursive. The thus strictified initial algebra can be then shown to be equivalent to the one with the quotienting.\nThe point is that we can often redefine quotients as functions, which is more convenient in formalization. This is useful even if we can't easily get rid of all quotienting. Endo-IR can be viewed as a shortcut to such strictification which relies on Agda's termination checking.\n3. Summary of Agda features in the literature\n\nI describe sets of features which are covered in the literature. These are the largest such sets that I was able to find.\n\nChapter 5 of my thesis specifies infinitary inductive-inductive types. It's a special case of quotient induction-induction, but quotients can be trivially dropped from the syntax & the semantics. This chapter allows us to compute eliminators and their beta rules in pretty much the same way as they would appear in Agda. Prop is not  supported though, nor sorts at different universe levels; Section 5.6 only supports having the same universe level for all sorts. I skipped this feature because universe levels are rather tedious to wrestle all the time.\nThis note of mine extends this to infinitary induction-induction-recursion. This follows the style of Dybjer & Setzer, in that recursive functions eliminate from exactly one inductive sort to an external type. Semantics is not developed in the note but I don't expect any complication for the set-truncated case.\nAs I mentioned, Johan & Polonsky cover most ground from nested induction.\nEvan Cavallo covers most ground from higher inductive types in the cubical mode. It's a different cubical type theory, but everything should be straightforward to port over to Agda's flavor.\n\nI shall leave coinduction to someone else as I'm not that familiar with the literature.", "meta": {"post_id": 1981, "input_score": 15, "output_score": 21, "post_title": "What are the complex induction patterns supported by Agda?"}}
{"input": "I heard that many proof assistant programs are made based on the type theory.\nFor me, as a mathematician, when I met Coq at first, it is difficult to accustomed with it.\nSo I have a question.\nIs it possible to make a proof assistant program based on ZFC?\nOr Can we make a setting on Coq so that we can do set theoretical approach on Coq?\nIf it is possible, then I think that many undergraduate mathematicians can use it easily and get many benefits from it.", "output": "There are three major foundations to theorem provers.  First order logic with set theory (usually something close to ZFC but maybe with inaccessible cardinals and/or better support for proper classes).  Higher order logic (a form of type theory).  Dependent type theory.  Mizar, Metamath (set.mm), Isabelle/ZFC all use the first.  HOL-light, HOL4, and Isabelle/HOL use HOL.  Coq, Lean, and Agda use DTT.\nSo this answers your first question.  Yes, it is possible to base a theorem prover on something like ZFC.  Also see Open source proof assistants for first order logic with equality and set theory.\nAs for taking a set theoretical approach in one of the later theorem provers, I am not sure I see the point.  But nonetheless, let me explain how I see the connections.  Pure HOL is the theory of a topos, so instead of the theory having set based models, it has topos based models.  But once one adds in the axiom of choice (which all HOL based provers do), then you can think of all the types as sets.  The rules of HOL plus the axiom of choice and the axiom of infinity are basically just another fairly natural way to describe set theory.  It may be a bit more like the category Set in category theory more than the models of ZFC in set theory, but still a form of set theory nonetheless.  (If I remember correctly, I think one also needs replacement to get equivalence with ZFC.)  In HOL (plus the various axioms), it is easy to write many theorems of set theory.\nAs for dependent type theory, again the base logic is more categorical (maybe describing various types of infinity categories or infinity topoi if I remember correctly).  It also works well as a syntax for a programming language and it has a nice computational interpretation.  Again if you add in the axiom of choice (as is common in Lean), your types behave like sets and you can think of it as a different form of set theory. Mario Carniero\u2019s master\u2019s thesis for example shows that Lean\u2019s logic is equi-consistent with ZFC plus countably many inaccessible cardinals.  It is certainly possible to do (meta) set theory in DTT like proving the independence the continuum hypothesis in ZFC.  (Also see https://proofassistants.stackexchange.com/a/1533/122.)\nAlso, if you are interested in the meta theoretic aspects of these three logics, see Jeremy Avigad\u2019s article on the Foundations of proof assistants.\nEdit: as for making it more accessible to mathematicians, I would argue that mathematicians rarely think about foundations (with the exceptions of some with more of a logical bent).  The biggest practical aspects are (1) usability of the systems and (2) being able to describe the mathematical objects and reasoning that one wants.  As for (1), that doesn\u2019t have much to do with foundations.  As for (2), I think a lot of mathematicians who got into proof assistants actually find type theory, especially dependent type theory, to be a very natural way to encode the mathematical objects they work with.  The biggest sticking point with foundations is the need for the axiom of choice.  In Coq it is common to avoid it if possible, but many mathematicians, in say Lean, use it freely.  That way one doesn\u2019t have to worry about constructive math, which while interesting and important, is very foreign to the working mathematician.", "meta": {"post_id": 2167, "input_score": 7, "output_score": 10, "post_title": "Is it possible to make a proof assistant program based on ZFC?"}}
{"input": "I'm unclear on the terms \"deep embedding\" and \"shallow embedding\".  I understand they are concerned with using one logic inside another, e.g. using Metamath/set.mm proofs in Lean or building Coq inside Coq.  I also get the impression that deep embeddings involve some sort of re-implementation, whereas shallow embeddings do not.\nI'm looking for the following:\n\nA good explanation of the differences between deep and shallow embeddings.\nAn example of each type.  (My impression is that most projects are deep embeddings, so in particular, I don't have a good example of a shallow embedding.)\n(Bonus) Whether each of the following projects are shallow or deep embeddings:\n\nCoq in Coq (Coq -> Coq)\nTranslating Dirichlet's theorem from Metamath to Lean (paper )(Metamath/set.mm -> Lean)\nCoq is a Lean Typechecker (Lean -> Coq)\nTowards self-verification of HOL Light\n (HOL Light -> HOL Light)\nSelf-verifying Metamath Zero (MM0 -> MM0/Peano)\nFlypitch (ZFC -> Lean)\nDedukti (? -> ?)\nLean4Lean (Lean -> Lean)\nMathPort (Lean 3 -> Lean 4)\nEncoding the HOL Light logic in Coq (HOL Light -> Coq)", "output": "The terms \"deep\" and \"shallow\" embedding are somewhat informal and have many variations. I will explain the essentials only.\nAn embedding from a source language $S$ to a target language $T$ is shallow if the constituent parts of $S$ are translated (mostly) directly to the corresponding parts of $T$. This is useful when we want to use $S$ but all we have is $T$. For example, if $S$ is a domain-specific logic and $T$ is a full-fledged proof assistant, we encode $S$ using the logic of $T$ and then use $T$'s theorem proving abilities to prove $S$-theorems. Even more specifically, suppose $S$ is the classical propositional calculus and $T$ is a proof assistant based on constructive type theory, equipped with a tactic for proving intuitionistic propositions. We can shallowly translate $S$ into $T$ using the double-negation translation and use $T$'s tactic to prove classical $S$-statements.\nAn embedding is deep if in $T$ we define (inductive) types that describe the syntax and inference rules of $S$. This is useful when we want to use $T$ to study $S$ as a formal system. For example, if $S$ is a programming language and $T$ is a full-fledged proof assistant, we can define in $T$ types that represent the syntax, the typing rules, and the operational semantics of $S$. This allows us to prove theorems about $S$, such as type safety. (In fact, this was one of successful early applications of proof assistant technology.)\nExample: simply-typed $\\lambda$-calculus\nSuppose $S$ is the simply typed $\\lambda$-calculus with the following parts:\n\nthe types of natural numbers nat\nfunction types, written a \u21d2 b\nconstructors zero and succ\n$\\lambda$-abstraction $\\lambda (x{:}A),\\, e$ and application $e_1 \\, e_2$.\n\nThere are also typing contexts and typing rules which I am not going to write here, as they are quite standard (and also formalized below as a deep embedding in Agda).\nShallow embedding\nThe following is a shallow embedding of the calculus in Agda. It is mostly just a collection of abbreviations, so that we can write nat instead of Agda's \u2115, and A \u21d2 B instead of Agda's A \u2192 B, etc.\n(Note: the embedding shown below is not as shallow as it could be, because we map the typing contexts of \u03bb-calulus to types in Agda. An even shallower embedding would map contexts to contexts, but that would be a bit confusing to beginners because most of the embedding would become invisible.)\nmodule Shallow where\n\n  open import Data.Nat\n  open import Data.Product\n  open import Data.Unit\n\n  -- The types of the \u03bb-calculus are Agda's types.\n  type = Set\n\n  -- The typing contexts are sets.\n  context = Set\n\n  -- The empty context is the unit type\n  \u2219 : context\n  \u2219 = \u22a4\n\n  -- Extending a context with type is Agda's cartesian product\n  infixl 5 _\u2a1f_\n  _\u2a1f_ : context \u2192 type \u2192 context\n  \u0393 \u2a1f A = \u0393 \u00d7 A\n\n  -- The \u03bb-calculus type nat is Agda's type \u2115\n  nat : type\n  nat = \u2115\n\n  -- The \u03bb-calculus function type is Agda's function type\n  infixr 7 _\u21d2_\n  _\u21d2_ : type \u2192 type \u2192 type\n  A \u21d2 B = A \u2192 B\n\n  -- The \u03bb-calculus typing judgement x : nat, f : nat \u21d2 nat, y : nat \u22a2 succ (succ x) : nat\n  example-1 : (\u2219 \u2a1f nat \u2a1f nat \u21d2 nat \u2a1f nat) \u2192 nat\n  example-1 (((_ , x) , f) , y) = suc (suc x)\n\n  -- The \u03bb-calculus typing judgement \u2219 \u22a2 (\u03bb (x y : nat) succ x) (succ (succ 0))\n  example-2 : \u2219 \u2192 nat \u21d2 nat\n  example-2 tt = (\u03bb (x y : nat) \u2192 suc x) (suc (suc 0))\n\n  -- The \u03bb-calculus typing judgement f : A \u21d2 A \u22a2 \u03bb (x : A) f (f x) : A \u21d2 A\n  example-3 : {A : type} \u2192 (\u2219 \u2a1f A \u21d2 A) \u2192 (A \u21d2 A)\n  example-3 {A} (_ , f) = \u03bb (x : A) \u2192 f (f x)\n\nWith the shallow embedding it is very easy to compute in \u03bb-calculus, just use Agda's normalization abilities to compute their normal forms. This is possible because our embedding makes sure that the \u03bb-calculus normalization and Agda's normalization coincide.\nDeep embedding\nHere is the corresponding deep embedding.\n\nmodule Deep where\n\n  -- The \u03bb-calculus simple types\n  infixr 7 _\u21d2_\n  data type : Set where\n    nat : type\n    _\u21d2_ : type \u2192 type \u2192 type\n\n  -- The \u03bb-calculus typing context, we use de Bruijn indices\n  infixl 5 _\u2a1f_\n  data context : Set where\n    \u2219 : context\n    _\u2a1f_ : context \u2192 type \u2192 context\n\n  -- A \u2208 \u0393 is the type of those de Bruijn indices whose type in \u0393 is A\n  infix 3 _\u2208_\n  data _\u2208_ : type \u2192 context \u2192 Set where\n    z : \u2200 {A : type} {\u0393 : context} \u2192 A \u2208 \u0393 \u2a1f A\n    s : \u2200 {A B : type} {\u0393 : context} \u2192 B \u2208 \u0393 \u2192 B \u2208 \u0393 \u2a1f A\n\n  -- term \u0393 A is the type of \u03bb-calculus terms of type A in context \u0393\n  data term : context \u2192 type \u2192 Set where\n    zero : \u2200 {\u0393} \u2192 term \u0393 nat\n    succ : \u2200 {\u0393} \u2192 term \u0393 nat \u2192 term \u0393 nat\n    var : \u2200 {\u0393 A} \u2192 A \u2208 \u0393 \u2192 term \u0393 A\n    lam : \u2200 {\u0393} A {B} \u2192 term (\u0393 \u2a1f A) B \u2192 term \u0393 (A \u21d2 B)\n    app : \u2200 {\u0393 A B} \u2192 term \u0393 (A \u21d2 B) \u2192 term \u0393 A \u2192 term \u0393 B\n\n  -- The \u03bb-calculus typing judgement x : nat \u2a1f f : nat \u21d2 nat\u2a1f y : nat \u22a2 succ (succ x) : nat\n  example-1 : term (\u2219 \u2a1f nat \u2a1f nat \u21d2 nat \u2a1f nat) nat\n  example-1 = succ (succ (var (s (s z))))\n\n  -- The \u03bb-calculus typing judgement \u2219 \u22a2 (\u03bb (x y : nat) succ x) (succ (succ 0)) : nat \u21d2 nat\n  example-2 : term \u2219 (nat \u21d2 nat)\n  example-2 = app (lam nat (lam nat (succ (var (s z))))) (succ (succ zero) )\n\n  -- The \u03bb-calculus typing judgement f : A \u21d2 A \u22a2 \u03bb (x : A) f (f x) : A \u21d2 A\n  example-3 : {A : type} \u2192 term (\u2219 \u2a1f A \u21d2 A) (A \u21d2 A)\n  example-3 {A} = lam A (app (var (s z)) (app (var (s z)) (var z)))\n\nIn the deep embedding the terms of the \u03bb-calculus are elements of the inductive type term. We cannot use Agda's normalization algorithm to compute with these, they're all in normal form already.\nHowever, with the deep embedding we can reason about \u03bb-calculus. For example, we can define a model of the \u03bb-calculus:\nmodule Semantics where\n\n  -- The set-theoretic model of \u03bb-calculus\n\n  open import Data.Nat\n  open import Data.Unit\n  open import Data.Product\n\n  open Deep\n\n  -- Types are interpreted as sets\n  \u27e6_\u27e7\u1d57 : type \u2192 Set\n  \u27e6 nat \u27e7\u1d57 = \u2115\n  \u27e6 A \u21d2 B \u27e7\u1d57 = \u27e6 A \u27e7\u1d57 \u2192 \u27e6 B \u27e7\u1d57\n\n  -- Contexts are interpreted as sets\n  \u27e6_\u27e7\u1d9c : context \u2192 Set\n  \u27e6 \u2219 \u27e7\u1d9c = \u22a4\n  \u27e6 \u0393 \u2a1f A \u27e7\u1d9c =  \u27e6 \u0393 \u27e7\u1d9c \u00d7 \u27e6 A \u27e7\u1d57\n\n  -- Auxiliary map, for looking up values of variables\n  lookup : \u2200 {\u0393 A} \u2192 A \u2208 \u0393 \u2192 \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  lookup z (_ , a) = a\n  lookup (s x) (\u03b7 , _) = lookup x \u03b7\n\n  -- A typing judgement \u0393 \u22a2 e : A is interpreted as a map \u27e6 e \u27e7 : \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  \u27e6_\u27e7 : \u2200 {\u0393 A} \u2192 term \u0393 A \u2192 \u27e6 \u0393 \u27e7\u1d9c \u2192 \u27e6 A \u27e7\u1d57\n  \u27e6 zero \u27e7 \u03b7 = 0\n  \u27e6 succ e \u27e7 \u03b7 = suc (\u27e6 e \u27e7 \u03b7)\n  \u27e6 var x \u27e7 \u03b7 = lookup x \u03b7\n  \u27e6 lam A e \u27e7 \u03b7 = \u03bb a \u2192 \u27e6 e \u27e7 (\u03b7 , a)\n  \u27e6 app e\u2081 e\u2082 \u27e7 \u03b7 = \u27e6 e\u2081 \u27e7 \u03b7 (\u27e6 e\u2082 \u27e7 \u03b7)\n\n  -- The interpretation of \u2219 \u22a2 (\u03bb (x : nat) . succ (succ x)) (succ zero) : nat\n  cow : \u27e6 \u2219 \u27e7\u1d9c \u2192 \u27e6 nat \u27e7\u1d57\n  cow tt = \u27e6 app (lam nat (succ (succ (var z)))) (succ zero) \u27e7 tt\n  -- Agda normalizes cow tt to the numeral 3\n\nYes, the model is also an interpreter for \u03bb-calculus. Welcome to constructive mathematics.\nThe laundry list\nI am not familiar with all the embeddings you listed in the question. Hopefully other will provide answers for the missing ones:\n\n\u201cCoq in Coq\u201d is deep. They want to prove things about Coq. A shallow embedding would be the identity map.\nDedukti is a logical framework, which makes it a bit special. Its purpose is formalization of formal systems. One defines the syntax and typing rules of the embedded system (deep embedding), but in such a way that Dedukti's contexts, variables, type-checking, normalization and rewriting abilities can be reused by the embedded system (shallow embedding).\n\u201cCoq is a Lean Typechecker\u201c is shallow. The web page even says: \u201cThe basic idea is to translate Lean Prop to Coq SProp, inductives to inductives, etc.\u201c\n\u201cTowards self-verification of HOL Light\u201d has a section called \u201cFormalized syntax\u201d so I will guess it is deep without reading the entire paper.\n\u201cLean4Lean\u201c is about implementing Lean in Lean, a self-interpreter. This is a deep embedding because it will involve parsing syntax and interpreting it into machine code. A shallow embedding of Lean in Lean is just the identity transformation.\n\u201cMathPort\u201d is a shallow embedding, because they tried as hard as possible to make it the identity transformation.\n\u201cEncoding the HOL Light logic in Coq\u201d is shallow.\n\nSupplemental: What about set theories?\nAn interesting question is whether emeddings of set theory into type theory are shallow or deep. Let us just focus on material set theory such as ZFC (although the story is much the same for structural set thoeries).\nA material set theory is typically a single-sorted first-order theory. In type theory we commonly find something like this:\nopen import Level\nopen import Data.Empty\nopen import Data.Product\nopen \u03a3\n\nmodule SetTheory (\u2113 : Level) where\n\n  -- The type of well-founded trees with branching type from universe level \u2113\n  data V : Set (suc \u2113) where\n    set : \u2200 {I : Set \u2113} \u2192 (I \u2192 V) \u2192 V\n\n  mutual\n    -- Membership relation\n    infix 5 _\u2208_\n    data _\u2208_ : V \u2192 V \u2192 Set \u2113 where\n      \u2208-set : \u2200 {I} {t} {u : I \u2192 V} \u2192 (\u03a3[ i \u2208 I ] t \u2248 u i) \u2192 t \u2208 set u\n\n    -- Subset relation\n    infix 5 _\u2286_\n    _\u2286_ : V \u2192 V \u2192 Set \u2113\n    set x \u2286 t = \u2200 i \u2192 x i \u2208 t\n\n    -- Equality of well-founded trees as sets\n    infix 5 _\u2248_\n    data _\u2248_ : V \u2192 V \u2192 Set \u2113 where\n      \u2248-set : \u2200 {t u : V} \u2192 t \u2286 u \u2192 u \u2286 t  \u2192 u \u2248 t\n\n  -- We check that (V, \u2208, \u2248) satisfies the set-theoretic axioms.\n  -- For illustration we only provide extensionality and empty set.\n  -- (Note that we cannot validate the powerset axiom, Agda is too predicative.)\n\n  data empty-type : Set \u2113 where\n\n  \u2205 : V\n  \u2205 = set {I = empty-type} \u03bb {()}\n\n  axiom-empty-set : \u03a3[ x \u2208 V ] \u2200 (y : V) \u2192 y \u2208 x \u2192 \u22a5\n  axiom-empty-set = \u2205 , \u03bb {y (\u2208-set ())}\n\n  axiom-extensionality : \u2200 {x y : V} \u2192 x \u2286 y \u2192 y \u2286 x \u2192 x \u2248 y\n  axiom-extensionality x\u2286y y\u2286x = \u2248-set y\u2286x x\u2286y\n\n  -- and so on\n\nI would call this a model of set theory in type theory. It is not an \"embedding\" in the sense of having a syntactic translation from one formal system to another.\nThis situation is completely analogous to other situations involving first-order theories, which is what a material set theory usually is. Take the theory of a group, for example. If we define a single group, say $\\mathbb{Z}_7$, then that is like defining a single model V of set theory, as above. We might define the structure of all groups\nstructure Group : Type where\n  carrier : Type\n  mul : carrier \u2192 carrier \u2192 carrier\n  inv : carrier \u2192 carrier\n  assoc : ...\n  unit-left : ...\n  unit-right : ...\n  inv-left : ...\n  inv-right : ...\n\nbut that would still just be semantics, as we're now just defining (the object part of) the category of all groups. We could to the same for set theories and define the category of all models of set theory, so not an embedding.\nIf you insist, you may call the above a shallow something-or-other of set theory into type theory (because we are using type-theoretic logic to validate set-theoretic axioms), but I hesitate to call it an \"embedding\" because it does not faithfully reflect provability of set-theoretic statements. That is, type theory might prove some set-theoretic statements about V which set theory does not prove (this is not an easily answered question, and it depends on the exact set-theoretic axioms and the exact type-theoretic rules).\nA deep embedding of set theory into type theory would begin with inductive types representing the first-order formulas and terms. But that's a different kind of animal that we would use if we wanted to prove meta-theorems about the first-order theory known as \"set theory\".", "meta": {"post_id": 2499, "input_score": 5, "output_score": 10, "post_title": "What is a deep embedding vs a shallow embedding? With examples?"}}
