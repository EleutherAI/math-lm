# For fine-tuning on my (Yale LILY) lab server
experiment_name: mathlm0-1.3B-gsm8k
task: "gsm8k"
model_load_path: "hoskinson-center/proofGPT-v0.1"
log_dir: "runs"

max_length: 500

lr: 2.e-5
lr_schedule: "cosine"
warmup_steps: 200
train_steps: 4000
log_steps: 50
save_steps: 2000
batch_size_per_device: 1 # want effective batch size of 32 across experiments
gradient_accumulation: 32
weight_decay: 0.01

